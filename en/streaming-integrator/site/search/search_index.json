{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to WSO2 Streaming Integrator Documentation This is the home page of WSO2 streaming integrator documentation.","title":"Home"},{"location":"#welcome-to-wso2-streaming-integrator-documentation","text":"This is the home page of WSO2 streaming integrator documentation.","title":"Welcome to WSO2 Streaming Integrator Documentation"},{"location":"concepts/","text":"Concepts in Streaming Integrator","title":"Concepts"},{"location":"concepts/#concepts-in-streaming-integrator","text":"","title":"Concepts in Streaming Integrator"},{"location":"page-not-found/","text":"Page not found.","title":""},{"location":"page-not-found/#page-not-found","text":"","title":"Page not found."},{"location":"admin/adding-third-party-non-osgi-libraries/","text":"Adding Third Party Non OSGi Libraries WSO2 SP is a OSGi-based product. Therefore, when you integrate third pary products such as Oracle with WSO2 SP, you need to check whether the libraries you need to add to WSO2 SP are OSGi based. If they are not, you need to convert them to OSGi bundles before adding them to the SP_HOME /lib directory. To convert jar files to OSGi bundles, follow the procedure given below: Download the non-OSGi jar for the required third party product, and save it in a preferred directory in your machine. In your CLI, navigate to the \\ SP_HOME>/bin directory. Then issue the following command. ./jartobundle.sh PATH_TO_NON-OSGi_JAR ../lib This generates the converted file in the SP_HOME /lib directory. 3. Restart the WSO2 SP server.","title":"Adding Third Party Non OSGi Libraries"},{"location":"admin/adding-third-party-non-osgi-libraries/#adding-third-party-non-osgi-libraries","text":"WSO2 SP is a OSGi-based product. Therefore, when you integrate third pary products such as Oracle with WSO2 SP, you need to check whether the libraries you need to add to WSO2 SP are OSGi based. If they are not, you need to convert them to OSGi bundles before adding them to the SP_HOME /lib directory. To convert jar files to OSGi bundles, follow the procedure given below: Download the non-OSGi jar for the required third party product, and save it in a preferred directory in your machine. In your CLI, navigate to the \\ SP_HOME>/bin directory. Then issue the following command. ./jartobundle.sh PATH_TO_NON-OSGi_JAR ../lib This generates the converted file in the SP_HOME /lib directory. 3. Restart the WSO2 SP server.","title":"Adding Third Party Non OSGi Libraries"},{"location":"admin/app-Overview/","text":"App Overview When you open the WSO2 SP Status Dashboard, the Node Overview page is displayed by default. If you want to view all the Siddhi applications deployed in your WSO2 SP setup, click on the App View tab (marked in the image below). The App Overview tab opens and all the Siddhi applications that are currently deployed are displayed as shown in the image below. The status is displayed in green for active Siddhi applications, and in red for inactive Siddhi applications. If no Siddhi applications are deployed in your WSO2 SP setup, the following message is displayed. The Siddhi applications are listed under the deployment mode in which they are deployed (i.e., Single Node Deployment , HA Deployment , and Distributed Deployment ). Info If your WSO2 SP setup is a distributed deployment, only the parent Siddhi applications are displayed in this tab. The following information is displayed for each Siddhi aplication. Siddhi Application : The name of the Siddhi application. Status : This indicates whether the Siddhi application is currently active or inactive. Deployed Time : The time duration that has elapsed since the Siddhi application was deployed in the SP setup. Deployed Node : The host and the port of the SP node in which the Siddhi application is displayed. The purpose of this tab is to check the status of all the Siddhi applications that are currently deploed in the SP setup. If you click on a Siddhi Application under Single Node Deployment or HA Deployment , information specific to that Siddhi application is displayed as explained in Viewing Statistics for Siddhi Applications . If you click on the parent Siddhi application under Distributed Deployment , information specific to that parent Siddhi application is displayed as explained in Viewing Statistics for Parent Siddhi Applications . If you click on a deployed node, information specific to that node is displayed as explained in Viewing Node-specific Pages .","title":"App Overview"},{"location":"admin/app-Overview/#app-overview","text":"When you open the WSO2 SP Status Dashboard, the Node Overview page is displayed by default. If you want to view all the Siddhi applications deployed in your WSO2 SP setup, click on the App View tab (marked in the image below). The App Overview tab opens and all the Siddhi applications that are currently deployed are displayed as shown in the image below. The status is displayed in green for active Siddhi applications, and in red for inactive Siddhi applications. If no Siddhi applications are deployed in your WSO2 SP setup, the following message is displayed. The Siddhi applications are listed under the deployment mode in which they are deployed (i.e., Single Node Deployment , HA Deployment , and Distributed Deployment ). Info If your WSO2 SP setup is a distributed deployment, only the parent Siddhi applications are displayed in this tab. The following information is displayed for each Siddhi aplication. Siddhi Application : The name of the Siddhi application. Status : This indicates whether the Siddhi application is currently active or inactive. Deployed Time : The time duration that has elapsed since the Siddhi application was deployed in the SP setup. Deployed Node : The host and the port of the SP node in which the Siddhi application is displayed. The purpose of this tab is to check the status of all the Siddhi applications that are currently deploed in the SP setup. If you click on a Siddhi Application under Single Node Deployment or HA Deployment , information specific to that Siddhi application is displayed as explained in Viewing Statistics for Siddhi Applications . If you click on the parent Siddhi application under Distributed Deployment , information specific to that parent Siddhi application is displayed as explained in Viewing Statistics for Parent Siddhi Applications . If you click on a deployed node, information specific to that node is displayed as explained in Viewing Node-specific Pages .","title":"App Overview"},{"location":"admin/business-Rules-Templates/","text":"Business Rules Templates Rule Templates are used as specifications to gain inputs from users through dynamically generated fields, for creating business rules. A template group is a business domain level grouping. The definition of a template looks as follows. { \"templateGroup\" : { \"name\" : \" Name of the template group \", \"uuid\":\" UUID for the template group \", \"description\" : \" (Optional) description for the template group \", \"ruleTemplates\" : [ { \"name\" : \" Name of the rule template \" , \"uuid\" : \" UUID for the rule template \", \"type\" : \"template\", \"instanceCount\" : \"one or many\", \"description\" : \" (Optional) description for the rule template \", \"script\" : \" (Optional) Javascript with reference to the properties \", \"templates\" : [ { \"type\" : \"siddhiApp\", \"content\" : \" SiddhiApp_1 with ${templatedProperty_x} \" }, { \"type\" : \"siddhiApp\", \"content\" : \" SiddhiApp_n with ${templatedProperty_y} \" } ], \"properties\" : { \"templatedProperty_x\" : {\"fieldName\" : \" Field name for the property \", \"description\" : \" Description for the property \", \"defaultValue\" : \" Default value for the property \"}, \"templatedProperty_y\" : {\"fieldName\" : \" Field name for the property \", \"description\" : \" Description for the property \", \"defaultValue\" : \" Default value for the property \", \"options\" : [\" option_1 \", \" option_n \"]} } }, { \"name\" : \" Name of the rule template \", \"uuid\" : \" UUID for the rule template \", \"type\" : \"input\", \"instanceCount\" : \"one or many\", \"description\" : \" (Optional) description for the rule template \", \"script\" : \" (Optional) Javascript with reference to the properties \", \"templates\" : [ { \"type\" : \"siddhiApp\", \"content\" : \" SiddhiApp with ${templatedProperty_x} \", \"exposedStreamDefinition\" :\" Exposed stream definition \" } ], \"properties\" : { \"templatedProperty_x\" : {\"fieldName\" : \" Field name for the property \", \"description\" : \" Description for the property \", \"defaultValue\" : \" Default value for the property \", \"options\" : [\" option_1 \", \" option_n \"]} } }, { \"name\" : \" Name of the rule template \", \"uuid\" : \" UUID for the rule template \", \"type\" : \"output\", \"instanceCount\" : \"one or many\", \"description\" : \" (Optional) description for the rule template \", \"script\" : \" (Optional) Javascript with reference to the properties \", \"templates\" : [ { \"type\" : \"siddhiApp\", \"content\" : \" SiddhiApp with ${templatedProperty_x} \", \"exposedStreamDefinition\" :\" Exposed stream definition \" } ], \"properties\" : { \"templatedProperty_x\" : {\"fieldName\" : \" Field name for the property \", \"description\" : \" Description for the property \", \"defaultValue\" : \" Default value for the property \", \"options\" : [\" option_1 \", \" option_n \"]} } } ] } } The following parameters are configured: Template Group basic data The following parameters are configured under templateGroup . Parameter Description Required/Optional name A name for the template group Required uuid A uniquely identifiable id for the template group Required description A description for the template. Optional Rule Template details Multiple rule templates can be defined under a templateGroup . For each ruleTemplate , the following set of parameters need to be configured: Parameter Description Required/Optional name A name for the rule template Required uuid A uniquely identifiable id for the rule template Required type The type of the rule template. Possible values are as follows: template: Used only to create an entire business rule from template input : Used only in creating a business rule from scratch output : Used only in creating a business rule from scratch Required instanceCount This specifies whether the business rules derived from the template can be deployed only on one node, or whether they can be deployed on many nodes. Possible values are as follows: one many Required script The Java script to be executed on the templated fields. Developers can use this script for: validating purposes. deriving values for a templated parameter by combining some other entered parameters Each templated element that is going to be derived from entered parameters, has to be mentioned as a variable in the global scope of the javascript. The entered parameters should be templated in the script itself, and will be later replaced with their respective entered values. Consider the following script /* * Validates a number and returns after adding 10 to it * @throws Error when a non number is entered */ function deriveValue (value) { if ( ! isNan (value) ) { return value + 10 ; } throw A number is required ; } var derivedValue = deriveValue ($ { enteredValue } ) ; enteredValue should be defined as a property under properties in order to be filled by the user and replaced later. The derived value stored in derivedValue will be then used to replace ${derivedValue } in the SiddhiApp template. Optional description A brief description of the rule template. Optional templates These are the artifacts (i.e SiddhiApps) with templated parameters, that will be instantiated with replaced values when a business rule is created. Required properties You can add a field name, description, default value and possible values (optional) for the templated parameters. Required Save the template group you created as a .json file in the SP_HOME /wso2/dashboard/resources/businessRules/templates directory. In the BusinessRules section of the SP_HOME /conf/dashboard/deployment.yaml file, add a configuration for the template you created as shown below. wso2.business.rules.manager: datasource: datasourceName - nodeURL1: - ruleTemplateUUID1 - ruleTemplateUUID2 nodeURL2: - ruleTemplateUUID1 - ruleTemplateUUID2 !!! info If you add this configuration, the business rules template is deployed only in the specified nodes when you run the worker and dashboard servers of your SP setup. If you do not add this configuration, the template is deployed in all the worker nodes of your SP set up.","title":"Business Rules Templates"},{"location":"admin/business-Rules-Templates/#business-rules-templates","text":"Rule Templates are used as specifications to gain inputs from users through dynamically generated fields, for creating business rules. A template group is a business domain level grouping. The definition of a template looks as follows. { \"templateGroup\" : { \"name\" : \" Name of the template group \", \"uuid\":\" UUID for the template group \", \"description\" : \" (Optional) description for the template group \", \"ruleTemplates\" : [ { \"name\" : \" Name of the rule template \" , \"uuid\" : \" UUID for the rule template \", \"type\" : \"template\", \"instanceCount\" : \"one or many\", \"description\" : \" (Optional) description for the rule template \", \"script\" : \" (Optional) Javascript with reference to the properties \", \"templates\" : [ { \"type\" : \"siddhiApp\", \"content\" : \" SiddhiApp_1 with ${templatedProperty_x} \" }, { \"type\" : \"siddhiApp\", \"content\" : \" SiddhiApp_n with ${templatedProperty_y} \" } ], \"properties\" : { \"templatedProperty_x\" : {\"fieldName\" : \" Field name for the property \", \"description\" : \" Description for the property \", \"defaultValue\" : \" Default value for the property \"}, \"templatedProperty_y\" : {\"fieldName\" : \" Field name for the property \", \"description\" : \" Description for the property \", \"defaultValue\" : \" Default value for the property \", \"options\" : [\" option_1 \", \" option_n \"]} } }, { \"name\" : \" Name of the rule template \", \"uuid\" : \" UUID for the rule template \", \"type\" : \"input\", \"instanceCount\" : \"one or many\", \"description\" : \" (Optional) description for the rule template \", \"script\" : \" (Optional) Javascript with reference to the properties \", \"templates\" : [ { \"type\" : \"siddhiApp\", \"content\" : \" SiddhiApp with ${templatedProperty_x} \", \"exposedStreamDefinition\" :\" Exposed stream definition \" } ], \"properties\" : { \"templatedProperty_x\" : {\"fieldName\" : \" Field name for the property \", \"description\" : \" Description for the property \", \"defaultValue\" : \" Default value for the property \", \"options\" : [\" option_1 \", \" option_n \"]} } }, { \"name\" : \" Name of the rule template \", \"uuid\" : \" UUID for the rule template \", \"type\" : \"output\", \"instanceCount\" : \"one or many\", \"description\" : \" (Optional) description for the rule template \", \"script\" : \" (Optional) Javascript with reference to the properties \", \"templates\" : [ { \"type\" : \"siddhiApp\", \"content\" : \" SiddhiApp with ${templatedProperty_x} \", \"exposedStreamDefinition\" :\" Exposed stream definition \" } ], \"properties\" : { \"templatedProperty_x\" : {\"fieldName\" : \" Field name for the property \", \"description\" : \" Description for the property \", \"defaultValue\" : \" Default value for the property \", \"options\" : [\" option_1 \", \" option_n \"]} } } ] } } The following parameters are configured:","title":"Business Rules Templates"},{"location":"admin/business-Rules-Templates/#template-group-basic-data","text":"The following parameters are configured under templateGroup . Parameter Description Required/Optional name A name for the template group Required uuid A uniquely identifiable id for the template group Required description A description for the template. Optional","title":"Template Group basic data"},{"location":"admin/business-Rules-Templates/#rule-template-details","text":"Multiple rule templates can be defined under a templateGroup . For each ruleTemplate , the following set of parameters need to be configured: Parameter Description Required/Optional name A name for the rule template Required uuid A uniquely identifiable id for the rule template Required type The type of the rule template. Possible values are as follows: template: Used only to create an entire business rule from template input : Used only in creating a business rule from scratch output : Used only in creating a business rule from scratch Required instanceCount This specifies whether the business rules derived from the template can be deployed only on one node, or whether they can be deployed on many nodes. Possible values are as follows: one many Required script The Java script to be executed on the templated fields. Developers can use this script for: validating purposes. deriving values for a templated parameter by combining some other entered parameters Each templated element that is going to be derived from entered parameters, has to be mentioned as a variable in the global scope of the javascript. The entered parameters should be templated in the script itself, and will be later replaced with their respective entered values. Consider the following script /* * Validates a number and returns after adding 10 to it * @throws Error when a non number is entered */ function deriveValue (value) { if ( ! isNan (value) ) { return value + 10 ; } throw A number is required ; } var derivedValue = deriveValue ($ { enteredValue } ) ; enteredValue should be defined as a property under properties in order to be filled by the user and replaced later. The derived value stored in derivedValue will be then used to replace ${derivedValue } in the SiddhiApp template. Optional description A brief description of the rule template. Optional templates These are the artifacts (i.e SiddhiApps) with templated parameters, that will be instantiated with replaced values when a business rule is created. Required properties You can add a field name, description, default value and possible values (optional) for the templated parameters. Required Save the template group you created as a .json file in the SP_HOME /wso2/dashboard/resources/businessRules/templates directory. In the BusinessRules section of the SP_HOME /conf/dashboard/deployment.yaml file, add a configuration for the template you created as shown below. wso2.business.rules.manager: datasource: datasourceName - nodeURL1: - ruleTemplateUUID1 - ruleTemplateUUID2 nodeURL2: - ruleTemplateUUID1 - ruleTemplateUUID2 !!! info If you add this configuration, the business rules template is deployed only in the specified nodes when you run the worker and dashboard servers of your SP setup. If you do not add this configuration, the template is deployed in all the worker nodes of your SP set up.","title":"Rule Template details"},{"location":"admin/configure-si-profile/","text":"Configuring the Streaming Integration Profile","title":"Configuring the Streaming Integration Profile"},{"location":"admin/configure-si-profile/#configuring-the-streaming-integration-profile","text":"","title":"Configuring the Streaming Integration Profile"},{"location":"admin/configuring-Business-Rule-Deployment/","text":"Configuring Business Rule Deployment WSO2 Business Rules Manager uses ruleTemplates within templateGroups, to derive business rules from. Each ruleTemplate will have a UUID - that is used to uniquely identify itself. When a ruleTemplate is specified under a worker node; SiddhiApps derived in business rules created out of that ruleTemplate will be deployed in the specified worker node. Open the deployment.yaml file in the WSO2SP_HOME /conf/dashboard directory. Deployment configurations for the Business Rules Manager are specified under wso2.business.rules.manager . Provide the URL(s) of worker node(s) that is/are available to deploy SiddhiApps, under deployment_configs in the HOST_NAME : PORT format. deployment_configs: - NODE1_HOST_NAME : NODE1_PORT NODE2_HOST_NAME : NODE2_PORT Eg: deployment_configs: - localhost:9090 10.100.4.140:9090 List down the UUIDs of required rule templates under each node. This results in Siddhi applications created out of the business rules derived from those templates being deployed in the required nodes. deployment_configs: - NODE1_HOST_NAME : NODE1_PORT : - ruleTemplate1_UUID - ruleTemplate2_UUID - ruleTemplate3_UUID e.g., deployment_configs: - localhost:9090: - sweet-production-kpi-analysis - stock-exchange-input - stock-exchange-output If required, you can enter a specific rule template under multiple nodes as shown below. !!! info Before entering a specific rule template under multiple node, make sure that you have selected **Many** for the **Instance Count** field of the template. For more information, see [Creating a Business Rule Template](https://docs.wso2.com/display/SP440/Creating+a+Business+Rule+Template#CreatingaBusinessRuleTemplate-InstanceCount) . deployment_configs: - NODE1_HOST_NAME : NODE1_PORT : - ruleTemplate1_UUID - ruleTemplate2_UUID - ruleTemplate3_UUID NODE2_HOST_NAME : NODE2_PORT : - ruleTemplate1_UUID - ruleTemplate3_UUID NODE3_HOST_NAME : NODE3_PORT : - ruleTemplate2_UUID - ruleTemplate3_UUID - ruleTemplate4_UUID e.g., deployment_configs: - localhost:9090: - sweet-production-kpi-analysis - stock-exchange-input - stock-exchange-output 10.100.40.169:9090: - identifying-continuous-production-decrease - sweet-production-kpi-analysis Note that the rule template with the sweet-production-kpi-analysis UUID has been configured under two worker nodes. As indicated by this, if a business rule is derived from sweet-production-kpi-analysis , Siddhi Applications created from it are deployed in both the nodes. Specify the username and password that are common for all the worker nodes. username: admin password: admin The complete deployment configuration for Business Rules looks as follows. wso2.business.rules.manager: datasource: BUSINESS_RULES_DB deployment_configs: - localhost:9090: - stock-data-analysis - stock-exchange-input - stock-exchange-output - identifying-continuous-production-decrease - sweet-production-kpi-analysis username: admin password: admin","title":"Configuring Business Rule Deployment"},{"location":"admin/configuring-Business-Rule-Deployment/#configuring-business-rule-deployment","text":"WSO2 Business Rules Manager uses ruleTemplates within templateGroups, to derive business rules from. Each ruleTemplate will have a UUID - that is used to uniquely identify itself. When a ruleTemplate is specified under a worker node; SiddhiApps derived in business rules created out of that ruleTemplate will be deployed in the specified worker node. Open the deployment.yaml file in the WSO2SP_HOME /conf/dashboard directory. Deployment configurations for the Business Rules Manager are specified under wso2.business.rules.manager . Provide the URL(s) of worker node(s) that is/are available to deploy SiddhiApps, under deployment_configs in the HOST_NAME : PORT format. deployment_configs: - NODE1_HOST_NAME : NODE1_PORT NODE2_HOST_NAME : NODE2_PORT Eg: deployment_configs: - localhost:9090 10.100.4.140:9090 List down the UUIDs of required rule templates under each node. This results in Siddhi applications created out of the business rules derived from those templates being deployed in the required nodes. deployment_configs: - NODE1_HOST_NAME : NODE1_PORT : - ruleTemplate1_UUID - ruleTemplate2_UUID - ruleTemplate3_UUID e.g., deployment_configs: - localhost:9090: - sweet-production-kpi-analysis - stock-exchange-input - stock-exchange-output If required, you can enter a specific rule template under multiple nodes as shown below. !!! info Before entering a specific rule template under multiple node, make sure that you have selected **Many** for the **Instance Count** field of the template. For more information, see [Creating a Business Rule Template](https://docs.wso2.com/display/SP440/Creating+a+Business+Rule+Template#CreatingaBusinessRuleTemplate-InstanceCount) . deployment_configs: - NODE1_HOST_NAME : NODE1_PORT : - ruleTemplate1_UUID - ruleTemplate2_UUID - ruleTemplate3_UUID NODE2_HOST_NAME : NODE2_PORT : - ruleTemplate1_UUID - ruleTemplate3_UUID NODE3_HOST_NAME : NODE3_PORT : - ruleTemplate2_UUID - ruleTemplate3_UUID - ruleTemplate4_UUID e.g., deployment_configs: - localhost:9090: - sweet-production-kpi-analysis - stock-exchange-input - stock-exchange-output 10.100.40.169:9090: - identifying-continuous-production-decrease - sweet-production-kpi-analysis Note that the rule template with the sweet-production-kpi-analysis UUID has been configured under two worker nodes. As indicated by this, if a business rule is derived from sweet-production-kpi-analysis , Siddhi Applications created from it are deployed in both the nodes. Specify the username and password that are common for all the worker nodes. username: admin password: admin The complete deployment configuration for Business Rules looks as follows. wso2.business.rules.manager: datasource: BUSINESS_RULES_DB deployment_configs: - localhost:9090: - stock-data-analysis - stock-exchange-input - stock-exchange-output - identifying-continuous-production-decrease - sweet-production-kpi-analysis username: admin password: admin","title":"Configuring Business Rule Deployment"},{"location":"admin/configuring-Business-Rules-Manager-Permissions/","text":"Configuring Business Rules Manager Permissions There are two permission levels for a business rules application: Manager : User roles with this permission level have administrative privileges over business rules. They are allowed to create, view, edit, deploy or delete business rules. Viewer : User roles with this permission level are only allowed to view business rules. The following topics cover how to configure Business Rules Manager permissions. Prerequisites Configuring permissions Prerequisites Before configuring Business Rules Manager permissions, the user roles to be assigned permissions must be already defined in the user store with the required user IDs. For detailed instructions, see User Management . Configuring permissions Roles related to the Business Rules Manager need to be added under the wso2.business.rules.manager component namespace in the SP_HOME /conf/ dashboard/deployment.yaml file. The following is a sample configuration of user roles for the Business Rules Manager. wso2.business.rules.manager: roles: manager: - name: role1 id: 1 viewer: - name: role2 id: 2","title":"Configuring Business Rules Manager Permissions"},{"location":"admin/configuring-Business-Rules-Manager-Permissions/#configuring-business-rules-manager-permissions","text":"There are two permission levels for a business rules application: Manager : User roles with this permission level have administrative privileges over business rules. They are allowed to create, view, edit, deploy or delete business rules. Viewer : User roles with this permission level are only allowed to view business rules. The following topics cover how to configure Business Rules Manager permissions. Prerequisites Configuring permissions","title":"Configuring Business Rules Manager Permissions"},{"location":"admin/configuring-Business-Rules-Manager-Permissions/#prerequisites","text":"Before configuring Business Rules Manager permissions, the user roles to be assigned permissions must be already defined in the user store with the required user IDs. For detailed instructions, see User Management .","title":"Prerequisites"},{"location":"admin/configuring-Business-Rules-Manager-Permissions/#configuring-permissions","text":"Roles related to the Business Rules Manager need to be added under the wso2.business.rules.manager component namespace in the SP_HOME /conf/ dashboard/deployment.yaml file. The following is a sample configuration of user roles for the Business Rules Manager. wso2.business.rules.manager: roles: manager: - name: role1 id: 1 viewer: - name: role2 id: 2","title":"Configuring permissions"},{"location":"admin/configuring-Cluster-Coordination/","text":"Configuring Cluster Coordination Multiple WSO2 SP nodes can be configured to work together by configuring a cluster coordination strategy that is used in various deployments such as the Minimum HA Deployment and Fully Distributed Deployment . At present, cluster coordination is supported via an RDBMS instance using and RDBMS coordination strategy. Support for cluster coordination via a Zookeeper instance will be supported in the near future. At any given time, there is a leader in an SP cluster that is arbitrarily selected among the members of the cluster. The RDBMS coordination strategy that is used for cluster coordination works on the concept of heartbeats where the members of the cluster periodically send heartbeat signals via the datasource to the leader of the cluster. If the leader node does not detect a pre configured consecutive number of heartbeats from a specific node, the relevant node is removed from the cluster. Simillarly, if the leader node fails to update its heartbeat, the cluster re-elects a new leader. Prerequisites In order to configure a cluster, the following prerequisites must be completed: A minimum of two binary packs of WSO2 SP must be available. A working RDBMS instance must be available to be shared among the nodes of the cluster. !!! info Currently, we only support MySQL. Support for other databases will be added soon. Configuring the Cluster with the RDBMS coordination strategy To configure a cluster for several nodes, the cluster.config section of the SP_HOME /conf/ worker|manager /deployment.yaml should be configured for all the nodes as follows: Parameter Purpose Sample Values enabled Set this value to true to enable cluster coordination for the node. true/false groupId The group ID is used to identify the cluster to which the node belongs. Nodes that belong to the same cluster must be configured with the same group ID. group-1 coordinationStrategyClass The clustering class to be used. org.wso2.carbon.cluster.coordinator.rdbms.RDBMSCoordinationStrategy strategyConfig datasource The shared datasource to be used in the cluster. The datasource specified must be properly configured in the deployment.yaml file. For detailed instructions to configure a datasource, see Configuring Datasources . WSO2_CARBON_DB strategyConfig heartbeatInterval This value defines the time interval in milliseconds between heartbeat pulses sent by nodes to indicate that they are still alive within the cluster. 1000 strategyConfig heartbeatMaxRetry The number of times the heartbeat pulse can be unavailable until a node is identified as unresponsive. If a node fails to send its heartbeat pulse to the leader of the cluster after a number of retries equal to the number specified here, that node is removed from the cluster. 2 strategyConfig eventPollingInterval The time interval in millseconds at which a node listens to identify the changes happening within the cluster. The changes may include a new node joining the cluster, a node being removed from the cluster and the coordinator changed event. 1000 Following is a sample segment of the configurations needed for RDBMS coordination in the deployment.yaml Sample deployment.yaml segment cluster.config: enabled: true groupId: GROUP ID coordinationStrategyClass: org.wso2.carbon.cluster.coordinator.rdbms.RDBMSCoordinationStrategy strategyConfig: datasource: DATASOURCE NAME heartbeatInterval: 1000 heartbeatMaxRetry: 2 eventPollingInterval: 1000","title":"Configuring Cluster Coordination"},{"location":"admin/configuring-Cluster-Coordination/#configuring-cluster-coordination","text":"Multiple WSO2 SP nodes can be configured to work together by configuring a cluster coordination strategy that is used in various deployments such as the Minimum HA Deployment and Fully Distributed Deployment . At present, cluster coordination is supported via an RDBMS instance using and RDBMS coordination strategy. Support for cluster coordination via a Zookeeper instance will be supported in the near future. At any given time, there is a leader in an SP cluster that is arbitrarily selected among the members of the cluster. The RDBMS coordination strategy that is used for cluster coordination works on the concept of heartbeats where the members of the cluster periodically send heartbeat signals via the datasource to the leader of the cluster. If the leader node does not detect a pre configured consecutive number of heartbeats from a specific node, the relevant node is removed from the cluster. Simillarly, if the leader node fails to update its heartbeat, the cluster re-elects a new leader.","title":"Configuring Cluster Coordination"},{"location":"admin/configuring-Cluster-Coordination/#prerequisites","text":"In order to configure a cluster, the following prerequisites must be completed: A minimum of two binary packs of WSO2 SP must be available. A working RDBMS instance must be available to be shared among the nodes of the cluster. !!! info Currently, we only support MySQL. Support for other databases will be added soon.","title":"Prerequisites"},{"location":"admin/configuring-Cluster-Coordination/#configuring-the-cluster-with-the-rdbms-coordination-strategy","text":"To configure a cluster for several nodes, the cluster.config section of the SP_HOME /conf/ worker|manager /deployment.yaml should be configured for all the nodes as follows: Parameter Purpose Sample Values enabled Set this value to true to enable cluster coordination for the node. true/false groupId The group ID is used to identify the cluster to which the node belongs. Nodes that belong to the same cluster must be configured with the same group ID. group-1 coordinationStrategyClass The clustering class to be used. org.wso2.carbon.cluster.coordinator.rdbms.RDBMSCoordinationStrategy strategyConfig datasource The shared datasource to be used in the cluster. The datasource specified must be properly configured in the deployment.yaml file. For detailed instructions to configure a datasource, see Configuring Datasources . WSO2_CARBON_DB strategyConfig heartbeatInterval This value defines the time interval in milliseconds between heartbeat pulses sent by nodes to indicate that they are still alive within the cluster. 1000 strategyConfig heartbeatMaxRetry The number of times the heartbeat pulse can be unavailable until a node is identified as unresponsive. If a node fails to send its heartbeat pulse to the leader of the cluster after a number of retries equal to the number specified here, that node is removed from the cluster. 2 strategyConfig eventPollingInterval The time interval in millseconds at which a node listens to identify the changes happening within the cluster. The changes may include a new node joining the cluster, a node being removed from the cluster and the coordinator changed event. 1000 Following is a sample segment of the configurations needed for RDBMS coordination in the deployment.yaml Sample deployment.yaml segment cluster.config: enabled: true groupId: GROUP ID coordinationStrategyClass: org.wso2.carbon.cluster.coordinator.rdbms.RDBMSCoordinationStrategy strategyConfig: datasource: DATASOURCE NAME heartbeatInterval: 1000 heartbeatMaxRetry: 2 eventPollingInterval: 1000","title":"Configuring the Cluster with the RDBMS coordination strategy"},{"location":"admin/configuring-Database-and-File-System-State-Persistence/","text":"Configuring Database and File System State Persistence This section explains how to prevent the loss of data that can result from a system failure by persisting the state of WSO2 SP periodically either into a database system or into the file system. Prerequisites Before configuring database persistence, the following prerequisites must be completed. One or more Siddhi Applications must be running in the WSO2 SP server. A working RDBMS instance that can be used for data persistence must exist. The requirements of the datasource must be already defined . Database persistence involves updating the databases connected to WSO2 Steam Processor with the latest information relating to the events that are being processed by WSO2 SP at a given time. Configuring database system persistence The supported databases are H2, MySQL, Postgres, MSSQL and Oracle. The relevant jdbc driver jar should be downloaded and added to the SP_HOME /lib directory to prior to using database system persistence. To configure periodic data persistence, update the SP_HOME /conf/worker/deployment.yaml file under state.persistence as follows: Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. When a new persist takes place, the old revisions are removed. 3 persistenceStore The persistence store . org.wso2.carbon.stream.processor.core.persistence.DBPersistenceStore config datasource The datasource to be used in persisting the state. The provided datasource should be properly defined in the deployment.yaml. For detailed instructions of how to configure a datasource, see Configuring Datasources . WSO2_PERSISTENCE_DB (Datasource with this name should be defined in wso2.datasources) config table The table that should be created and used for the persisting of the state. PERSISTENCE_TABLE The following is a sample segment of the required configurations in the SP_HOME /conf/worker/deployment.yaml file to configure file system persistence. Sample deployment.yaml segment state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 3 persistenceStore: org.wso2.carbon.stream.processor.core.persistence.DBPersistenceStore config: datasource: DATASOURCE NAME # A datasource with this name should be defined in wso2.datasources namespace table: TABLE NAME Configuring file system persistence This section explains how to persist the states of Siddhi applications during a required time interval in the file system in order to maintain back-ups. To configure state persistence, update the SP_HOME /conf/worker/deployment.yaml file under state .p ersistence as follows: Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. When a new persist takes place, the old revisions are removed. 3 persistenceStore The persistence store. org.wso2.carbon.stream.processor.core.persistence.FileSystemPersistenceStore config location A fully qualified folder location to where the revision files should be persisted. siddhi-app-persistence The following is a sample segment of the required configurations in the SP_HOME /conf/worker/deployment.yaml file to configure file system persistence. Sample deployment.yaml segment state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: org.wso2.carbon.stream.processor.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence","title":"Configuring Database and File System State Persistence"},{"location":"admin/configuring-Database-and-File-System-State-Persistence/#configuring-database-and-file-system-state-persistence","text":"This section explains how to prevent the loss of data that can result from a system failure by persisting the state of WSO2 SP periodically either into a database system or into the file system.","title":"Configuring Database and File System State Persistence"},{"location":"admin/configuring-Database-and-File-System-State-Persistence/#prerequisites","text":"Before configuring database persistence, the following prerequisites must be completed. One or more Siddhi Applications must be running in the WSO2 SP server. A working RDBMS instance that can be used for data persistence must exist. The requirements of the datasource must be already defined . Database persistence involves updating the databases connected to WSO2 Steam Processor with the latest information relating to the events that are being processed by WSO2 SP at a given time.","title":"Prerequisites"},{"location":"admin/configuring-Database-and-File-System-State-Persistence/#configuring-database-system-persistence","text":"The supported databases are H2, MySQL, Postgres, MSSQL and Oracle. The relevant jdbc driver jar should be downloaded and added to the SP_HOME /lib directory to prior to using database system persistence. To configure periodic data persistence, update the SP_HOME /conf/worker/deployment.yaml file under state.persistence as follows: Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. When a new persist takes place, the old revisions are removed. 3 persistenceStore The persistence store . org.wso2.carbon.stream.processor.core.persistence.DBPersistenceStore config datasource The datasource to be used in persisting the state. The provided datasource should be properly defined in the deployment.yaml. For detailed instructions of how to configure a datasource, see Configuring Datasources . WSO2_PERSISTENCE_DB (Datasource with this name should be defined in wso2.datasources) config table The table that should be created and used for the persisting of the state. PERSISTENCE_TABLE The following is a sample segment of the required configurations in the SP_HOME /conf/worker/deployment.yaml file to configure file system persistence. Sample deployment.yaml segment state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 3 persistenceStore: org.wso2.carbon.stream.processor.core.persistence.DBPersistenceStore config: datasource: DATASOURCE NAME # A datasource with this name should be defined in wso2.datasources namespace table: TABLE NAME","title":"Configuring database system persistence"},{"location":"admin/configuring-Database-and-File-System-State-Persistence/#configuring-file-system-persistence","text":"This section explains how to persist the states of Siddhi applications during a required time interval in the file system in order to maintain back-ups. To configure state persistence, update the SP_HOME /conf/worker/deployment.yaml file under state .p ersistence as follows: Parameter Purpose Required Value enabled This enables data persistence. true intervalInMin The time interval in minutes that defines the interval in which state of Siddhi applications should be persisted 1 revisionsToKeep The number of revisions to keep in the system. When a new persist takes place, the old revisions are removed. 3 persistenceStore The persistence store. org.wso2.carbon.stream.processor.core.persistence.FileSystemPersistenceStore config location A fully qualified folder location to where the revision files should be persisted. siddhi-app-persistence The following is a sample segment of the required configurations in the SP_HOME /conf/worker/deployment.yaml file to configure file system persistence. Sample deployment.yaml segment state.persistence: enabled: true intervalInMin: 1 revisionsToKeep: 2 persistenceStore: org.wso2.carbon.stream.processor.core.persistence.FileSystemPersistenceStore config: location: siddhi-app-persistence","title":"Configuring file system persistence"},{"location":"admin/configuring-Keystores/","text":"Configuring Keystores Default keystore settings in WSO2 products Default keystore settings in WSO2 products WSO2 SP is shipped with two default keystore files stored in the SP_HOME /resources/security/ directory. wso2carbon.jks : client-truststore.jks :","title":"Configuring Keystores"},{"location":"admin/configuring-Keystores/#configuring-keystores","text":"Default keystore settings in WSO2 products","title":"Configuring Keystores"},{"location":"admin/configuring-Keystores/#default-keystore-settings-in-wso2-products","text":"WSO2 SP is shipped with two default keystore files stored in the SP_HOME /resources/security/ directory. wso2carbon.jks : client-truststore.jks :","title":"Default keystore settings in WSO2 products"},{"location":"admin/configuring-Single-Sign-On-for-WSO2-SP/","text":"Configuring Single Sign-On for WSO2 SP Note The functionality described in this section is not yet released. SSO (Single Sign-On) allows you to be authenticated to access one application, and gain access to multiple other applications without having to repeatedly provide your credentials for authentication purposes. This section explains how you can configure single sign-on for the WSO2 Dashboard Portal, Status Dashboard and the Business Rules Manager. Tip Before you begin: Configure the external identity provider (IdP) that you are using for SSO. By default, WSO2 SP uses WSO2 IS (versions 5.4.0 and later) as the Identity Provider. For detailed instructions to configure WSO2 IS for this scenario, see OAuth2 Token Validation and Introspection . If you want to use any other identity provider, make sure that it supports OAuth 2 Dynamic Client Registration, and do the required configurations (which differ based on the IdP). Enabling SSO Testing the SSO configuration Enabling SSO To configure SSO for the WSO2 SP, open the SP_HOME /conf/dashboard/deployment.yaml file and update it as follows: In the auth.configs section, start creating a new entry with a new client type. You need an external IdP client for SSO. Therefore, enter external as the type. auth.configs: type: external To enable SSO, set the ssoEnabled property as shown below. auth.configs: type: external ssoEnabled: true In order to allow SSO to function in your SP setup, you need to set the following properties under the ssoEnabled property. auth.configs: type: external ssoEnabled: true properties: kmDcrUrl: https://localhost:9443/identity/connect/register kmTokenUrl: https://localhost:9443/oauth2 kmUsername: admin kmPassword: admin idpBaseUrl: https://localhost:9443/scim2 idpUsername: admin idpPassword: admin portalAppContext: portal statusDashboardAppContext: monitoring businessRulesAppContext : business-rules databaseName: WSO2_OAUTH_APP_DB cacheTimeout: 900 baseUrl: https://localhost:9643 grantType: authorization_code The purposes of these properties are explained in the table below. Property Default Value Description kmDcrUrl https://localhost:9443/identity/connect/register The Dynamic Client Registration (DCR) endpoint of the key manager in the IdP. kmTokenUrl https://localhost:9443/oauth2 The token endpoint of the key manager in the IdP. kmUsername admin The username for the key manager in the IdP. kmPassword admin The password for the key manager in the IdP. idpBaseUrl https://localhost:9443/scim2 The SCIM2 endpoint of the IdP. idpUsername admin The username for the IdP. idpPassword admin The password for the IdP. portalAppContext portal The application context of the Dashboard Portal application in WSO2 SP. statusDashboardAppContext monitoring The application context of the Status Dashboard application in WSO2 SP. businessRulesAppContext business-rules The application context of the Business Rules application in WSO2 SP. databaseName WSO2_OAUTH_APP_DB The application context of the Business Rules application in WSO2 SP. cacheTimeout 900 The cache timeout for the validity period of the token in seconds. baseUrl https://localhost:9643 The base URL to which the token should be redirected after the code returned from the Authorization Code grant type is used to get the token. grantType authorization_code The grant type used in the OAuth application token request. externalLogoutURL https://localhost:9443/samlsso The URL via which you can llog out from the external IDP provider side in the SSO. Save your changes. Testing the SSO configuration Once the above changes are made, you can start the dashboard server of WSO2 SP and access all the UIs in it with a single sign-on. To try this out, follow the steps below: Start the dashbaord server by issuing one of the following commands: On Windows : dashboard.bat --run On Linux/Mac OS : sh dashboard.sh Access the Dashboard Portal via the following URL. https://localhost:9643/portal In the dialog box that appears to sign in, enter admin as both the user name and the password, and then click LOG IN . Now access the Business Rules Manager via the following URL. https://localhost:9643/b usiness-rules No dialog box appears for the Business Rules Manager. This because you provided your credentials to access the Dashboard Portal, and the activation of SSO makes that sign-in valid for all the UIs accessible via the dashboard profile.","title":"Configuring Single Sign-On for WSO2 SP"},{"location":"admin/configuring-Single-Sign-On-for-WSO2-SP/#configuring-single-sign-on-for-wso2-sp","text":"Note The functionality described in this section is not yet released. SSO (Single Sign-On) allows you to be authenticated to access one application, and gain access to multiple other applications without having to repeatedly provide your credentials for authentication purposes. This section explains how you can configure single sign-on for the WSO2 Dashboard Portal, Status Dashboard and the Business Rules Manager. Tip Before you begin: Configure the external identity provider (IdP) that you are using for SSO. By default, WSO2 SP uses WSO2 IS (versions 5.4.0 and later) as the Identity Provider. For detailed instructions to configure WSO2 IS for this scenario, see OAuth2 Token Validation and Introspection . If you want to use any other identity provider, make sure that it supports OAuth 2 Dynamic Client Registration, and do the required configurations (which differ based on the IdP). Enabling SSO Testing the SSO configuration","title":"Configuring Single Sign-On for WSO2 SP"},{"location":"admin/configuring-Single-Sign-On-for-WSO2-SP/#enabling-sso","text":"To configure SSO for the WSO2 SP, open the SP_HOME /conf/dashboard/deployment.yaml file and update it as follows: In the auth.configs section, start creating a new entry with a new client type. You need an external IdP client for SSO. Therefore, enter external as the type. auth.configs: type: external To enable SSO, set the ssoEnabled property as shown below. auth.configs: type: external ssoEnabled: true In order to allow SSO to function in your SP setup, you need to set the following properties under the ssoEnabled property. auth.configs: type: external ssoEnabled: true properties: kmDcrUrl: https://localhost:9443/identity/connect/register kmTokenUrl: https://localhost:9443/oauth2 kmUsername: admin kmPassword: admin idpBaseUrl: https://localhost:9443/scim2 idpUsername: admin idpPassword: admin portalAppContext: portal statusDashboardAppContext: monitoring businessRulesAppContext : business-rules databaseName: WSO2_OAUTH_APP_DB cacheTimeout: 900 baseUrl: https://localhost:9643 grantType: authorization_code The purposes of these properties are explained in the table below. Property Default Value Description kmDcrUrl https://localhost:9443/identity/connect/register The Dynamic Client Registration (DCR) endpoint of the key manager in the IdP. kmTokenUrl https://localhost:9443/oauth2 The token endpoint of the key manager in the IdP. kmUsername admin The username for the key manager in the IdP. kmPassword admin The password for the key manager in the IdP. idpBaseUrl https://localhost:9443/scim2 The SCIM2 endpoint of the IdP. idpUsername admin The username for the IdP. idpPassword admin The password for the IdP. portalAppContext portal The application context of the Dashboard Portal application in WSO2 SP. statusDashboardAppContext monitoring The application context of the Status Dashboard application in WSO2 SP. businessRulesAppContext business-rules The application context of the Business Rules application in WSO2 SP. databaseName WSO2_OAUTH_APP_DB The application context of the Business Rules application in WSO2 SP. cacheTimeout 900 The cache timeout for the validity period of the token in seconds. baseUrl https://localhost:9643 The base URL to which the token should be redirected after the code returned from the Authorization Code grant type is used to get the token. grantType authorization_code The grant type used in the OAuth application token request. externalLogoutURL https://localhost:9443/samlsso The URL via which you can llog out from the external IDP provider side in the SSO. Save your changes.","title":"Enabling SSO"},{"location":"admin/configuring-Single-Sign-On-for-WSO2-SP/#testing-the-sso-configuration","text":"Once the above changes are made, you can start the dashboard server of WSO2 SP and access all the UIs in it with a single sign-on. To try this out, follow the steps below: Start the dashbaord server by issuing one of the following commands: On Windows : dashboard.bat --run On Linux/Mac OS : sh dashboard.sh Access the Dashboard Portal via the following URL. https://localhost:9643/portal In the dialog box that appears to sign in, enter admin as both the user name and the password, and then click LOG IN . Now access the Business Rules Manager via the following URL. https://localhost:9643/b usiness-rules No dialog box appears for the Business Rules Manager. This because you provided your credentials to access the Dashboard Portal, and the activation of SSO makes that sign-in valid for all the UIs accessible via the dashboard profile.","title":"Testing the SSO configuration"},{"location":"admin/configuring-System-Parameters-for-Siddhi-Extensions/","text":"Configuring System Parameters for Siddhi Extensions The pre-written Siddhi extensions supported by WSO2 Stream Processor are configured with default values for system parameters. If you need to override those values, you can refer to those extensions from the SP_HOME /conf/ RUNTIME /deployment.yaml file and add the system parameters with the required values as key-value pairs. To do this, follow the procedure below: Open the SP_HOME /conf/ RUNTIME /deployment.yaml file. !!! info The ` lt;RUNTIME gt; ` can be ` worker ` or ` editor ` where Siddhi is run. The extensions belong to the Siddhi component. Therefore, to edit the Siddhi component, add a main section to the file named siddhi . Then add a subsection named extensions to indicate that the configurations related to Siddhi extensions as shown below. siddhi: extensions: For each separate extension you want to configure, add a sub-section named extension under the extensions subsection. siddhi: extensions: - extension: Under each extension subsection, add two key-value pairs as follows. Key Value name The name of the extension. e.g., tcp namespace The archetype of the extension. e.g., source !!! info The archetypes of extensions supported are ` source ` , ` sink ` , ` execution ` , ` io ` , ` map ` , ` script ` and ` store ` . Add a subsection named properties to overide the system properties. Then add the system properties with the required values as ke value pairs. as shown below. siddhi: extensions: - extension: name: [extension-name] namespace: [extension-namespace] properties: [key]: [value] Following are examples for overriding default values for system properties. Example 1: Defining host and port for TCP siddhi: extensions: - extension: name: tcp namespace: source properties: host: 0.0.0.0 port: 5511 Example 2: Overwriting the default RDBMS configuration siddhi: extensions: - extension: name: rdbms namespace: store properties: mysql.batchEnable: true mysql.batchSize: 1000 mysql.indexCreateQuery: \"CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}})\" mysql.recordDeleteQuery: \"DELETE FROM {{TABLE_NAME}} {{CONDITION}}\" mysql.recordExistsQuery: \"SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1\"","title":"Configuring System Parameters for Siddhi Extensions"},{"location":"admin/configuring-System-Parameters-for-Siddhi-Extensions/#configuring-system-parameters-for-siddhi-extensions","text":"The pre-written Siddhi extensions supported by WSO2 Stream Processor are configured with default values for system parameters. If you need to override those values, you can refer to those extensions from the SP_HOME /conf/ RUNTIME /deployment.yaml file and add the system parameters with the required values as key-value pairs. To do this, follow the procedure below: Open the SP_HOME /conf/ RUNTIME /deployment.yaml file. !!! info The ` lt;RUNTIME gt; ` can be ` worker ` or ` editor ` where Siddhi is run. The extensions belong to the Siddhi component. Therefore, to edit the Siddhi component, add a main section to the file named siddhi . Then add a subsection named extensions to indicate that the configurations related to Siddhi extensions as shown below. siddhi: extensions: For each separate extension you want to configure, add a sub-section named extension under the extensions subsection. siddhi: extensions: - extension: Under each extension subsection, add two key-value pairs as follows. Key Value name The name of the extension. e.g., tcp namespace The archetype of the extension. e.g., source !!! info The archetypes of extensions supported are ` source ` , ` sink ` , ` execution ` , ` io ` , ` map ` , ` script ` and ` store ` . Add a subsection named properties to overide the system properties. Then add the system properties with the required values as ke value pairs. as shown below. siddhi: extensions: - extension: name: [extension-name] namespace: [extension-namespace] properties: [key]: [value] Following are examples for overriding default values for system properties. Example 1: Defining host and port for TCP siddhi: extensions: - extension: name: tcp namespace: source properties: host: 0.0.0.0 port: 5511 Example 2: Overwriting the default RDBMS configuration siddhi: extensions: - extension: name: rdbms namespace: store properties: mysql.batchEnable: true mysql.batchSize: 1000 mysql.indexCreateQuery: \"CREATE INDEX {{TABLE_NAME}}_INDEX ON {{TABLE_NAME}} ({{INDEX_COLUMNS}})\" mysql.recordDeleteQuery: \"DELETE FROM {{TABLE_NAME}} {{CONDITION}}\" mysql.recordExistsQuery: \"SELECT 1 FROM {{TABLE_NAME}} {{CONDITION}} LIMIT 1\"","title":"Configuring System Parameters for Siddhi Extensions"},{"location":"admin/configuring-system-parameters-for-siddhi-connectors/","text":"Configuring System Parameters for Siddhi Connectors","title":"Configuring system parameters for siddhi connectors"},{"location":"admin/configuring-system-parameters-for-siddhi-connectors/#configuring-system-parameters-for-siddhi-connectors","text":"","title":"Configuring System Parameters for Siddhi Connectors"},{"location":"admin/configuring-the-Status-Dashboard/","text":"Configuring the Status Dashboard The following sections cover the configurations that need to be done in order to view statistics relating to the performance of your WSO2 SP deployment in the status dashboard. Assigning unique carbon IDs to nodes Setting up the database Configuring metrics Configuring cluster credentials Configuring permissions Assigning unique carbon IDs to nodes Carbon metrics uses the carbon ID as the source ID for metrics. Therefore, all the worker nodes are required to have a unique carbon ID defined in the wso2.carbon: section of the SP_HOME /conf/worker/deployment.yaml file as shown in the extract below. Info You need to ensure that the carbon ID of each node is unique because it is required for the Status dashboard to identify the worker nodes and display their statistics accordingly. wso2.carbon: # value to uniquely identify a server id: wso2-sp # server name name: WSO2 Stream Processor # ports used by this server Setting up the database To monitor statistics in the Status Dashboard, you need a shared metrics database that stores the metrics of all the nodes. Set up a database of the required type by following the steps below. In this section, a MySQL database is created as an example. Info The Status Dashboard is only supported with H2, MySQL, MSSQL and Oracle database types. It is configured with the H2 database type by default. If you want to continue to use H2, skip this step. Download and install the required database type. For this example, let's download and install MySQL Server . Download the required database driver. For this example, download the MySQL JDBC driver . Unzip the database driver you downloaded, copy its JAR file ( mysql-connector-java-x.x.xx-bin.jar in this example), and place it in the SP_HOME /lib directory. Enter the following command in a terminal/command window, where username is the username you want to use to access the databases. mysql -u username -p When prompted, specify the password you are using to access the databases with the username you specified. Create two databases named WSO2_METRICS_DB (to store metrics data) and WSO2_STATUS_DASHBOARD_DB (to store statistics) with tables. To create MySQL databases and tables for this example, run the following commands. mysql create database WSO2_METRICS_DB; mysql use WSO2_METRICS_DB; mysql source SP_HOME /wso2/editor/dbscripts/metrics/mysql.sql; mysql grant all on WSO2_METRICS_DB.* TO username@localhost identified by \"password\"; mysql create database WSO2_STATUS_DASHBOARD_DB; mysql use WSO2_STATUS_DASHBOARD_DB; mysql source SP_HOME /wso2/editor/dbscripts/metrics/mysql.sql; mysql grant all on WSO2_STATUS_DASHBOARD_DB.* TO username@localhost identified by \"password\"; Create two datasources named WSO2_METRICS_DB and WSO2_STATUS_DASHBOARD_DB by adding the following datasource configurations under the wso2.datasources: section of the SP_HOME /conf/worker/deployment.yaml file. !!! info The names of the datasources must be thesame as the names of the database tables you created for metrics and statistics. You need to change the values for the ` username ` and ` password ` parameters to the username and password that you are using to access the MySQL database. For detailed information about datasources, see [carbon-datasources](https://github.com/wso2/carbon-datasources/) . WSO2_METRICS_DB - name: WSO2_METRICS_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/WSO2MetricsDB definition: type: RDBMS configuration: jdbcUrl: 'jdbc:mysql://localhost/WSO2_METRICS_DB?useSSL=false' username: root password: root driverClassName: com.mysql.jdbc.Driver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false WSO2_STATUS_DASHBOARD_DB - name: WSO2_STATUS_DASHBOARD_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/wso2_status_dashboard useJndiReference: true definition: type: RDBMS configuration: jdbcUrl: 'jdbc:mysql://localhost/WSO2_STATUS_DASHBOARD_DB?useSSL=false' username: root password: root driverClassName: com.mysql.jdbc.Driver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false The following are sample configurations for database tables when you use other database types supported. {.expand-control-image} Click here to view the sample datasource configurations Database Type Metrics Datasource Dashboard Datasource MSSQL name: WSO2_METRICS_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/WSO2MetricsDB definition: type: RDBMS configuration: jdbcUrl: jdbc:sqlserver://localhost;databaseName=wso2_metrics username: root password: root driverClassName: com.microsoft.sqlserver.jdbc.SQLServerDriver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false name: WSO2_STATUS_DASHBOARD_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/wso2_status_dashboard useJndiReference: true definition: type: RDBMS configuration: jdbcUrl: jdbc:sqlserver://localhost;databaseName=monitoring username: root password: root driverClassName: com.microsoft.sqlserver.jdbc.SQLServerDriver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false Oracle name: WSO2_METRICS_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/WSO2MetricsDB definition: type: RDBMS configuration: jdbcUrl: jdbc:oracle:thin:@localhost:1521/xe username: root password: root driverClassName: oracle.jdbc.driver.OracleDriver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false name: WSO2_STATUS_DASHBOARD_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/wso2_status_dashboard useJndiReference: true definition: type: RDBMS configuration: jdbcUrl: jdbc:oracle:thin:@localhost:1521/xe username: root password: root driverClassName: oracle.jdbc.driver.OracleDriver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false Configuring metrics This section explains how to configure metrics for your status dashboard. Configuring worker metrics To enable metrics and to configure metric-related properties, do the following configurations in the \\ SP_HOME /conf/worker/deployment.yaml file for the required nodes. To enable Carbon metrics, set the enabled property to true under wso2.metrics as shown below. wso2.metrics: enabled: true To enable JDBC reporting, set the Enable JDBC parameter to true in the wso2.metrics.jdbc: => reporting: subsection as shown below. If JDBC reporting is not enabled, only real-time metrics are displayed in the first page of the Status dashboard, and information relating to metrics history is not displayed in the other pages of the dashboard. To render the first entry of the graph, you need to wait for the time duration specified as the pollingPeriod . # Enable JDBC Reporter name: JDBC enabled: true pollingPeriod: 60 Under wso2.metrics.jdbc , configure the following properties to clean up the database entries. wso2.metrics.jdbc: # Data Source Configurations for JDBC Reporters dataSource: # Default Data Source Configuration - JDBC01 # JNDI name of the data source to be used by the JDBC Reporter. # This data source should be defined in a *-datasources.xml file in conf/datasources directory. dataSourceName: java:comp/env/jdbc/WSO2MetricsDB # Schedule regular deletion of metrics data older than a set number of days. # It is recommended that you enable this job to ensure your metrics tables do not get extremely large. # Deleting data older than seven days should be sufficient. scheduledCleanup: # Enable scheduled cleanup to delete Metrics data in the database. enabled: false # The scheduled job will cleanup all data older than the specified days daysToKeep: 7 # This is the period for each cleanup operation in seconds. scheduledCleanupPeriod: 86400 Parameter Default Value Description dataSource JDBC01 dataSource dataSourceName java:comp/env/jdbc/WSO2MetricsDB The name of the datasource used to store metric data. dataSource scheduledCleanup enabled false If this is set to true , metrics data stored in the database is cleared at a specific time interval as scheduled. dataSource scheduledCleanup daysToKeep 3 If scheduled clean-up of metric data is enabled, all metric data in the database that are older than the number of days specified in this parameter are deleted. dataSource scheduledCleanup scheduledCleanupPeriod 86400 This parameter specifies the time interval in seconds at which all metric data stored in the database must be cleared. JVM metrics of which the log level is set to OFF are not measured by default. If you need to monitor one or more of them, add the relevant metric name(s) under the wso2.metrics: => levels subsection as shown in the extract below. As shown below, you also need to mention log4j mode in which the metrics need to be monitored (i.e., OFF , INFO , DEBUG , TRACE , or ALL ). wso2.metrics: # Enable Metrics enabled: true jmx: # Register MBean when initializing Metrics registerMBean: true # MBean Name name: org.wso2.carbon:type=Metrics # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: 'OFF' jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO {.expand-control-image} Click here to view the default metric levels supported... Class loading Property Garbage collector Property Memory Property Operating system load Property Threads Property Default Level Description jvm.threads.count Debug The gauge for showing the number of active and idle threads currently available in the JVM thread pool. jvm.threads.daemon.count Debug The gauge for showing the number of active daemon threads currently available in the JVM thread pool. jvm.threads.blocked.count OFF The gauge for showing the number of threads that are currently blocked in the JVM thread pool. jvm.threads.deadlock.count OFF The gauge for showing the number of threads that are currently deadlocked in the JVM thread pool. jvm.threads.new.count OFF The gauge for showing the number of new threads generated in the JVM thread pool. jvm.threads.runnable.count OFF The gauge for showing the number of runnable threads currently available in the JVM thread pool. jvm.threads.terminated.count OFF The gauge for showing the number of threads terminated from the JVM thread pool since user started running the WSO2 API Manager instance. jvm.threads.timed_waiting.count OFF The gauge for showing the number of threads in the Timed_Waiting state. jvm.threads.waiting.count OFF The gauge for showing the number of threads in the Waiting state in the JVM thread pool. One or more other threads are required to perform certain actions before these threads can proceed with their actions. File descriptor details Property Swap space Property Configuring Siddhi application metrics To enable Siddhi application level metrics for a Siddhi application, you need to add the @app:statistics annotation bellow the Siddhi application name in the Siddhi file as shown in the example below. @App:name('TestMetrics') @app:statistics(reporter = 'jdbc') define stream TestStream (message string); The following are the metrics measured for a Siddhi application. Info The default level after enabling metrics is INFO for all the meytrics listed in the following table. Metric Components to which the metric is applied Latency Windows (per window.find and window.add) Mappers (per sink mapper, source mapper) Queries (per query) Tables (per table insert, find, update, updateOrInsert, delete, contains) Throughput Windows (per window.find and window.add) Mappers (per sink mapper, source mapper) Queries (per query) Tables (per table insert, find, update, updateOrInsert, delete, contains ) Memory Queries (per query) Buffered Events Count Number of events at disruptor Streams (per stream) Number of events produced/received after restart Sources (per source) Sinks (per sink) Configuring cluster credentials In order to access the nodes in a cluster and derive statistics, you need to maintain and share a user name and a password for each node in a SP cluster. This user name and password must be specified in the SP_HOME /conf/dashboard/deployment.yaml file. If you want to secure sensitive information such as the user name and the password, you can encrypt them via WSO2 Secure Vault. To specify the user name and the password to access a node, define them under the wso2.status.dashboard section as shown in the following example. wso2.status.dashboard: workerAccessCredentials: username: 'admin' password: 'admin' To encrypt the user name and the password you defined, define aliases for them as described in Protecting Sensitive Data via the Secure Vault . !!! info This functionality is currently supported only for single tenant environments. Configuring permissions The following are the three levels of permissions that can be granted for the users of WSO2 Stream Processor Status Dashboard. Permission Level Granted permissions SysAdmin Enabling/disabling metrics Adding workers Deleting workers Viewing workers Developers Adding workers Deleting workers Viewing workers Viewers Viewing workers The admin user in the userstore is assigned the SysAdmin permission level by default. To assign different permission levels to different roles, you can list the required roles under the relevant permission level in the wso2.status.dashboard section of the SP_HOME /conf/dashboard/deployment.yaml file as shown in the extract below. wso2.status.dashboard: sysAdminRoles: - role_1 developerRoles: - role_2 viewerRoles: - role_3 Info The display name of the roles given in the configuration must be present in the user store. To configure user store check, User Management .","title":"Configuring the Status Dashboard"},{"location":"admin/configuring-the-Status-Dashboard/#configuring-the-status-dashboard","text":"The following sections cover the configurations that need to be done in order to view statistics relating to the performance of your WSO2 SP deployment in the status dashboard. Assigning unique carbon IDs to nodes Setting up the database Configuring metrics Configuring cluster credentials Configuring permissions","title":"Configuring the Status Dashboard"},{"location":"admin/configuring-the-Status-Dashboard/#assigning-unique-carbon-ids-to-nodes","text":"Carbon metrics uses the carbon ID as the source ID for metrics. Therefore, all the worker nodes are required to have a unique carbon ID defined in the wso2.carbon: section of the SP_HOME /conf/worker/deployment.yaml file as shown in the extract below. Info You need to ensure that the carbon ID of each node is unique because it is required for the Status dashboard to identify the worker nodes and display their statistics accordingly. wso2.carbon: # value to uniquely identify a server id: wso2-sp # server name name: WSO2 Stream Processor # ports used by this server","title":"Assigning unique carbon IDs to nodes"},{"location":"admin/configuring-the-Status-Dashboard/#setting-up-the-database","text":"To monitor statistics in the Status Dashboard, you need a shared metrics database that stores the metrics of all the nodes. Set up a database of the required type by following the steps below. In this section, a MySQL database is created as an example. Info The Status Dashboard is only supported with H2, MySQL, MSSQL and Oracle database types. It is configured with the H2 database type by default. If you want to continue to use H2, skip this step. Download and install the required database type. For this example, let's download and install MySQL Server . Download the required database driver. For this example, download the MySQL JDBC driver . Unzip the database driver you downloaded, copy its JAR file ( mysql-connector-java-x.x.xx-bin.jar in this example), and place it in the SP_HOME /lib directory. Enter the following command in a terminal/command window, where username is the username you want to use to access the databases. mysql -u username -p When prompted, specify the password you are using to access the databases with the username you specified. Create two databases named WSO2_METRICS_DB (to store metrics data) and WSO2_STATUS_DASHBOARD_DB (to store statistics) with tables. To create MySQL databases and tables for this example, run the following commands. mysql create database WSO2_METRICS_DB; mysql use WSO2_METRICS_DB; mysql source SP_HOME /wso2/editor/dbscripts/metrics/mysql.sql; mysql grant all on WSO2_METRICS_DB.* TO username@localhost identified by \"password\"; mysql create database WSO2_STATUS_DASHBOARD_DB; mysql use WSO2_STATUS_DASHBOARD_DB; mysql source SP_HOME /wso2/editor/dbscripts/metrics/mysql.sql; mysql grant all on WSO2_STATUS_DASHBOARD_DB.* TO username@localhost identified by \"password\"; Create two datasources named WSO2_METRICS_DB and WSO2_STATUS_DASHBOARD_DB by adding the following datasource configurations under the wso2.datasources: section of the SP_HOME /conf/worker/deployment.yaml file. !!! info The names of the datasources must be thesame as the names of the database tables you created for metrics and statistics. You need to change the values for the ` username ` and ` password ` parameters to the username and password that you are using to access the MySQL database. For detailed information about datasources, see [carbon-datasources](https://github.com/wso2/carbon-datasources/) . WSO2_METRICS_DB - name: WSO2_METRICS_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/WSO2MetricsDB definition: type: RDBMS configuration: jdbcUrl: 'jdbc:mysql://localhost/WSO2_METRICS_DB?useSSL=false' username: root password: root driverClassName: com.mysql.jdbc.Driver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false WSO2_STATUS_DASHBOARD_DB - name: WSO2_STATUS_DASHBOARD_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/wso2_status_dashboard useJndiReference: true definition: type: RDBMS configuration: jdbcUrl: 'jdbc:mysql://localhost/WSO2_STATUS_DASHBOARD_DB?useSSL=false' username: root password: root driverClassName: com.mysql.jdbc.Driver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false The following are sample configurations for database tables when you use other database types supported. {.expand-control-image} Click here to view the sample datasource configurations Database Type Metrics Datasource Dashboard Datasource MSSQL name: WSO2_METRICS_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/WSO2MetricsDB definition: type: RDBMS configuration: jdbcUrl: jdbc:sqlserver://localhost;databaseName=wso2_metrics username: root password: root driverClassName: com.microsoft.sqlserver.jdbc.SQLServerDriver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false name: WSO2_STATUS_DASHBOARD_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/wso2_status_dashboard useJndiReference: true definition: type: RDBMS configuration: jdbcUrl: jdbc:sqlserver://localhost;databaseName=monitoring username: root password: root driverClassName: com.microsoft.sqlserver.jdbc.SQLServerDriver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false Oracle name: WSO2_METRICS_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/WSO2MetricsDB definition: type: RDBMS configuration: jdbcUrl: jdbc:oracle:thin:@localhost:1521/xe username: root password: root driverClassName: oracle.jdbc.driver.OracleDriver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false name: WSO2_STATUS_DASHBOARD_DB description: The datasource used for dashboard feature jndiConfig: name: jdbc/wso2_status_dashboard useJndiReference: true definition: type: RDBMS configuration: jdbcUrl: jdbc:oracle:thin:@localhost:1521/xe username: root password: root driverClassName: oracle.jdbc.driver.OracleDriver maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false","title":"Setting up the database"},{"location":"admin/configuring-the-Status-Dashboard/#configuring-metrics","text":"This section explains how to configure metrics for your status dashboard.","title":"Configuring metrics"},{"location":"admin/configuring-the-Status-Dashboard/#configuring-worker-metrics","text":"To enable metrics and to configure metric-related properties, do the following configurations in the \\ SP_HOME /conf/worker/deployment.yaml file for the required nodes. To enable Carbon metrics, set the enabled property to true under wso2.metrics as shown below. wso2.metrics: enabled: true To enable JDBC reporting, set the Enable JDBC parameter to true in the wso2.metrics.jdbc: => reporting: subsection as shown below. If JDBC reporting is not enabled, only real-time metrics are displayed in the first page of the Status dashboard, and information relating to metrics history is not displayed in the other pages of the dashboard. To render the first entry of the graph, you need to wait for the time duration specified as the pollingPeriod . # Enable JDBC Reporter name: JDBC enabled: true pollingPeriod: 60 Under wso2.metrics.jdbc , configure the following properties to clean up the database entries. wso2.metrics.jdbc: # Data Source Configurations for JDBC Reporters dataSource: # Default Data Source Configuration - JDBC01 # JNDI name of the data source to be used by the JDBC Reporter. # This data source should be defined in a *-datasources.xml file in conf/datasources directory. dataSourceName: java:comp/env/jdbc/WSO2MetricsDB # Schedule regular deletion of metrics data older than a set number of days. # It is recommended that you enable this job to ensure your metrics tables do not get extremely large. # Deleting data older than seven days should be sufficient. scheduledCleanup: # Enable scheduled cleanup to delete Metrics data in the database. enabled: false # The scheduled job will cleanup all data older than the specified days daysToKeep: 7 # This is the period for each cleanup operation in seconds. scheduledCleanupPeriod: 86400 Parameter Default Value Description dataSource JDBC01 dataSource dataSourceName java:comp/env/jdbc/WSO2MetricsDB The name of the datasource used to store metric data. dataSource scheduledCleanup enabled false If this is set to true , metrics data stored in the database is cleared at a specific time interval as scheduled. dataSource scheduledCleanup daysToKeep 3 If scheduled clean-up of metric data is enabled, all metric data in the database that are older than the number of days specified in this parameter are deleted. dataSource scheduledCleanup scheduledCleanupPeriod 86400 This parameter specifies the time interval in seconds at which all metric data stored in the database must be cleared. JVM metrics of which the log level is set to OFF are not measured by default. If you need to monitor one or more of them, add the relevant metric name(s) under the wso2.metrics: => levels subsection as shown in the extract below. As shown below, you also need to mention log4j mode in which the metrics need to be monitored (i.e., OFF , INFO , DEBUG , TRACE , or ALL ). wso2.metrics: # Enable Metrics enabled: true jmx: # Register MBean when initializing Metrics registerMBean: true # MBean Name name: org.wso2.carbon:type=Metrics # Metrics Levels are organized from most specific to least: # OFF (most specific, no metrics) # INFO # DEBUG # TRACE (least specific, a lot of data) # ALL (least specific, all data) levels: # The root level configured for Metrics rootLevel: INFO # Metric Levels levels: jvm.buffers: 'OFF' jvm.class-loading: INFO jvm.gc: DEBUG jvm.memory: INFO {.expand-control-image} Click here to view the default metric levels supported... Class loading Property Garbage collector Property Memory Property Operating system load Property Threads Property Default Level Description jvm.threads.count Debug The gauge for showing the number of active and idle threads currently available in the JVM thread pool. jvm.threads.daemon.count Debug The gauge for showing the number of active daemon threads currently available in the JVM thread pool. jvm.threads.blocked.count OFF The gauge for showing the number of threads that are currently blocked in the JVM thread pool. jvm.threads.deadlock.count OFF The gauge for showing the number of threads that are currently deadlocked in the JVM thread pool. jvm.threads.new.count OFF The gauge for showing the number of new threads generated in the JVM thread pool. jvm.threads.runnable.count OFF The gauge for showing the number of runnable threads currently available in the JVM thread pool. jvm.threads.terminated.count OFF The gauge for showing the number of threads terminated from the JVM thread pool since user started running the WSO2 API Manager instance. jvm.threads.timed_waiting.count OFF The gauge for showing the number of threads in the Timed_Waiting state. jvm.threads.waiting.count OFF The gauge for showing the number of threads in the Waiting state in the JVM thread pool. One or more other threads are required to perform certain actions before these threads can proceed with their actions. File descriptor details Property Swap space Property","title":"Configuring worker metrics"},{"location":"admin/configuring-the-Status-Dashboard/#configuring-siddhi-application-metrics","text":"To enable Siddhi application level metrics for a Siddhi application, you need to add the @app:statistics annotation bellow the Siddhi application name in the Siddhi file as shown in the example below. @App:name('TestMetrics') @app:statistics(reporter = 'jdbc') define stream TestStream (message string); The following are the metrics measured for a Siddhi application. Info The default level after enabling metrics is INFO for all the meytrics listed in the following table. Metric Components to which the metric is applied Latency Windows (per window.find and window.add) Mappers (per sink mapper, source mapper) Queries (per query) Tables (per table insert, find, update, updateOrInsert, delete, contains) Throughput Windows (per window.find and window.add) Mappers (per sink mapper, source mapper) Queries (per query) Tables (per table insert, find, update, updateOrInsert, delete, contains ) Memory Queries (per query) Buffered Events Count Number of events at disruptor Streams (per stream) Number of events produced/received after restart Sources (per source) Sinks (per sink)","title":"Configuring Siddhi application metrics"},{"location":"admin/configuring-the-Status-Dashboard/#configuring-cluster-credentials","text":"In order to access the nodes in a cluster and derive statistics, you need to maintain and share a user name and a password for each node in a SP cluster. This user name and password must be specified in the SP_HOME /conf/dashboard/deployment.yaml file. If you want to secure sensitive information such as the user name and the password, you can encrypt them via WSO2 Secure Vault. To specify the user name and the password to access a node, define them under the wso2.status.dashboard section as shown in the following example. wso2.status.dashboard: workerAccessCredentials: username: 'admin' password: 'admin' To encrypt the user name and the password you defined, define aliases for them as described in Protecting Sensitive Data via the Secure Vault . !!! info This functionality is currently supported only for single tenant environments.","title":"Configuring cluster credentials"},{"location":"admin/configuring-the-Status-Dashboard/#configuring-permissions","text":"The following are the three levels of permissions that can be granted for the users of WSO2 Stream Processor Status Dashboard. Permission Level Granted permissions SysAdmin Enabling/disabling metrics Adding workers Deleting workers Viewing workers Developers Adding workers Deleting workers Viewing workers Viewers Viewing workers The admin user in the userstore is assigned the SysAdmin permission level by default. To assign different permission levels to different roles, you can list the required roles under the relevant permission level in the wso2.status.dashboard section of the SP_HOME /conf/dashboard/deployment.yaml file as shown in the extract below. wso2.status.dashboard: sysAdminRoles: - role_1 developerRoles: - role_2 viewerRoles: - role_3 Info The display name of the roles given in the configuration must be present in the user store. To configure user store check, User Management .","title":"Configuring permissions"},{"location":"admin/creating-Business-Rules/","text":"Creating Business Rules This section explains how to create a business rule. A business rule can be created from a template or from scratch. Prerequisites Creating a business rule from a template Creating a business rule from scratch Prerequisites In order to create a business rule from a template, the following prerequisites must be completed: Both the Dashboard and Worker runtimes of the Stream Processor server must be started. The templates for business rules must be defined in a template group, and must be configured in the SP_HOME /conf/dashboard/deployment.yaml file. For detailed instructions, see Business Rules Templates . Creating a business rule from a template Creating business rules from an existing template allows you to use sources, sinks and filters already defined and assign variable values to process events. To create a business rule from a template, follow the procedure below: Go to SP_HOME from the terminal and start the Dashboard runtime of WSO2 SP with one of the following commands: On Windows: dashboard.bat --run On Linux/Mac OS: ./ dashboard.sh Start the Worker runtime of WSO2 SP with one of the following commands: On Windows: worker.bat --run On Linux/Mac OS: ./ worker.sh Access the Business Rule Manager via one of the following URLs. Protocol URL Format Example HTTP http:// SP_HOST : HTTP_PORT /business-rules http://0.0.0.0:9090/business-rules HTTPS https:// SP_HOST : HTTPS_PORT /business-rules https://0.0.0.0:9443/business-rules This opens the following: {height=\"150\"} Click Create to open the following page. {width=\"682\" height=\"250\"} Then click From Template to open the Select a Template Group page where the available templates are displayed. Click on the template group that contains the required template to create a rule from it. In this example, the rule is created based on a template in the Sweet Factory template group that is packed with WSO2 SP by default. Therefore, click Sweet Factory to open this template group. {height=\"250\"} In the template group, expand the Rule Template list as shown below, and click on the required template. For this example, click Identify Continuous Production Decrease . {height=\"250\"} If you want to change the rule template from which you want to create the rule, select the required value for the Rule Template field. Enter a name for the business rule in the Business Rule Name field. Enter values for the rest of the fields following the instructions in the UI. !!! info The fields displayed for the rule differ based on the template selected. If you want to save the rule and deploy it later, click Save . If you want to deploy the rule immediately, click Save and Deploy . Creating a business rule from scratch Creating a rule from scratch allows you to define the filter logic for the rule at the time of creating instead of using the filter logic already defined in a template. However, you can select the required source and sink configurations from an existing template. To create a business rule from scratch, follow the procedure below: Start the Dashboard Portal of WSO2 SP with one of the following commands: On Windows: dashboard.bat --run On Linux/Mac OS: sh dashboard.sh Start the Worker runtime of WSO2 SP with one of the following commands: On Windows: worker.bat --run On Linux/Mac OS: ./worker.sh Access the Business Rule Manager via one of the following URLs. Protocol URL Format Example HTTP http:// SP_HOST : HTTP_PORT /business-rules http://0.0.0.0:9090/business-rules HTTPS https:// SP_HOST : HTTPS_PORT /business-rules https://0.0.0.0:9443/business-rules This opens the following: {height=\"150\"} Click Create to open the following page, and then click From Scratch . {height=\"250\"} This opens the Select a Template Group page where the available template groups are displayed as shown in the example below. {width=\"342\" height=\"250\"} Click on the template group from which you want to select the required sources and sinks for your business rule. For this example, click Stock Exchange to open that template group as shown below. {height=\"400\"} Click Input to expand the Input section. Then select the rule template from which the source and input configurations for the business rule must be selected. {height=\"250\"} This displays the list of available sources and the exposed attributes of the selected template as shown below. {width=\"710\"} Click Filters to expand the Filters section, and click + to add a new filter. A table is displayed as shown below. {width=\"813\" height=\"360\"} To define a filter, follow the steps below: In the Attribute field, select the attribute based on which you want to define the filter condition. In the Operator field, select an operator. In the Value/Attribute field, enter the value or the attribute based on which the operator is applied to the attribute you selected in step a . e.g., If you want to filter events where the price is less than 100, select values for the fields as follows: Field Value Attribute price Operator Value/Attribute 100 Once you have defined two or more filters, enter the rule logic in the Rule Logic field using OR , AND and NOT conditions. The examples of how you can use these keywords are explained in the table below. Keyword Example OR 1 OR 2 returns events that match either filter 1 or 2. AND 1 AND 2 returns events that match both filters 1 and 2. NOT NOT 1 returns events that do not match filter 1. Click Output to expand the Output section. Then select the rule template from which the sink and output connfigurations for the business rule must be selected. This displays the section for mapping configurations as shown in the example below. {width=\"700\"} Select the relevant attribute names for the Input column. When publishing the events to which the rule is applied via the selected predefined sink, each input event you select is published with the corresponding name in the Output column. !!! info The output mappings displayed differ based on the output rule template you select. If you want to save the rule and deploy it later, click Save . If you want to deploy the rule immediately, click Save and Deploy .","title":"Creating Business Rules"},{"location":"admin/creating-Business-Rules/#creating-business-rules","text":"This section explains how to create a business rule. A business rule can be created from a template or from scratch. Prerequisites Creating a business rule from a template Creating a business rule from scratch","title":"Creating Business Rules"},{"location":"admin/creating-Business-Rules/#prerequisites","text":"In order to create a business rule from a template, the following prerequisites must be completed: Both the Dashboard and Worker runtimes of the Stream Processor server must be started. The templates for business rules must be defined in a template group, and must be configured in the SP_HOME /conf/dashboard/deployment.yaml file. For detailed instructions, see Business Rules Templates .","title":"Prerequisites"},{"location":"admin/creating-Business-Rules/#creating-a-business-rule-from-a-template","text":"Creating business rules from an existing template allows you to use sources, sinks and filters already defined and assign variable values to process events. To create a business rule from a template, follow the procedure below: Go to SP_HOME from the terminal and start the Dashboard runtime of WSO2 SP with one of the following commands: On Windows: dashboard.bat --run On Linux/Mac OS: ./ dashboard.sh Start the Worker runtime of WSO2 SP with one of the following commands: On Windows: worker.bat --run On Linux/Mac OS: ./ worker.sh Access the Business Rule Manager via one of the following URLs. Protocol URL Format Example HTTP http:// SP_HOST : HTTP_PORT /business-rules http://0.0.0.0:9090/business-rules HTTPS https:// SP_HOST : HTTPS_PORT /business-rules https://0.0.0.0:9443/business-rules This opens the following: {height=\"150\"} Click Create to open the following page. {width=\"682\" height=\"250\"} Then click From Template to open the Select a Template Group page where the available templates are displayed. Click on the template group that contains the required template to create a rule from it. In this example, the rule is created based on a template in the Sweet Factory template group that is packed with WSO2 SP by default. Therefore, click Sweet Factory to open this template group. {height=\"250\"} In the template group, expand the Rule Template list as shown below, and click on the required template. For this example, click Identify Continuous Production Decrease . {height=\"250\"} If you want to change the rule template from which you want to create the rule, select the required value for the Rule Template field. Enter a name for the business rule in the Business Rule Name field. Enter values for the rest of the fields following the instructions in the UI. !!! info The fields displayed for the rule differ based on the template selected. If you want to save the rule and deploy it later, click Save . If you want to deploy the rule immediately, click Save and Deploy .","title":"Creating a business rule from a template"},{"location":"admin/creating-Business-Rules/#creating-a-business-rule-from-scratch","text":"Creating a rule from scratch allows you to define the filter logic for the rule at the time of creating instead of using the filter logic already defined in a template. However, you can select the required source and sink configurations from an existing template. To create a business rule from scratch, follow the procedure below: Start the Dashboard Portal of WSO2 SP with one of the following commands: On Windows: dashboard.bat --run On Linux/Mac OS: sh dashboard.sh Start the Worker runtime of WSO2 SP with one of the following commands: On Windows: worker.bat --run On Linux/Mac OS: ./worker.sh Access the Business Rule Manager via one of the following URLs. Protocol URL Format Example HTTP http:// SP_HOST : HTTP_PORT /business-rules http://0.0.0.0:9090/business-rules HTTPS https:// SP_HOST : HTTPS_PORT /business-rules https://0.0.0.0:9443/business-rules This opens the following: {height=\"150\"} Click Create to open the following page, and then click From Scratch . {height=\"250\"} This opens the Select a Template Group page where the available template groups are displayed as shown in the example below. {width=\"342\" height=\"250\"} Click on the template group from which you want to select the required sources and sinks for your business rule. For this example, click Stock Exchange to open that template group as shown below. {height=\"400\"} Click Input to expand the Input section. Then select the rule template from which the source and input configurations for the business rule must be selected. {height=\"250\"} This displays the list of available sources and the exposed attributes of the selected template as shown below. {width=\"710\"} Click Filters to expand the Filters section, and click + to add a new filter. A table is displayed as shown below. {width=\"813\" height=\"360\"} To define a filter, follow the steps below: In the Attribute field, select the attribute based on which you want to define the filter condition. In the Operator field, select an operator. In the Value/Attribute field, enter the value or the attribute based on which the operator is applied to the attribute you selected in step a . e.g., If you want to filter events where the price is less than 100, select values for the fields as follows: Field Value Attribute price Operator Value/Attribute 100 Once you have defined two or more filters, enter the rule logic in the Rule Logic field using OR , AND and NOT conditions. The examples of how you can use these keywords are explained in the table below. Keyword Example OR 1 OR 2 returns events that match either filter 1 or 2. AND 1 AND 2 returns events that match both filters 1 and 2. NOT NOT 1 returns events that do not match filter 1. Click Output to expand the Output section. Then select the rule template from which the sink and output connfigurations for the business rule must be selected. This displays the section for mapping configurations as shown in the example below. {width=\"700\"} Select the relevant attribute names for the Input column. When publishing the events to which the rule is applied via the selected predefined sink, each input event you select is published with the corresponding name in the Output column. !!! info The output mappings displayed differ based on the output rule template you select. If you want to save the rule and deploy it later, click Save . If you want to deploy the rule immediately, click Save and Deploy .","title":"Creating a business rule from scratch"},{"location":"admin/creating-a-Business-Rule-Template/","text":"Creating a Business Rule Template To create a business template using the Business Template editor, follow the procedure below: Go to SP_HOME from the terminal and Access the Stream Processor Studio via the http:// HOST_NAME : EDITOR_PORT /editor URL. On Windows: editor.bat --run On Linux/Mac OS: ./ editor.sh Access the Business Rules Template Editor via the http:// HOST_NAME : PORT /template-editor URL. !!! info The default URL is ` http://localhost:9390/template-editor ` The Template Editor opens as shown below. There are two views from which you can interact and create a template group. Design view allows you to visualize a template group and interact with it. Code view allows you to interact with a template group by typing content. (For more information about template group structure, see Business Rules Templates .) !!! warning Do not template sensitive information such as passwords in a Siddhi application or expose them directly in a Siddhi application. For detailed instructions to protect sensitive data by obfuscating them, see [Protecting Sensitive Data via the Secure Vault](https://docs.wso2.com/display/SP440/Protecting+Sensitive+Data+via+the+Secure+Vault) . {width=\"1000\"} The following sections explain the two methods of creating a template group. to create a template group. Create from Design View Create from code view Create from Design View To create a business rules template group from the design view, follow the procedure below: Enter a UUID (Universally Unique Identifier), name and a description for the template group as follows. Field Name UUID sweet-factory Name Sweet Factory Description Analyzes Sweet Factory scenarios {width=\"571\"} Expand the first rule template that exists by default, and enter the following details. (Note that, you need to configure the deployment nodes as explained in Prerequisites for Business Rules ) Field Name Value UUID identifying-continuous-production-decrease Name Identify Continuous Production Decrease Description Alert factory managers if the rate of production continuously decreases for a specified time period Type Template Instance Count One {width=\"593\" height=\"612\"} To include a Siddhi application template, expand the first template that is displayed by default, and enter the following Siddhi application template. {height=\"400\"} @App:name('SweetFactory-TrendAnalysis') @source(type='http', @map(type='json')) define stream SweetProductionStream (name string, amount double, factoryId int); @sink(type='log', @map(type='text', @payload(\"\"\" Hi ${username}, Production at Factory {{factoryId}} has gone from {{initalamout}} to {{finalAmount}} in ${timeInterval} seconds!\"\"\"))) define stream ContinousProdReductionStream (factoryId int, initaltime long, finalTime long, initalamout double, finalAmount double); from SweetProductionStream#window.timeBatch(${timeInterval} sec) select factoryId, sum(amount) as amount, currentTimeMillis() as ts insert into ProdRateStream; partition with ( factoryId of ProdRateStream ) begin from every e1=ProdRateStream, e2=ProdRateStream[ts - e1.ts = ${timeRange} and e1.amount amount ]*, e3=ProdRateStream[ts - e1.ts = ${timeRange} and e1.amount amount ] select e1.factoryId, e1.ts as initaltime, e3.ts as finalTime, e1.amount as initalamout, e3.amount as finalAmount insert into ContinousProdReductionStream; end; To add variable attributes to the script, click Add Variables . !!! info A script is a javascript that can be applied when the inputs provided by the business user who uses the template need to be processed before replacing the values for the template variables. e.g., If the average value is not provided, a function within the script can derive it by calculating it from the minimum value and the maximum value provided by the business user. {width=\"400\"} To specify the attributes that need to be considered as variables, select the relevant check boxes under Select templated elements . In this example, you can select the username and timeRange check boxes to to select the attributes with those names as the variables {height=\"250\"} Then click Add Script to update the script with the selected variuables with auto-generated function bodies as shown below. {width=\"616\" height=\"619\"} Edit the script to add the required functions. In this example, let's rename myFunction1(input) to getUsername(email) , and myFunction2(input ) to validateTimeRange(number) . {height=\"400\"} var username = getUsername('${userInputForusername}'); var timeRange = validateTimeRange('${userInputFortimeRange}'); /** * Extracts the username from given email * @returns Extracted username * @param email Provided email */ function getUsername(email) { if (email.match(/\\S+@\\S+/g)) { if (email.match(/\\S+@\\S+/g)[0] === email) { return email.split('@')[0]; } throw 'Invalid email address provided'; } throw 'Invalid email address provided'; } /** * Validates the given value for time range * @returns Processed input * @param input User given value */ function validateTimeRange(number) { if (!isNaN(number) (number 0)) { return number; } else { throw 'A positive number expected for time range'; } } To generate properties, click Generate against Properties . {width=\"185\"} This expands the Properties section as follows. {width=\"473\" height=\"250\"} Enter values for the available properties as follows. For this example, let's enter values as shown in the following table. !!! info A property is defined for each templated attribute (defined in the ` ${templatedElement ` } format) so that it is self descriptive for the business user who uses the template. The values configured for each property is as follows: - **Field Name** : The name with which the templated attribute is displayed to the business user. - **Field Description** : A description of the property for the business user to understand its purpose. - **Default Value** : The value assigned to the property by default. The business user can change this value if required. - **Options** : this is an optional configuration that allows you to define a set of values for a property so that the business user can select the required value from a list. This is useful when the the possible value for the property is a limited set options. Property Field Name Field Description Default Value timeInterval Time interval (in seconds) Production amounts are considered per time interval 6 userInputForusername Manager Email ID Email address to show in greeting example@email.com userInputFortimeRange Time Range (in milliseconds) Time period in which, product amounts are analyzed for decrease 5 {height=\"400\"} To save the template, click the save icon at the top of the page. Create from code view When you use the code view, the same parameters for which you enter values in the design view are represented as JSON keys. For each parameter, you can specify a value against the relevant JSON key as shown in the extract below. {height=\"250\"} When you update the code view with a valid template group definition, the design view is updated simultaneously as shown below. {width=\"885\" height=\"400\"} However, if the content you enter in the code view is an invalid template group, the design view is not updated, and an error is displayed as follows. {height=\"250\"} When an error is detected in the entered template group structure, the Recover button is displayed with the error message. {width=\"602\"} When you click Recover , the code view is receted to the latest detected valid template group definition. At any given time, the design view displays information based on the latest detected valid template group definition. Info It is not recommended to add Siddhi application templates and scripts using the code view because they need to be provided as a single line, and the possible escape characters should be handled carefully.","title":"Creating a Business Rule Template"},{"location":"admin/creating-a-Business-Rule-Template/#creating-a-business-rule-template","text":"To create a business template using the Business Template editor, follow the procedure below: Go to SP_HOME from the terminal and Access the Stream Processor Studio via the http:// HOST_NAME : EDITOR_PORT /editor URL. On Windows: editor.bat --run On Linux/Mac OS: ./ editor.sh Access the Business Rules Template Editor via the http:// HOST_NAME : PORT /template-editor URL. !!! info The default URL is ` http://localhost:9390/template-editor ` The Template Editor opens as shown below. There are two views from which you can interact and create a template group. Design view allows you to visualize a template group and interact with it. Code view allows you to interact with a template group by typing content. (For more information about template group structure, see Business Rules Templates .) !!! warning Do not template sensitive information such as passwords in a Siddhi application or expose them directly in a Siddhi application. For detailed instructions to protect sensitive data by obfuscating them, see [Protecting Sensitive Data via the Secure Vault](https://docs.wso2.com/display/SP440/Protecting+Sensitive+Data+via+the+Secure+Vault) . {width=\"1000\"} The following sections explain the two methods of creating a template group. to create a template group. Create from Design View Create from code view","title":"Creating a Business Rule Template"},{"location":"admin/creating-a-Business-Rule-Template/#create-from-design-view","text":"To create a business rules template group from the design view, follow the procedure below: Enter a UUID (Universally Unique Identifier), name and a description for the template group as follows. Field Name UUID sweet-factory Name Sweet Factory Description Analyzes Sweet Factory scenarios {width=\"571\"} Expand the first rule template that exists by default, and enter the following details. (Note that, you need to configure the deployment nodes as explained in Prerequisites for Business Rules ) Field Name Value UUID identifying-continuous-production-decrease Name Identify Continuous Production Decrease Description Alert factory managers if the rate of production continuously decreases for a specified time period Type Template Instance Count One {width=\"593\" height=\"612\"} To include a Siddhi application template, expand the first template that is displayed by default, and enter the following Siddhi application template. {height=\"400\"} @App:name('SweetFactory-TrendAnalysis') @source(type='http', @map(type='json')) define stream SweetProductionStream (name string, amount double, factoryId int); @sink(type='log', @map(type='text', @payload(\"\"\" Hi ${username}, Production at Factory {{factoryId}} has gone from {{initalamout}} to {{finalAmount}} in ${timeInterval} seconds!\"\"\"))) define stream ContinousProdReductionStream (factoryId int, initaltime long, finalTime long, initalamout double, finalAmount double); from SweetProductionStream#window.timeBatch(${timeInterval} sec) select factoryId, sum(amount) as amount, currentTimeMillis() as ts insert into ProdRateStream; partition with ( factoryId of ProdRateStream ) begin from every e1=ProdRateStream, e2=ProdRateStream[ts - e1.ts = ${timeRange} and e1.amount amount ]*, e3=ProdRateStream[ts - e1.ts = ${timeRange} and e1.amount amount ] select e1.factoryId, e1.ts as initaltime, e3.ts as finalTime, e1.amount as initalamout, e3.amount as finalAmount insert into ContinousProdReductionStream; end; To add variable attributes to the script, click Add Variables . !!! info A script is a javascript that can be applied when the inputs provided by the business user who uses the template need to be processed before replacing the values for the template variables. e.g., If the average value is not provided, a function within the script can derive it by calculating it from the minimum value and the maximum value provided by the business user. {width=\"400\"} To specify the attributes that need to be considered as variables, select the relevant check boxes under Select templated elements . In this example, you can select the username and timeRange check boxes to to select the attributes with those names as the variables {height=\"250\"} Then click Add Script to update the script with the selected variuables with auto-generated function bodies as shown below. {width=\"616\" height=\"619\"} Edit the script to add the required functions. In this example, let's rename myFunction1(input) to getUsername(email) , and myFunction2(input ) to validateTimeRange(number) . {height=\"400\"} var username = getUsername('${userInputForusername}'); var timeRange = validateTimeRange('${userInputFortimeRange}'); /** * Extracts the username from given email * @returns Extracted username * @param email Provided email */ function getUsername(email) { if (email.match(/\\S+@\\S+/g)) { if (email.match(/\\S+@\\S+/g)[0] === email) { return email.split('@')[0]; } throw 'Invalid email address provided'; } throw 'Invalid email address provided'; } /** * Validates the given value for time range * @returns Processed input * @param input User given value */ function validateTimeRange(number) { if (!isNaN(number) (number 0)) { return number; } else { throw 'A positive number expected for time range'; } } To generate properties, click Generate against Properties . {width=\"185\"} This expands the Properties section as follows. {width=\"473\" height=\"250\"} Enter values for the available properties as follows. For this example, let's enter values as shown in the following table. !!! info A property is defined for each templated attribute (defined in the ` ${templatedElement ` } format) so that it is self descriptive for the business user who uses the template. The values configured for each property is as follows: - **Field Name** : The name with which the templated attribute is displayed to the business user. - **Field Description** : A description of the property for the business user to understand its purpose. - **Default Value** : The value assigned to the property by default. The business user can change this value if required. - **Options** : this is an optional configuration that allows you to define a set of values for a property so that the business user can select the required value from a list. This is useful when the the possible value for the property is a limited set options. Property Field Name Field Description Default Value timeInterval Time interval (in seconds) Production amounts are considered per time interval 6 userInputForusername Manager Email ID Email address to show in greeting example@email.com userInputFortimeRange Time Range (in milliseconds) Time period in which, product amounts are analyzed for decrease 5 {height=\"400\"} To save the template, click the save icon at the top of the page.","title":"Create from Design View"},{"location":"admin/creating-a-Business-Rule-Template/#create-from-code-view","text":"When you use the code view, the same parameters for which you enter values in the design view are represented as JSON keys. For each parameter, you can specify a value against the relevant JSON key as shown in the extract below. {height=\"250\"} When you update the code view with a valid template group definition, the design view is updated simultaneously as shown below. {width=\"885\" height=\"400\"} However, if the content you enter in the code view is an invalid template group, the design view is not updated, and an error is displayed as follows. {height=\"250\"} When an error is detected in the entered template group structure, the Recover button is displayed with the error message. {width=\"602\"} When you click Recover , the code view is receted to the latest detected valid template group definition. At any given time, the design view displays information based on the latest detected valid template group definition. Info It is not recommended to add Siddhi application templates and scripts using the code view because they need to be provided as a single line, and the possible escape characters should be handled carefully.","title":"Create from code view"},{"location":"admin/download-and-install-connectors/","text":"Downloading and Installing Connectors","title":"Download and install connectors"},{"location":"admin/download-and-install-connectors/#downloading-and-installing-connectors","text":"","title":"Downloading and Installing Connectors"},{"location":"admin/downloading-and-Installing-Siddhi-Extensions/","text":"Downloading and Installing Siddhi Extensions The Siddhi extensions supported for WSO2 SP are shipped with WSO2 SP by default. If you need to download and install a different version of an extension, it can be downloaded from the Siddhi Extensions Store . To download and install an Siddhi extension, follow the sections below. Downloading Siddhi extensions Installing Siddhi extensions Downloading Siddhi extensions To download the Siddhii extensions, follow the steps below Open the Siddhi Extensions page . The available Siddhi extensions are displayed as follows. {width=\"900\"} Click on the required extension. In this example, let's click on the IBM MQ extension. {height=\"250\"} In the dialog box that appears, enter your e-mail address and click Submit . The extension JAR is downloaded to the default location in your machine (based on your settings). If you are not using the latest version of WSO2 SP/CEP/DAS, and you want to select the version of the extension that matches your current product version, expand Version Support in the left navigator for the selected extension. !!! tip Each extension has a separate **Version Support** navigator item for SP, CEP and DAS. If you need to download an older version of an extension, follow the substeps below. Once you have clicked on the required extension, click on the Older Versions tab. Then click on the link displayed within the tab. {width=\"900\"} You are directed to the maven central page where all the available versions of the extension are listed. {width=\"654\"} Click on the relavent version. It directs you to the download page. To download the bundle, click on it. {width=\"715\" height=\"250\"} Installing Siddhi extensions To install the Siddhi extension in your WSO2 SP pack, place the extension JAR you downloaded in the SP_HOME /lib directory.","title":"Downloading and Installing Siddhi Connectors"},{"location":"admin/downloading-and-Installing-Siddhi-Extensions/#downloading-and-installing-siddhi-extensions","text":"The Siddhi extensions supported for WSO2 SP are shipped with WSO2 SP by default. If you need to download and install a different version of an extension, it can be downloaded from the Siddhi Extensions Store . To download and install an Siddhi extension, follow the sections below. Downloading Siddhi extensions Installing Siddhi extensions","title":"Downloading and Installing Siddhi Extensions"},{"location":"admin/downloading-and-Installing-Siddhi-Extensions/#downloading-siddhi-extensions","text":"To download the Siddhii extensions, follow the steps below Open the Siddhi Extensions page . The available Siddhi extensions are displayed as follows. {width=\"900\"} Click on the required extension. In this example, let's click on the IBM MQ extension. {height=\"250\"} In the dialog box that appears, enter your e-mail address and click Submit . The extension JAR is downloaded to the default location in your machine (based on your settings). If you are not using the latest version of WSO2 SP/CEP/DAS, and you want to select the version of the extension that matches your current product version, expand Version Support in the left navigator for the selected extension. !!! tip Each extension has a separate **Version Support** navigator item for SP, CEP and DAS. If you need to download an older version of an extension, follow the substeps below. Once you have clicked on the required extension, click on the Older Versions tab. Then click on the link displayed within the tab. {width=\"900\"} You are directed to the maven central page where all the available versions of the extension are listed. {width=\"654\"} Click on the relavent version. It directs you to the download page. To download the bundle, click on it. {width=\"715\" height=\"250\"}","title":"Downloading Siddhi extensions"},{"location":"admin/downloading-and-Installing-Siddhi-Extensions/#installing-siddhi-extensions","text":"To install the Siddhi extension in your WSO2 SP pack, place the extension JAR you downloaded in the SP_HOME /lib directory.","title":"Installing Siddhi extensions"},{"location":"admin/editing-a-Business-Template/","text":"Editing a Business Template WSO2 SP allows you to make edits to a business template that you have already created and saved. To edit a template via the Template Editor tool, follow the steps below. Start the WSO2 editor profile by issuing one of the following commands. For Windows: editor.bat For Linux: ./ editor.sh Access the Template Editor via the URL that appears for it in the start up logs as shown in the example below. !!! info The default URL is ` http://localhost:9390/template-editor ` . The Template Editor opens as follows. {width=\"854\"} To open an existing template, click the Open icon in the top panel (marked in the image above). In the Open Template File dialog box, click Choose File and browse for the required template. Once you have selected the template, click Load to open it in the Template Editor. {width=\"547\" height=\"143\"} Edit the template as required. You can update it in the Design View or the Source View as you prefer. For more information, see Creating a Business Rule Template . Save your edits by clicking the Save icon in the top panel.","title":"Editing a Business Template"},{"location":"admin/editing-a-Business-Template/#editing-a-business-template","text":"WSO2 SP allows you to make edits to a business template that you have already created and saved. To edit a template via the Template Editor tool, follow the steps below. Start the WSO2 editor profile by issuing one of the following commands. For Windows: editor.bat For Linux: ./ editor.sh Access the Template Editor via the URL that appears for it in the start up logs as shown in the example below. !!! info The default URL is ` http://localhost:9390/template-editor ` . The Template Editor opens as follows. {width=\"854\"} To open an existing template, click the Open icon in the top panel (marked in the image above). In the Open Template File dialog box, click Choose File and browse for the required template. Once you have selected the template, click Load to open it in the Template Editor. {width=\"547\" height=\"143\"} Edit the template as required. You can update it in the Design View or the Source View as you prefer. For more information, see Creating a Business Rule Template . Save your edits by clicking the Save icon in the top panel.","title":"Editing a Business Template"},{"location":"admin/extending-WSO2-Stream-Processor/","text":"Extending WSO2 Stream Processor The following topics cover the ways in which WSO2 Stream Processor can be extended. Downloading and Installing Siddhi Extensions Writing Custom Siddhi Extensions Configuring System Parameters for Siddhi Extensions","title":"Extending WSO2 Stream Processor"},{"location":"admin/extending-WSO2-Stream-Processor/#extending-wso2-stream-processor","text":"The following topics cover the ways in which WSO2 Stream Processor can be extended. Downloading and Installing Siddhi Extensions Writing Custom Siddhi Extensions Configuring System Parameters for Siddhi Extensions","title":"Extending WSO2 Stream Processor"},{"location":"admin/forget-me-Tool-Overview/","text":"Forget-me Tool Overview The Forget-me tool is shipped with WSO2 SP by default in the SP_HOME /wso2/tools/identity-anonymization-tool-x.x.x directory. If required, you can change the default location of the configurations of this tool or make changes to the default configurations. You can also run the Forget-me tool in the standalone mode. Changing the default configurations location Changing the default configurations of the tool Running the Forget-me tool in the standalone mode Changing the default configurations location You can change the default location of the tool configurations if desired. You may want to do this if you are working with a multi-product environment where you want to manage configurations in a single location for ease of use. Note that this is optional . To change the default configurations location for the embedded tool, do the following: Open the forgetme.sh file found inside the SP_HOME /bin directory. The location path is the value given after -d within the following line. Modify the value after -d to change the location. !!! info The default location path is ` $CARBON_HOME/repository/components/tools/forget-me/conf. ` sh $CARBON_HOME/repository/components/tools/identity-anonymization-tool/bin/forget-me -d $CARBON_HOME/repository/components/tools/identity-anonymization-tool/conf -carbon $CARBON_HOME $@ Changing the default configurations of the tool All configurations related to this tool can be found inside the SP_HOME /wso2/tools/identity-anonymization-tool/conf directory. The default configurations are set up as follows: Read Logs: SP_HOME /wso2/ PROFILE /logs Read Datasource: SP_HOME /conf/ PROFILE deployment.yaml file Default datasources: WSO2_CARBON_DB, WSO2_METRICS_DB , WSO2_PERMISSIONS_DB , WSO2_DASHBOARD_DB , BUSINESS_RULES_DB , SAMPLE_DB , WSO2_STATUS_DASHBOARD_DB Log file name regex : The regex patterns defined in all the files in the SP_HOME /wso2/tools/identity-anonymization-tool/conf/log-config directory are considered. For information on changing these configurations, see Configuring the config.json file in the Product Administration Guide. Running the Forget-me tool in the standalone mode This tool can run standalone and therefore cater to multiple products. This means that if you are using multiple WSO2 products and need to delete the user's identity from all products at once, you can do so by running the tool in standalone mode. For information on how to build and run the Forget-Me tool, see Removing References to Deleted User Identities in WSO2 Products in the WSO2 Administration Guide.","title":"Forget-me Tool Overview"},{"location":"admin/forget-me-Tool-Overview/#forget-me-tool-overview","text":"The Forget-me tool is shipped with WSO2 SP by default in the SP_HOME /wso2/tools/identity-anonymization-tool-x.x.x directory. If required, you can change the default location of the configurations of this tool or make changes to the default configurations. You can also run the Forget-me tool in the standalone mode. Changing the default configurations location Changing the default configurations of the tool Running the Forget-me tool in the standalone mode","title":"Forget-me Tool Overview"},{"location":"admin/forget-me-Tool-Overview/#changing-the-default-configurations-location","text":"You can change the default location of the tool configurations if desired. You may want to do this if you are working with a multi-product environment where you want to manage configurations in a single location for ease of use. Note that this is optional . To change the default configurations location for the embedded tool, do the following: Open the forgetme.sh file found inside the SP_HOME /bin directory. The location path is the value given after -d within the following line. Modify the value after -d to change the location. !!! info The default location path is ` $CARBON_HOME/repository/components/tools/forget-me/conf. ` sh $CARBON_HOME/repository/components/tools/identity-anonymization-tool/bin/forget-me -d $CARBON_HOME/repository/components/tools/identity-anonymization-tool/conf -carbon $CARBON_HOME $@","title":"Changing the default configurations location"},{"location":"admin/forget-me-Tool-Overview/#changing-the-default-configurations-of-the-tool","text":"All configurations related to this tool can be found inside the SP_HOME /wso2/tools/identity-anonymization-tool/conf directory. The default configurations are set up as follows: Read Logs: SP_HOME /wso2/ PROFILE /logs Read Datasource: SP_HOME /conf/ PROFILE deployment.yaml file Default datasources: WSO2_CARBON_DB, WSO2_METRICS_DB , WSO2_PERMISSIONS_DB , WSO2_DASHBOARD_DB , BUSINESS_RULES_DB , SAMPLE_DB , WSO2_STATUS_DASHBOARD_DB Log file name regex : The regex patterns defined in all the files in the SP_HOME /wso2/tools/identity-anonymization-tool/conf/log-config directory are considered. For information on changing these configurations, see Configuring the config.json file in the Product Administration Guide.","title":"Changing the default configurations of the tool"},{"location":"admin/forget-me-Tool-Overview/#running-the-forget-me-tool-in-the-standalone-mode","text":"This tool can run standalone and therefore cater to multiple products. This means that if you are using multiple WSO2 products and need to delete the user's identity from all products at once, you can do so by running the tool in standalone mode. For information on how to build and run the Forget-Me tool, see Removing References to Deleted User Identities in WSO2 Products in the WSO2 Administration Guide.","title":"Running the Forget-me tool in the standalone mode"},{"location":"admin/general-Data-Protection-Regulations-GDPR-for-WSO2-Stream-Processor/","text":"General Data Protection Regulations (GDPR) for WSO2 Stream Processor The General Data Protection Regulation (GDPR) is a new legal framework formalized by the European Union (EU) in 2016. This regulation comes into effect from 28, May 2018, and can affect any organization that processes Personally Identifiable Information (PII) of individuals who live in Europe. Organizations that fail to demonstrate GDPR compliance are subjected to financial penalties. Info Do you want to learn more about GDPR? If you are new to GDPR, we recommend that you take a look at our article series on Creating a Winning GDPR Strategy. Part 1 - Introduction to GDPR Part 2 - 7 Steps for GDPR Compliance Part 3 - Identity and Access Management to the Rescue Part 4 - GDPR Compliant Consent Design For more resources on GDPR, see the white papers, case studies, solution briefs, webinars, and talks published on our WSO2 GDPR homepage . You can also find the original GDPR legal text here . The following topics cover how GDPR-compliancy is achieved in WSO2 Stream Processor. Removing Personally Identifiable Information via the Forget-me Tool Removing References to Deleted User Identities Forget-me Tool Overview Creating GDPR Compliant Siddhi Applications","title":"General Data Protection Regulations (GDPR) for WSO2 Stream Processor"},{"location":"admin/general-Data-Protection-Regulations-GDPR-for-WSO2-Stream-Processor/#general-data-protection-regulations-gdpr-for-wso2-stream-processor","text":"The General Data Protection Regulation (GDPR) is a new legal framework formalized by the European Union (EU) in 2016. This regulation comes into effect from 28, May 2018, and can affect any organization that processes Personally Identifiable Information (PII) of individuals who live in Europe. Organizations that fail to demonstrate GDPR compliance are subjected to financial penalties. Info Do you want to learn more about GDPR? If you are new to GDPR, we recommend that you take a look at our article series on Creating a Winning GDPR Strategy. Part 1 - Introduction to GDPR Part 2 - 7 Steps for GDPR Compliance Part 3 - Identity and Access Management to the Rescue Part 4 - GDPR Compliant Consent Design For more resources on GDPR, see the white papers, case studies, solution briefs, webinars, and talks published on our WSO2 GDPR homepage . You can also find the original GDPR legal text here . The following topics cover how GDPR-compliancy is achieved in WSO2 Stream Processor. Removing Personally Identifiable Information via the Forget-me Tool Removing References to Deleted User Identities Forget-me Tool Overview Creating GDPR Compliant Siddhi Applications","title":"General Data Protection Regulations (GDPR) for WSO2 Stream Processor"},{"location":"admin/general-data-protection-regulations/","text":"General Data Protection Regulations","title":"General Data Protection Regulations"},{"location":"admin/general-data-protection-regulations/#general-data-protection-regulations","text":"","title":"General Data Protection Regulations"},{"location":"admin/introduction-to-User-Management/","text":"Introduction to User Management User management is a mechanism which involves defining and managing users, roles and their access levels in a system. A user management dashboard or console provides system administrators a holistic view of a system's active user sessions, their log-in statuses, the privileges of each user and their activity in the system, enabling the system administrators to make business-critical, real-time security decisions. A typical user management implementation involves a wide range of functionality such as adding/deleting users, controlling user activity through permissions, managing user roles, defining authentication policies, managing external user stores, manual/automatic log-out, resetting user passwords etc. Any user management system has users, roles, user stores and user permissions as its basic components . Users Users are consumers who interact with your organizational applications, databases or any other systems. These users can be a person, a device or another application/program within or outside of the organization's network. Since these users interact with internal systems and access data, the need to define which user is allowed to do what is critical to most security-conscious organizations. This is how the concept of user management developed. Permission A permission is a 'delegation of authority' or a 'right' assigned to a user or a group of users to perform an action on a system. Permissions can be granted to or revoked from a user/user group/user role automatically or by a system administrator. For example, if a user has the permission to log-in to a system , then the permission to log-out is automatically implied without the need of granting it specifically. User Roles A user role is a consolidation of several permissions. Instead of associating permissions with a user, administrator can associate permissions with a user role and assign the role to users. User roles can be reused throughout the system and prevents the overhead of granting multiple permissions to each and every user individually. User Store A user store is a persistent storage where information of the users and/or user roles is stored. User information includes log-in name, password, fist name, last name, e-mail etc. It can be either file based or a database maintained within SP or externally to it. User stores used in SP differs based on the interface(IdP Client) used to interact with the user store. By default, a file based user store maintained in the \\ SP_HOME>/conf/\\ PROFILE>/deployment.yaml file interfaced through 'Local' IdP Client is enabled.","title":"Introduction to User Management"},{"location":"admin/introduction-to-User-Management/#introduction-to-user-management","text":"User management is a mechanism which involves defining and managing users, roles and their access levels in a system. A user management dashboard or console provides system administrators a holistic view of a system's active user sessions, their log-in statuses, the privileges of each user and their activity in the system, enabling the system administrators to make business-critical, real-time security decisions. A typical user management implementation involves a wide range of functionality such as adding/deleting users, controlling user activity through permissions, managing user roles, defining authentication policies, managing external user stores, manual/automatic log-out, resetting user passwords etc. Any user management system has users, roles, user stores and user permissions as its basic components .","title":"Introduction to User Management"},{"location":"admin/introduction-to-User-Management/#users","text":"Users are consumers who interact with your organizational applications, databases or any other systems. These users can be a person, a device or another application/program within or outside of the organization's network. Since these users interact with internal systems and access data, the need to define which user is allowed to do what is critical to most security-conscious organizations. This is how the concept of user management developed.","title":"Users"},{"location":"admin/introduction-to-User-Management/#permission","text":"A permission is a 'delegation of authority' or a 'right' assigned to a user or a group of users to perform an action on a system. Permissions can be granted to or revoked from a user/user group/user role automatically or by a system administrator. For example, if a user has the permission to log-in to a system , then the permission to log-out is automatically implied without the need of granting it specifically.","title":"Permission"},{"location":"admin/introduction-to-User-Management/#user-roles","text":"A user role is a consolidation of several permissions. Instead of associating permissions with a user, administrator can associate permissions with a user role and assign the role to users. User roles can be reused throughout the system and prevents the overhead of granting multiple permissions to each and every user individually.","title":"User Roles"},{"location":"admin/introduction-to-User-Management/#user-store","text":"A user store is a persistent storage where information of the users and/or user roles is stored. User information includes log-in name, password, fist name, last name, e-mail etc. It can be either file based or a database maintained within SP or externally to it. User stores used in SP differs based on the interface(IdP Client) used to interact with the user store. By default, a file based user store maintained in the \\ SP_HOME>/conf/\\ PROFILE>/deployment.yaml file interfaced through 'Local' IdP Client is enabled.","title":"User Store"},{"location":"admin/managing-Business-Rules/","text":"Managing Business Rules This section covers how to view, edit, deploy and delete business rules that are created from a template. Prerequisites Viewing business rules Editing business rules Deploying business rules Undeploying business rules Viewing deployment information Deleting business rules Prerequisites In order to manage business rules, the following prerequisites must be completed: One or more business rules must be already created. For instructions to create a rule, see Creating Business Rules . The Business Rules Manager should be started and accessed by following the procedure below. Start the Dashboard Portal of WSO2 SP with one of the following commands: On Windows: dashboard.bat --run On Linux/Mac OS: ./dashboard.sh Start a worker node with one of the following commands: On Windows: worker.bat --run On Linux/Mac OS: ./worker.sh Access the Business Rule Manager via one of the following URLs. Protocol URL Format Example HTTP http:// SP_HOST : HTTP_PORT /portal http://0.0.0.0:9443/business-rules/ HTTPS https:// SP_HOST : HTTPS_PORT /portal https://0.0.0.0:9443/business-rules/ Viewing business rules Once you start and access the Business Rules Manager, the available business rules are displayed as shown in the example below. {width=\"900\"} To view a business rule, click the icon for viewing (marked in the above image) for the relevant row. This opens the rule as shown in the example below. {width=\"675\" height=\"557\"} Editing business rules Once you start and access the Business Rules Manager, the available business rules are displayed as shown in the example below. {width=\"900\"} To edit a business rule, click the icon for editing (marked in the above image) for the relevant row. This opens the rule as shown in the example below. {width=\"675\" height=\"614\"} Modify values for the parameters displayed as required and click Save . Deploying business rules To deploy a saved business rule in a worker node, follow the steps below: Start the worker node by issuing one of the following commands: On Windows: worker.bat --run On Linux/Mac OS: ./worker.sh Start the dashboard server and access the Business Rules Manager. Click the icon for deploying (marked in the image below) for the relevant row. As a result, a message appears to inform you that the rule is successfully deployed. {width=\"900\"} Undeploying business rules To undeploy a business rule, click the icon for undeploying (marked in the image below) for the relevant row. As a result, a message appears to inform you that the rule is successfully undeployed. {width=\"900\"} Viewing deployment information If you want to view information relating to the deployment of a business rule, click the icon for viewing deployment information (marked in the image below) for the relevant row. {width=\"900\" height=\"223\"} As a result, the deployment information including the host and port of the nodes in which the rule is deployed and the deployment status are displayed as shown in the image below. {width=\"343\" height=\"250\"} Possible deployment statuses are as follows: Saved : The business rule is created, but not yet deployed in any SP node. Deployed : The business rule is created and deployed in all the nodes of the current SP setup. Partially Deployed: The business rule is created and deployed only in some of the nodes in the SP cluster. This status is also assigned when you click Save and Deploy instead of Save at the time you create the business rule . Partially Undeployed: The business rule has been previously deployed and then undeployed only in some of the nodes in the SP cluster. Deleting business rules To delete a business rule, click the icon for deleting (marked in the image below) for the relevant row. A message appears to confirm whether you want to proceed with the deletion. Click Delete in the message. As a result, another message appears to inform you that the rule is successfully deleted. {width=\"900\"}","title":"Managing Business Rules"},{"location":"admin/managing-Business-Rules/#managing-business-rules","text":"This section covers how to view, edit, deploy and delete business rules that are created from a template. Prerequisites Viewing business rules Editing business rules Deploying business rules Undeploying business rules Viewing deployment information Deleting business rules","title":"Managing Business Rules"},{"location":"admin/managing-Business-Rules/#prerequisites","text":"In order to manage business rules, the following prerequisites must be completed: One or more business rules must be already created. For instructions to create a rule, see Creating Business Rules . The Business Rules Manager should be started and accessed by following the procedure below. Start the Dashboard Portal of WSO2 SP with one of the following commands: On Windows: dashboard.bat --run On Linux/Mac OS: ./dashboard.sh Start a worker node with one of the following commands: On Windows: worker.bat --run On Linux/Mac OS: ./worker.sh Access the Business Rule Manager via one of the following URLs. Protocol URL Format Example HTTP http:// SP_HOST : HTTP_PORT /portal http://0.0.0.0:9443/business-rules/ HTTPS https:// SP_HOST : HTTPS_PORT /portal https://0.0.0.0:9443/business-rules/","title":"Prerequisites"},{"location":"admin/managing-Business-Rules/#viewing-business-rules","text":"Once you start and access the Business Rules Manager, the available business rules are displayed as shown in the example below. {width=\"900\"} To view a business rule, click the icon for viewing (marked in the above image) for the relevant row. This opens the rule as shown in the example below. {width=\"675\" height=\"557\"}","title":"Viewing business rules"},{"location":"admin/managing-Business-Rules/#editing-business-rules","text":"Once you start and access the Business Rules Manager, the available business rules are displayed as shown in the example below. {width=\"900\"} To edit a business rule, click the icon for editing (marked in the above image) for the relevant row. This opens the rule as shown in the example below. {width=\"675\" height=\"614\"} Modify values for the parameters displayed as required and click Save .","title":"Editing business rules"},{"location":"admin/managing-Business-Rules/#deploying-business-rules","text":"To deploy a saved business rule in a worker node, follow the steps below: Start the worker node by issuing one of the following commands: On Windows: worker.bat --run On Linux/Mac OS: ./worker.sh Start the dashboard server and access the Business Rules Manager. Click the icon for deploying (marked in the image below) for the relevant row. As a result, a message appears to inform you that the rule is successfully deployed. {width=\"900\"}","title":"Deploying business rules"},{"location":"admin/managing-Business-Rules/#undeploying-business-rules","text":"To undeploy a business rule, click the icon for undeploying (marked in the image below) for the relevant row. As a result, a message appears to inform you that the rule is successfully undeployed. {width=\"900\"}","title":"Undeploying business rules"},{"location":"admin/managing-Business-Rules/#viewing-deployment-information","text":"If you want to view information relating to the deployment of a business rule, click the icon for viewing deployment information (marked in the image below) for the relevant row. {width=\"900\" height=\"223\"} As a result, the deployment information including the host and port of the nodes in which the rule is deployed and the deployment status are displayed as shown in the image below. {width=\"343\" height=\"250\"} Possible deployment statuses are as follows: Saved : The business rule is created, but not yet deployed in any SP node. Deployed : The business rule is created and deployed in all the nodes of the current SP setup. Partially Deployed: The business rule is created and deployed only in some of the nodes in the SP cluster. This status is also assigned when you click Save and Deploy instead of Save at the time you create the business rule . Partially Undeployed: The business rule has been previously deployed and then undeployed only in some of the nodes in the SP cluster.","title":"Viewing deployment information"},{"location":"admin/managing-Business-Rules/#deleting-business-rules","text":"To delete a business rule, click the icon for deleting (marked in the image below) for the relevant row. A message appears to confirm whether you want to proceed with the deletion. Click Delete in the message. As a result, another message appears to inform you that the rule is successfully deleted. {width=\"900\"}","title":"Deleting business rules"},{"location":"admin/monitoring-Stream-Processor/","text":"Monitoring Stream Processor The Status Dashboard of WSO2 SP allows you to monitor the metrics of a stand-alone WSO2 SP instance or a WSO2 SP cluster. This involves monitoring whether all processes of the WSO2 SP setup are working in a healthy manner, monitoring the current status of a SP node, and viewing metrics relating to the history of a node or the cluster. Both JVM level metrics or Siddhi application level metrics can be viewed from the monitoring dashboard. The following sections cover how to configure the Status Dashboard and analyze statistics relating to your WSO2 SP deployment in it. Configuring the Status Dashboard Viewing Statistics App Overview When you open the WSO2 SP Status Dashboard, the Node Overview page is displayed by default. If you want to view all the Siddhi applications deployed in your WSO2 SP setup, click on the App View tab (marked in the image below). The App Overview tab opens and all the Siddhi applications that are currently deployed are displayed as shown in the image below. The status is displayed in green for active Siddhi applications, and in red for inactive Siddhi applications. If no Siddhi applications are deployed in your WSO2 SP setup, the following message is displayed. The Siddhi applications are listed under the deployment mode in which they are deployed (i.e., Single Node Deployment , HA Deployment , and Distributed Deployment ). Info If your WSO2 SP setup is a distributed deployment, only the parent Siddhi applications are displayed in this tab. The following information is displayed for each Siddhi aplication. Siddhi Application : The name of the Siddhi application. Status : This indicates whether the Siddhi application is currently active or inactive. Deployed Time : The time duration that has elapsed since the Siddhi application was deployed in the SP setup. Deployed Node : The host and the port of the SP node in which the Siddhi application is displayed. The purpose of this tab is to check the status of all the Siddhi applications that are currently deploed in the SP setup. If you click on a Siddhi Application under Single Node Deployment or HA Deployment , information specific to that Siddhi application is displayed as explained in Viewing Statistics for Siddhi Applications . If you click on the parent Siddhi application under Distributed Deployment , information specific to that parent Siddhi application is displayed as explained in Viewing Statistics for Parent Siddhi Applications . If you click on a deployed node, information specific to that node is displayed as explained in Viewing Node-specific Pages . ## Viewing Statistics To view the status dashboard, follow the procedure below: Start the dashboard for your worker node by issuing one of the following commands: For Windows: dashboard.bat For Linux : ./dashboard.sh Access the Status Dashboard via the following URL format. https://localhost: SP_DASHBOARD_PORT /sp-status-dashboard e.g., https://0.0.0.0:9643/sp-status-dashboard After login this opens the Status Dashboard with the nodes that you have already added as shown in the example below. I f no nodes are displayed, add the nodes for which you wnt to view statistics by following the steps in Worker Overview - Adding a worker to the dashboard . For a detailed descripion of each page in this dashboard, see the following topics: Node Overview Viewing Node-specific Pages Viewing Worker History Viewing Statistics for Siddhi Applications Viewing Statistics for Parent Siddhi Applications App Overview","title":"Monitoring Stream Processor"},{"location":"admin/monitoring-Stream-Processor/#monitoring-stream-processor","text":"The Status Dashboard of WSO2 SP allows you to monitor the metrics of a stand-alone WSO2 SP instance or a WSO2 SP cluster. This involves monitoring whether all processes of the WSO2 SP setup are working in a healthy manner, monitoring the current status of a SP node, and viewing metrics relating to the history of a node or the cluster. Both JVM level metrics or Siddhi application level metrics can be viewed from the monitoring dashboard. The following sections cover how to configure the Status Dashboard and analyze statistics relating to your WSO2 SP deployment in it. Configuring the Status Dashboard Viewing Statistics","title":"Monitoring Stream Processor"},{"location":"admin/monitoring-Stream-Processor/#app-overview","text":"When you open the WSO2 SP Status Dashboard, the Node Overview page is displayed by default. If you want to view all the Siddhi applications deployed in your WSO2 SP setup, click on the App View tab (marked in the image below). The App Overview tab opens and all the Siddhi applications that are currently deployed are displayed as shown in the image below. The status is displayed in green for active Siddhi applications, and in red for inactive Siddhi applications. If no Siddhi applications are deployed in your WSO2 SP setup, the following message is displayed. The Siddhi applications are listed under the deployment mode in which they are deployed (i.e., Single Node Deployment , HA Deployment , and Distributed Deployment ). Info If your WSO2 SP setup is a distributed deployment, only the parent Siddhi applications are displayed in this tab. The following information is displayed for each Siddhi aplication. Siddhi Application : The name of the Siddhi application. Status : This indicates whether the Siddhi application is currently active or inactive. Deployed Time : The time duration that has elapsed since the Siddhi application was deployed in the SP setup. Deployed Node : The host and the port of the SP node in which the Siddhi application is displayed. The purpose of this tab is to check the status of all the Siddhi applications that are currently deploed in the SP setup. If you click on a Siddhi Application under Single Node Deployment or HA Deployment , information specific to that Siddhi application is displayed as explained in Viewing Statistics for Siddhi Applications . If you click on the parent Siddhi application under Distributed Deployment , information specific to that parent Siddhi application is displayed as explained in Viewing Statistics for Parent Siddhi Applications . If you click on a deployed node, information specific to that node is displayed as explained in Viewing Node-specific Pages . ## Viewing Statistics To view the status dashboard, follow the procedure below: Start the dashboard for your worker node by issuing one of the following commands: For Windows: dashboard.bat For Linux : ./dashboard.sh Access the Status Dashboard via the following URL format. https://localhost: SP_DASHBOARD_PORT /sp-status-dashboard e.g., https://0.0.0.0:9643/sp-status-dashboard After login this opens the Status Dashboard with the nodes that you have already added as shown in the example below. I f no nodes are displayed, add the nodes for which you wnt to view statistics by following the steps in Worker Overview - Adding a worker to the dashboard . For a detailed descripion of each page in this dashboard, see the following topics: Node Overview Viewing Node-specific Pages Viewing Worker History Viewing Statistics for Siddhi Applications Viewing Statistics for Parent Siddhi Applications App Overview","title":"App Overview"},{"location":"admin/monitoring-the-streaming-integrator/","text":"Monitoring the Streaming Integrator","title":"Monitoring the streaming integrator"},{"location":"admin/monitoring-the-streaming-integrator/#monitoring-the-streaming-integrator","text":"","title":"Monitoring the Streaming Integrator"},{"location":"admin/ndex/","text":"SP440 (Stream Processor 4.4.0) Available Pages: Working with Business Rules Creating Business Rules Managing Business Rules Creating a Business Rule Template Editing a Business Template Business Rules Templates Configuring Business Rules Manager Permissions Monitoring Stream Processor Configuring the Status Dashboard Viewing Statistics Node Overview Viewing Node-specific Pages Viewing Worker History Viewing Statistics for Siddhi Applications Viewing Statistics for Parent Siddhi Applications App Overview","title":"SP440 (Stream Processor 4.4.0)"},{"location":"admin/ndex/#sp440-stream-processor-440","text":"","title":"SP440 (Stream Processor 4.4.0)"},{"location":"admin/ndex/#available-pages","text":"Working with Business Rules Creating Business Rules Managing Business Rules Creating a Business Rule Template Editing a Business Template Business Rules Templates Configuring Business Rules Manager Permissions Monitoring Stream Processor Configuring the Status Dashboard Viewing Statistics Node Overview Viewing Node-specific Pages Viewing Worker History Viewing Statistics for Siddhi Applications Viewing Statistics for Parent Siddhi Applications App Overview","title":"Available Pages:"},{"location":"admin/node-Overview/","text":"Node Overview Once you login to the status dashboard, the nodes that are already added to the Status Dashboard are displayed as shown in the following example: Adding a node to the dashboard If no nodes are displayed, you can add the nodes for which you want to view the status by following the procedure below: Click ADD NEW NODE . This opens the following dialog box. {height=\"250\"} 2. Enter the following information in the dialog box and click ADD NODE to add a gadget for the required node in the Node Overview page. 1. In the Host parameter, enter the host ID of the node you want to add. 2. In the Port parameter, enter the port number of the node you want to add. 3. If the node you added is currently unreachable, the following dialog box is displayed. Click either WORKER or MANAGER. If you click WORKER , the node is displayed under Never Reached . If you click Manager , the node is displayed under Distributed Deployments as shown below. Info The following basic details are displayed for each node. CPU Usage : The CPU resources consumed by the SP node out of the available CPU resources in the machine in which it is deployed is expressed as a percentage. Memory Usage : The memory consumed by the node as a percentage of the total memory available in the system. Load Average : Siddhi Apps : The total number of Siddhi applications deployed in the node. Viewing status details The following is a list of sections displayed in the Node Overview page to provide information relating to the status of the nodes. Distributed Deployments View (Example) Description The nodes that are connected in the distributed deployment are displayed under the relevant group ID in the status dashboard (e.g., sp in the above example). Both managers and workers are displayed under separate labels. Managers : The active manager node in the cluster is indicated by a green dot that is displayed with the host name and the port of the node. Similarly, a grey dot is displayed for passive manager nodes in the cluster. Workers : When you add an active manager node, it automatically retrieves the worker node details that are connected with that particular deployment. If the worker node is already registered in the Status Dashboard, you can view the metrics of that node as follows: Purpose To determine whether the request load is efficiently allocated between the nodes of a cluster. To determine whether the cluster has sufficient resources to handle the load of requests. To identify the nodes connected with the particular deployment. Recommended Action If there is a disparity in the CPU usage and the memory consumption of the nodes, redeploy the Siddhi applications between the nodes to balance out the workload. If the CPU and memory are fully used and and the request load is increasing, allocate more resources (e.g., more memory, more nodes, etc.). Clustered nodes View (Example) Description The nodes that are clustered together in a high-availability deployment are displayed under the relevant cluster ID in the Status Dashboard (e.g., under WSO2_A_1 in the above example). The active node in the cluster (i.e., the active worker in a minimum HA cluster or the active manager in a fully distributed cluster) are indicated by a green dot that is displayed with the hostname and the port of the node. Similarly, a grey dot is displayed for passive nodes in the cluster. Purpose This allows you to determine the following: Whether the request load is efficiently allocated between the nodes of a cluster. Whether the cluster has sufficient resources to handle the load of requests. Recommended Action If there is a disparity in the CPU usage and the memory consumption of the nodes, redeploy the Siddhi applications between the nodes to balance out the workload. If the CPU and memory are fully used and and the request load is increasing, allocate more resources (e.g., more memory, more nodes, etc.). Single nodes View (Example) Description This section displays statistics for SP servers that operate as single node setups. Purpose This allows you to compare the performance of single nodes agaisnt each other. Recommended Action If the CPU usage of a node is too high, investigate the causes for it and take corrective action (e.g., undeploy unnecessary Siddhi applications). If any underutilized single nodes are identified, you can either deploy more Siddhi applications thatare currrently deployed in other nodes with a high request load. Alternatively, you can redeploy the Siddhi applications of the underutilized node to other nodes, and then shut it down. Nodes that cannot be reached View (Example) Description When a node is newly added to the Status dashboard and it is unavailable, it is displayed as shown in the above examples. Purpose This allows you to identify nodes that cannot be reached at specific hosts and ports. Recommended Action Check whether the host and port of the node you added is correct. Check whether any authentication errors have occured for the node. Nodes that are currently unavailable View (Example) Description When a node that could be viewed previously is no longer available, its status is displayed in red as shown in the example above. The status displayed for such nodes is applicable for the last time at which the node had been reachable. Purpose This allows you to identify previously available nodes that have become unreachable. Recommended Action Check whether the node is inactive. Check whether any authentication errors have occured for the node. Nodes for which metrics is disabled View (Example) Description When a node for which metrics is disabled is added to the Status dashboard, you can view the number of active and inactive Siddhi applications deployed in it. However, you cannot view the CPU usage, memory usage and the load average. Purpose This allows you to identify nodes for which metrics is not enabled. Recommended Action Enable metrics for the required nodes to view statistics about their status in the Status Dashboard. For instructions to enable metrics, see Monitoring the Stream Processor - Configuring the Status Dashboard . Nodes with JMX reporting disabled View (Example) Description When a node with JMX reporting disabled is added to the Status dashboard, you can view the number of active and inactive Siddhi applications deployed in it. However, you cannot view the CPU usage, memory usage and the load average. Purpose This allows you to identify nodes for which JMX reporting is disabled Recommended Action Enable JMX reporting for the required nodes to view statistics about their status in the Status Dashboard. For instructions to enable JMX reporting, see Monitoring the Stream Processor - Configuring the Status Dashboard . Statistics trends View (Example) Description This dispalys the change that has taken taken place in the CPU usage, memory usage and the load average of nodes since the status was last viewed in the status dashboard. Positive changes are indicated in green (e.g., a decrease in the CPU usage in the above example), and egative changes are indicated in red (an increase in the memory usage and the load average in the above example). Purpose This allows you to view a summary of the performance trends of your SP clusters and single nodes. Recommended Action Based on the performance trend observed, add more resources to your SP clusters/single nodes to handle more events, or shutdown one or more nodes if there is excess resources.","title":"Node Overview"},{"location":"admin/node-Overview/#node-overview","text":"Once you login to the status dashboard, the nodes that are already added to the Status Dashboard are displayed as shown in the following example:","title":"Node Overview"},{"location":"admin/node-Overview/#adding-a-node-to-the-dashboard","text":"If no nodes are displayed, you can add the nodes for which you want to view the status by following the procedure below: Click ADD NEW NODE . This opens the following dialog box. {height=\"250\"} 2. Enter the following information in the dialog box and click ADD NODE to add a gadget for the required node in the Node Overview page. 1. In the Host parameter, enter the host ID of the node you want to add. 2. In the Port parameter, enter the port number of the node you want to add. 3. If the node you added is currently unreachable, the following dialog box is displayed. Click either WORKER or MANAGER. If you click WORKER , the node is displayed under Never Reached . If you click Manager , the node is displayed under Distributed Deployments as shown below. Info The following basic details are displayed for each node. CPU Usage : The CPU resources consumed by the SP node out of the available CPU resources in the machine in which it is deployed is expressed as a percentage. Memory Usage : The memory consumed by the node as a percentage of the total memory available in the system. Load Average : Siddhi Apps : The total number of Siddhi applications deployed in the node.","title":"Adding a node to the dashboard"},{"location":"admin/node-Overview/#viewing-status-details","text":"The following is a list of sections displayed in the Node Overview page to provide information relating to the status of the nodes.","title":"Viewing status details"},{"location":"admin/node-Overview/#distributed-deployments","text":"View (Example) Description The nodes that are connected in the distributed deployment are displayed under the relevant group ID in the status dashboard (e.g., sp in the above example). Both managers and workers are displayed under separate labels. Managers : The active manager node in the cluster is indicated by a green dot that is displayed with the host name and the port of the node. Similarly, a grey dot is displayed for passive manager nodes in the cluster. Workers : When you add an active manager node, it automatically retrieves the worker node details that are connected with that particular deployment. If the worker node is already registered in the Status Dashboard, you can view the metrics of that node as follows: Purpose To determine whether the request load is efficiently allocated between the nodes of a cluster. To determine whether the cluster has sufficient resources to handle the load of requests. To identify the nodes connected with the particular deployment. Recommended Action If there is a disparity in the CPU usage and the memory consumption of the nodes, redeploy the Siddhi applications between the nodes to balance out the workload. If the CPU and memory are fully used and and the request load is increasing, allocate more resources (e.g., more memory, more nodes, etc.).","title":"Distributed Deployments"},{"location":"admin/node-Overview/#clustered-nodes","text":"View (Example) Description The nodes that are clustered together in a high-availability deployment are displayed under the relevant cluster ID in the Status Dashboard (e.g., under WSO2_A_1 in the above example). The active node in the cluster (i.e., the active worker in a minimum HA cluster or the active manager in a fully distributed cluster) are indicated by a green dot that is displayed with the hostname and the port of the node. Similarly, a grey dot is displayed for passive nodes in the cluster. Purpose This allows you to determine the following: Whether the request load is efficiently allocated between the nodes of a cluster. Whether the cluster has sufficient resources to handle the load of requests. Recommended Action If there is a disparity in the CPU usage and the memory consumption of the nodes, redeploy the Siddhi applications between the nodes to balance out the workload. If the CPU and memory are fully used and and the request load is increasing, allocate more resources (e.g., more memory, more nodes, etc.).","title":"Clustered nodes"},{"location":"admin/node-Overview/#single-nodes","text":"View (Example) Description This section displays statistics for SP servers that operate as single node setups. Purpose This allows you to compare the performance of single nodes agaisnt each other. Recommended Action If the CPU usage of a node is too high, investigate the causes for it and take corrective action (e.g., undeploy unnecessary Siddhi applications). If any underutilized single nodes are identified, you can either deploy more Siddhi applications thatare currrently deployed in other nodes with a high request load. Alternatively, you can redeploy the Siddhi applications of the underutilized node to other nodes, and then shut it down.","title":"Single nodes"},{"location":"admin/node-Overview/#nodes-that-cannot-be-reached","text":"View (Example) Description When a node is newly added to the Status dashboard and it is unavailable, it is displayed as shown in the above examples. Purpose This allows you to identify nodes that cannot be reached at specific hosts and ports. Recommended Action Check whether the host and port of the node you added is correct. Check whether any authentication errors have occured for the node.","title":"Nodes that cannot be reached"},{"location":"admin/node-Overview/#nodes-that-are-currently-unavailable","text":"View (Example) Description When a node that could be viewed previously is no longer available, its status is displayed in red as shown in the example above. The status displayed for such nodes is applicable for the last time at which the node had been reachable. Purpose This allows you to identify previously available nodes that have become unreachable. Recommended Action Check whether the node is inactive. Check whether any authentication errors have occured for the node.","title":"Nodes that are currently unavailable"},{"location":"admin/node-Overview/#nodes-for-which-metrics-is-disabled","text":"View (Example) Description When a node for which metrics is disabled is added to the Status dashboard, you can view the number of active and inactive Siddhi applications deployed in it. However, you cannot view the CPU usage, memory usage and the load average. Purpose This allows you to identify nodes for which metrics is not enabled. Recommended Action Enable metrics for the required nodes to view statistics about their status in the Status Dashboard. For instructions to enable metrics, see Monitoring the Stream Processor - Configuring the Status Dashboard .","title":"Nodes for which metrics is disabled"},{"location":"admin/node-Overview/#nodes-with-jmx-reporting-disabled","text":"View (Example) Description When a node with JMX reporting disabled is added to the Status dashboard, you can view the number of active and inactive Siddhi applications deployed in it. However, you cannot view the CPU usage, memory usage and the load average. Purpose This allows you to identify nodes for which JMX reporting is disabled Recommended Action Enable JMX reporting for the required nodes to view statistics about their status in the Status Dashboard. For instructions to enable JMX reporting, see Monitoring the Stream Processor - Configuring the Status Dashboard .","title":"Nodes with JMX reporting disabled"},{"location":"admin/node-Overview/#statistics-trends","text":"View (Example) Description This dispalys the change that has taken taken place in the CPU usage, memory usage and the load average of nodes since the status was last viewed in the status dashboard. Positive changes are indicated in green (e.g., a decrease in the CPU usage in the above example), and egative changes are indicated in red (an increase in the memory usage and the load average in the above example). Purpose This allows you to view a summary of the performance trends of your SP clusters and single nodes. Recommended Action Based on the performance trend observed, add more resources to your SP clusters/single nodes to handle more events, or shutdown one or more nodes if there is excess resources.","title":"Statistics trends"},{"location":"admin/protecting-sensitive-data-via-the secure-vault/","text":"Protecting Sensitive Data via the Secure Vault","title":"Protecting sensitive data via the secure vault"},{"location":"admin/protecting-sensitive-data-via-the secure-vault/#protecting-sensitive-data-via-the-secure-vault","text":"","title":"Protecting Sensitive Data via the Secure Vault"},{"location":"admin/removing-Personally-Identifiable-Information-via-the-Forget-me-Tool/","text":"Removing Personally Identifiable Information via the Forget-me Tool In WSO2 SP, event streams specify the schema for events to be selected into the SP event flow to be processed. This schema can include user IDs and other PII (Personally Identifiable Information) that you want to delete from log files and such. This can be done via the Forget-me Tool . Step 1: Configure the config.json file Step 2: Execute the Forget-me tool Step 1: Configure the config.json file The SP_HOME /wso2/tools/identity-anonymization-tool-x.x.x/conf/config.json file specifies the locations from which persisted data need to be removed. The log-file processor is specified in the configuration file of the Forget-Me tool as shown on the sample below in order to remove data with PII from the logs. If you have configured logs with PII to be saved in another location, you can add it to this list of processors. { \"processors\" : [ \"log-file\" ], \"directories\": [ { \"dir\": \"log-config\", \"type\": \"log-file\", \"processor\" : \"log-file\", \"log-file-path\" : \"logs\", \"log-file-name-regex\" : \"(.)*\" } ] } This extract shows the default configuration of WSO2 SP. SP only saves PII in log files by default. Therefore, this configuration allows the Forget-me tool to delete these logs that are saved in SP_HOME /wso2/ PROFILE /logs directory. Step 2: Execute the Forget-me tool To execute the Forget-me tool, issue the following command pointing to the SP_HOME directory. forget-me -U USERNAME -d CONF_DIR -carbon SP_HOME","title":"Removing Personally Identifiable Information via the Forget-me Tool"},{"location":"admin/removing-Personally-Identifiable-Information-via-the-Forget-me-Tool/#removing-personally-identifiable-information-via-the-forget-me-tool","text":"In WSO2 SP, event streams specify the schema for events to be selected into the SP event flow to be processed. This schema can include user IDs and other PII (Personally Identifiable Information) that you want to delete from log files and such. This can be done via the Forget-me Tool . Step 1: Configure the config.json file Step 2: Execute the Forget-me tool","title":"Removing Personally Identifiable Information via the Forget-me Tool"},{"location":"admin/removing-Personally-Identifiable-Information-via-the-Forget-me-Tool/#step-1-configure-the-configjson-file","text":"The SP_HOME /wso2/tools/identity-anonymization-tool-x.x.x/conf/config.json file specifies the locations from which persisted data need to be removed. The log-file processor is specified in the configuration file of the Forget-Me tool as shown on the sample below in order to remove data with PII from the logs. If you have configured logs with PII to be saved in another location, you can add it to this list of processors. { \"processors\" : [ \"log-file\" ], \"directories\": [ { \"dir\": \"log-config\", \"type\": \"log-file\", \"processor\" : \"log-file\", \"log-file-path\" : \"logs\", \"log-file-name-regex\" : \"(.)*\" } ] } This extract shows the default configuration of WSO2 SP. SP only saves PII in log files by default. Therefore, this configuration allows the Forget-me tool to delete these logs that are saved in SP_HOME /wso2/ PROFILE /logs directory.","title":"Step 1: Configure the config.json file"},{"location":"admin/removing-Personally-Identifiable-Information-via-the-Forget-me-Tool/#step-2-execute-the-forget-me-tool","text":"To execute the Forget-me tool, issue the following command pointing to the SP_HOME directory. forget-me -U USERNAME -d CONF_DIR -carbon SP_HOME","title":"Step 2: Execute the Forget-me tool"},{"location":"admin/removing-References-to-Deleted-User-Identities/","text":"Removing References to Deleted User Identities This section covers how to remove references to deleted user identities in WSO2 SP by running the Forget-me tool . Tip Before you begin Note that this tool is designed to run in offline mode (i.e., the server should be shut down or run on another machine) in order to prevent unnecessary load to the server. If this tool runs in online mode (i.e., when the server is running), DB lock situations on the H2 databases may occur. If you have configured any JDBC database other than the H2 database provided by default, copy the relevant JDBC driver to the SP_HOME /wso2/tools/identity-anonymization-tool/lib directory. Open a new terminal window and navigate to the SP_HOME /bin directory. Execute one of the following commands depending on your operating system: On Linux/Mac OS: ./forgetme.sh -U username On Windows: forgetme.bat -U username Info Note The commands specified above use only the -U username option, which is the only required option to run the tool. There are several other optional command line options that you can specify based on your requirement. The supported options are described in detail below. Info If you specify a tenant domain via this option, use the TID option to specify the ID of which the references must be removed. No -T acme-company The default value is carbon.super TID The tenant ID of the user whose identity references you want to remove. !!! info It is required to specify a tenant ID if you have specified a tenant domain via the TID option. No -TID 2346 D The user store domain name of the user whose identity references you want to remove. No -D Finance-Domain The default value is PRIMARY . pu The pseudonym with which the user name of the user whose identity references you want to remove should be replaced. If you do not specify a pseudonym when you run the tool, a random UUID value is generated as the pseudonym by default. No -pu \u201c123-343-435-545-dfd-4\u201d carbon The CARBON HOME. This should be replaced with the variable $CARBON_HOME in directories configured in the main configuration file. No -carbon \u201c/usr/bin/wso2sp/wso2sp4.1.0","title":"Removing References to Deleted User Identities"},{"location":"admin/removing-References-to-Deleted-User-Identities/#removing-references-to-deleted-user-identities","text":"This section covers how to remove references to deleted user identities in WSO2 SP by running the Forget-me tool . Tip Before you begin Note that this tool is designed to run in offline mode (i.e., the server should be shut down or run on another machine) in order to prevent unnecessary load to the server. If this tool runs in online mode (i.e., when the server is running), DB lock situations on the H2 databases may occur. If you have configured any JDBC database other than the H2 database provided by default, copy the relevant JDBC driver to the SP_HOME /wso2/tools/identity-anonymization-tool/lib directory. Open a new terminal window and navigate to the SP_HOME /bin directory. Execute one of the following commands depending on your operating system: On Linux/Mac OS: ./forgetme.sh -U username On Windows: forgetme.bat -U username Info Note The commands specified above use only the -U username option, which is the only required option to run the tool. There are several other optional command line options that you can specify based on your requirement. The supported options are described in detail below. Info If you specify a tenant domain via this option, use the TID option to specify the ID of which the references must be removed. No -T acme-company The default value is carbon.super TID The tenant ID of the user whose identity references you want to remove. !!! info It is required to specify a tenant ID if you have specified a tenant domain via the TID option. No -TID 2346 D The user store domain name of the user whose identity references you want to remove. No -D Finance-Domain The default value is PRIMARY . pu The pseudonym with which the user name of the user whose identity references you want to remove should be replaced. If you do not specify a pseudonym when you run the tool, a random UUID value is generated as the pseudonym by default. No -pu \u201c123-343-435-545-dfd-4\u201d carbon The CARBON HOME. This should be replaced with the variable $CARBON_HOME in directories configured in the main configuration file. No -carbon \u201c/usr/bin/wso2sp/wso2sp4.1.0","title":"Removing References to Deleted User Identities"},{"location":"admin/supporting-different-transports/","text":"Supporting Different Transports Follow the relevant section for the steps that need to be carried out before using the required transport to receive and publish events via WSO2 SP. Kafka transport JMS transport MQTT transport RabbitMQ transport Kafka transport To enable WSO2 SP to receive and publish events via the Kafka transport, follow the steps below: Download the Kafka broker from here . Convert and copy the Kafka client jars from the KAFKA_HOME /libs directory to the SP_HOME /lib directory as follows. Create a directory in a preferred location in your machine and copy the following JARs to it from the KAFKA_HOME /libs directory. !!! info This directory will be referred to as the ` SOURCE_DIRECTORY ` in the next steps. kafka_2.11-0.10.2.1.jar kafka-clients-0.10.2.1.jar metrics-core-2.2.0.jar scala-library-2.11.8.jar scala-parser-combinators_2.11-1.0.4.jar zkclient-0.10.jar zookeeper-3.4.9.jar Create another directory in a preferred location in your machine. !!! info This directory will be referred to as the ` DESTINATION_DIRECTORY ` in the next steps. To convert all the Kafka jars you copied into the SOURCE_DIRECTORY , issue the following command. For Windows: SP_HOME /bin/jartobundle.bat SOURCE_DIRECTORY_PATH DESTINATION_DIRECTORY_PATH For Linux : SP_HOME /bin/jartobundle.sh SOURCE_DIRECTORY_PATH DESTINATION_DIRECTORY_PATH Copy the converted files from the DESTINATION_DIRECTORY to the SP_HOME /lib directory. Copy the jars that are not converted from the SOURCE_DIRECTORY to the SP_HOME /samples/sample-clients/lib directory. The Kafka server should be started before sending events from WSO2 SP to a Kafka consumer. JMS transport Follow the steps to configure the Apache ActiveMQ message broker: Install Apache ActiveMQ JMS . !!! info This guide uses ActiveMQ versions 5.7.0 - 5.9.0. If you want to use a later version, for instructions on the necessary changes to the configuration steps, go to [Apache ActiveMQ Documentation](http://activemq.apache.org/activemq-580-release.html) . Download the activemq-client-5.x.x.jar from here . Register the InitialContextFactory implementation according to the OSGi JNDI spec and copy the client jar to the SP_HOME /lib directory as follows. Navigate to the SP_HOME /bin directory and issue the following command. For Linux : ./icf-provider.sh org.apache.activemq.jndi.ActiveMQInitialContextFactory Downloaded Jar Path /activemq-client-5.x.x.jar Output Jar Path For Windows : ./icf-provider.bat org.apache.activemq.jndi.ActiveMQInitialContextFactory Downloaded Jar Path \\activemq-client-5.x.x.jar Output Jar Path !!! info If required, you can provide privileges via ` chmod +x icf-provider.(sh|bat) ` . Once the client jar is successfully converted, the activemq-client-5.x.x directory is created. This directory contains the following: activemq-client-5.x.x.jar (original jar) activemq-client-5.x.x_1.0.0.jar (OSGi-converted jar) In addition, the following messages are logged in the terminal. INFO: Executing 'jar uf absolute_path /activemq-client-5.x.x/activemq-client-5.x.x.jar -C absolute_path /activemq-client-5.x.x /internal/CustomBundleActivator.class' [timestamp] org.wso2.carbon.tools.spi.ICFProviderTool addBundleActivatorHeader - INFO: Running jar to bundle conversion [timestamp] org.wso2.carbon.tools.converter.utils.BundleGeneratorUtils convertFromJarToBundle - INFO: Created the OSGi bundle activemq_client_5.x.x_1.0.0.jar for JAR file absolute_path /activemq-client-5.x.x/activemq-client-5.x.x.jar Copy activemq-client-5.x.x/activemq-client-5.x.x.jar and place it in the SP_HOME /samples/sample-clients/lib directory. Copy activemq-client-5.x.x/activemq_client_5.x.x_1.0.0.jar and place it in the SP_HOME /lib directory. Create a directory in a preferred location in your machine and copy the following JARs to it from the ActiveMQ_HOME /libs directory. !!! info This directory is referred to as the ` SOURCE_DIRECTORY ` in the next steps. hawtbuf-1.9.jar geronimo-jms_1.1_spec-1.1.1.jar geronimo-j2ee-management_1.1_spec-1.0.1.jar Create another directory in a preferred location in your machine. !!! info This directory will be referred to as the ` DESTINATION_DIRECTORY ` in the next steps. To convert all the Kafka jars you copied into the SOURCE_DIRECTORY , issue the following command. For Windows: SP_HOME /bin/jartobundle.bat SOURCE_DIRECTORY_PATH DESTINATION_DIRECTORY_PATH For Linux : SP_HOME /bin/jartobundle.sh SOURCE_DIRECTORY_PATH DESTINATION_DIRECTORY_PATH Copy the converted files from the DESTINATION_DIRECTORY to the SP_HOME /lib directory. Copy the jars that are not converted from the SOURCE_DIRECTORY to the SP_HOME /samples/sample-clients/lib directory. MQTT transport Follow the steps to configure the MQTT message broker: Download the org.eclipse.paho.client.mqttv3-1.1.1.jar file from here. Place the file you downloaded in the SP_HOME /lib directory. RabbitMQ transport Follow the steps below to configure the RabbitMQ message broker: Download RabbitMQ from here. Create a directory in a preferred location in your machine. This directory is referred to as at the \\ SOURCE_DIRECTORY> in the rest of the procedure. Copy the following files from the RabbitMQ_HOME /plugins directory to the SOURCE_DIRECTORY you created. Create another directory in a preferred location in your machine. This directory is referred to as the DESTINATION_DIRECTORY in this procedure.","title":"Supporting Different Transports"},{"location":"admin/supporting-different-transports/#supporting-different-transports","text":"Follow the relevant section for the steps that need to be carried out before using the required transport to receive and publish events via WSO2 SP. Kafka transport JMS transport MQTT transport RabbitMQ transport","title":"Supporting Different Transports"},{"location":"admin/supporting-different-transports/#kafka-transport","text":"To enable WSO2 SP to receive and publish events via the Kafka transport, follow the steps below: Download the Kafka broker from here . Convert and copy the Kafka client jars from the KAFKA_HOME /libs directory to the SP_HOME /lib directory as follows. Create a directory in a preferred location in your machine and copy the following JARs to it from the KAFKA_HOME /libs directory. !!! info This directory will be referred to as the ` SOURCE_DIRECTORY ` in the next steps. kafka_2.11-0.10.2.1.jar kafka-clients-0.10.2.1.jar metrics-core-2.2.0.jar scala-library-2.11.8.jar scala-parser-combinators_2.11-1.0.4.jar zkclient-0.10.jar zookeeper-3.4.9.jar Create another directory in a preferred location in your machine. !!! info This directory will be referred to as the ` DESTINATION_DIRECTORY ` in the next steps. To convert all the Kafka jars you copied into the SOURCE_DIRECTORY , issue the following command. For Windows: SP_HOME /bin/jartobundle.bat SOURCE_DIRECTORY_PATH DESTINATION_DIRECTORY_PATH For Linux : SP_HOME /bin/jartobundle.sh SOURCE_DIRECTORY_PATH DESTINATION_DIRECTORY_PATH Copy the converted files from the DESTINATION_DIRECTORY to the SP_HOME /lib directory. Copy the jars that are not converted from the SOURCE_DIRECTORY to the SP_HOME /samples/sample-clients/lib directory. The Kafka server should be started before sending events from WSO2 SP to a Kafka consumer.","title":"Kafka transport"},{"location":"admin/supporting-different-transports/#jms-transport","text":"Follow the steps to configure the Apache ActiveMQ message broker: Install Apache ActiveMQ JMS . !!! info This guide uses ActiveMQ versions 5.7.0 - 5.9.0. If you want to use a later version, for instructions on the necessary changes to the configuration steps, go to [Apache ActiveMQ Documentation](http://activemq.apache.org/activemq-580-release.html) . Download the activemq-client-5.x.x.jar from here . Register the InitialContextFactory implementation according to the OSGi JNDI spec and copy the client jar to the SP_HOME /lib directory as follows. Navigate to the SP_HOME /bin directory and issue the following command. For Linux : ./icf-provider.sh org.apache.activemq.jndi.ActiveMQInitialContextFactory Downloaded Jar Path /activemq-client-5.x.x.jar Output Jar Path For Windows : ./icf-provider.bat org.apache.activemq.jndi.ActiveMQInitialContextFactory Downloaded Jar Path \\activemq-client-5.x.x.jar Output Jar Path !!! info If required, you can provide privileges via ` chmod +x icf-provider.(sh|bat) ` . Once the client jar is successfully converted, the activemq-client-5.x.x directory is created. This directory contains the following: activemq-client-5.x.x.jar (original jar) activemq-client-5.x.x_1.0.0.jar (OSGi-converted jar) In addition, the following messages are logged in the terminal. INFO: Executing 'jar uf absolute_path /activemq-client-5.x.x/activemq-client-5.x.x.jar -C absolute_path /activemq-client-5.x.x /internal/CustomBundleActivator.class' [timestamp] org.wso2.carbon.tools.spi.ICFProviderTool addBundleActivatorHeader - INFO: Running jar to bundle conversion [timestamp] org.wso2.carbon.tools.converter.utils.BundleGeneratorUtils convertFromJarToBundle - INFO: Created the OSGi bundle activemq_client_5.x.x_1.0.0.jar for JAR file absolute_path /activemq-client-5.x.x/activemq-client-5.x.x.jar Copy activemq-client-5.x.x/activemq-client-5.x.x.jar and place it in the SP_HOME /samples/sample-clients/lib directory. Copy activemq-client-5.x.x/activemq_client_5.x.x_1.0.0.jar and place it in the SP_HOME /lib directory. Create a directory in a preferred location in your machine and copy the following JARs to it from the ActiveMQ_HOME /libs directory. !!! info This directory is referred to as the ` SOURCE_DIRECTORY ` in the next steps. hawtbuf-1.9.jar geronimo-jms_1.1_spec-1.1.1.jar geronimo-j2ee-management_1.1_spec-1.0.1.jar Create another directory in a preferred location in your machine. !!! info This directory will be referred to as the ` DESTINATION_DIRECTORY ` in the next steps. To convert all the Kafka jars you copied into the SOURCE_DIRECTORY , issue the following command. For Windows: SP_HOME /bin/jartobundle.bat SOURCE_DIRECTORY_PATH DESTINATION_DIRECTORY_PATH For Linux : SP_HOME /bin/jartobundle.sh SOURCE_DIRECTORY_PATH DESTINATION_DIRECTORY_PATH Copy the converted files from the DESTINATION_DIRECTORY to the SP_HOME /lib directory. Copy the jars that are not converted from the SOURCE_DIRECTORY to the SP_HOME /samples/sample-clients/lib directory.","title":"JMS transport"},{"location":"admin/supporting-different-transports/#mqtt-transport","text":"Follow the steps to configure the MQTT message broker: Download the org.eclipse.paho.client.mqttv3-1.1.1.jar file from here. Place the file you downloaded in the SP_HOME /lib directory.","title":"MQTT transport"},{"location":"admin/supporting-different-transports/#rabbitmq-transport","text":"Follow the steps below to configure the RabbitMQ message broker: Download RabbitMQ from here. Create a directory in a preferred location in your machine. This directory is referred to as at the \\ SOURCE_DIRECTORY> in the rest of the procedure. Copy the following files from the RabbitMQ_HOME /plugins directory to the SOURCE_DIRECTORY you created. Create another directory in a preferred location in your machine. This directory is referred to as the DESTINATION_DIRECTORY in this procedure.","title":"RabbitMQ transport"},{"location":"admin/user-Management-via-the-IdP-Client-Interface/","text":"User Management via the IdP Client Interface In WSO2 Stream Processor, user management is carried out through the Identity Provider Client (IdP Client) interface that can be switched as required for the user scenario. Furthermore, a custom IdP Client can be written to encompass the required user store connection and authentication. IdP clients can be switched by specifying te required IdP client in the auth.configs: section in the SP_HOME /conf/ PROFILE /deployment.yaml file. auth.configs: # Type of the IdP Client used for the user authentication type: local The active IdP client is local by default. Following are the IdP Clients available for WSO2 SP: Local IdP Client External IdP Client Local IdP Client The local IdP Client interacts with the file-based user store that is defined in the SP_HOME /conf/ PROFILE /deployment.yaml file under auth.configs namespace as follows: auth.configs: type: 'local' userManager: adminRole: admin userStore: users: - user: username: admin password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: admin The above user and role is added by default. Parameters The parameters used in the above configurations are as follows: Note If new users/roles are added and the above default user and role are also needed, the following parameters must be added to the user store along with the added user/role. Parameter Default Value Description userManager adminRole admin The name of the role that has administration privileges. userManager userStore users user username admin The username of the user. userManager userStore users user password YWRtaW4= The Base64(UTF-8) encrypted password of the user. userManager userStore users user roles 1 A comma separated list of the IDs of the roles assigned to the user. userManager userStore roles role id 1 The unique ID for the role. userManager userStore roles role admin admin The name of the role. Furthermore, Local IdP Client functionality can be controlled via the properties defined in the SP_HOME /conf/ PROFILE /deployment.yam l file under the auth.configs namespace as shown below. auth.configs: type: local properties: sessionTimeout: 3600 refreshSessionTimeout: 86400 The following are the properties that can be configured for the local IdP provider: Property Default Value Description properties sessiontimeout 3600 The number of seconds for which the session is valid once the user logs in. !!! info The value specified here needs to be greater than 60 seconds because the system checks the user credentials and keeps extending the session every minute until the session timeout is reached. properties refreshSessionTimeout 86400 The number of seconds for which the refresh token used to extend the session is valid. The complete default configuration of the local IdP Client is as follows: auth.configs: type: 'local' properties: sessionTimeout: 3600 refreshSessionTimeout: 86400 userManager: adminRole: admin userStore: users: - user: username: admin password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: admin External IdP Client External IdP Client authenticates users by interacting with an external identity provider via OAuth2 and SCIM2 protocols. The user store is maintained by the external identity provider. WSO2 SP authenticates by requesting an access token from the identity provider using the password grant type. Note The identity provider with which WSO2 SP interacts with to authenticate users must be started before the SP server. The auth manager must be configured under the auth.configs namespace as shown below: auth.configs: type: external authManager: adminRole: admin The parameters used in the above configurations areas follows: Parameter Default Value Description userManager adminRole admin The name of the role that has administration privilages. Furthermore, external IdP client functionality can be controlled via the properties defined in the SP_HOME /conf/ PROFILE /deployment.yaml file under the auth.configs namespace as shown below. auth.configs: type: external properties: kmDcrUrl: https://localhost:9443/identity/connect/register kmTokenUrl: https://localhost:9443/oauth2 kmUsername: admin kmPassword: admin idpBaseUrl: https://localhost:9443/scim2 idpUsername: admin idpPassword: admin portalAppContext: portal statusDashboardAppContext: monitoring businessRulesAppContext : business-rules databaseName: WSO2_OAUTH_APP_DB cacheTimeout: 900 baseUrl: https://localhost:9643 grantType: password The following are the properties that can be configured for the external IdP provider: Property Default Value Description kmDcrUrl https://localhost:9443/identity/connect/register The Dynamic Client Registration (DCR) endpoint of the key manager in the IdP. dcrAppOwner kmUsername kmTokenUrl https://localhost:9443/oauth2 The token endpoint of the key manager in the IdP. kmUsername admin The username for the key manager in the IdP. kmPassword admin The password for the key manager in the IdP. idpBaseUrl https://localhost:9443/scim2 The SCIM2 endpoint of the IdP. idpUsername admin The username for the IdP. idpPassword admin The password for the IdP. portalAppContext portal The application context of the Dashboard Portal application in WSO2 SP. statusDashboardAppContext monitoring The application context of the Status Dashboard application in WSO2 SP. businessRulesAppContext business-rules The application context of the Business Rules application in WSO2 SP. databaseName WSO2_OAUTH_APP_DB The name of the wso2.datasource used to store the OAuth application credentials cacheTimeout 900 The cache timeout for the validity period of the token in seconds. baseUrl https://localhost:9643 The base URL to which the token should be redirected after the code returned from the Authorization Code grant type is used to get the token. grantType password The grant type used in the OAuth application token request. spClientId/ portalClientId/ statusDashboardClientId/ businessRulesClientId N/A The client ID of the OAuth App. If no value is specified for this property, the DCR is called to register the application and persist the client ID in the data store. spClientId/ portalClientId/ statusDashboardClientId/ businessRulesClientId N/A The client secret of the OAuth application. If no value is specified for this property, the DCR is called to register the application and persist the client secret in the data store. Writing custom IdP Client When writing a custom IdP client, the following two interfaces must be implemented: IdPClientFactory : This is a factory OSGi service that initialtes the custom IdP client using the properties from IdPClientConfiguration. IdPClient : An interface with functions to provide user authentication and retrieval by the other services.","title":"User Management via the IdP Client Interface"},{"location":"admin/user-Management-via-the-IdP-Client-Interface/#user-management-via-the-idp-client-interface","text":"In WSO2 Stream Processor, user management is carried out through the Identity Provider Client (IdP Client) interface that can be switched as required for the user scenario. Furthermore, a custom IdP Client can be written to encompass the required user store connection and authentication. IdP clients can be switched by specifying te required IdP client in the auth.configs: section in the SP_HOME /conf/ PROFILE /deployment.yaml file. auth.configs: # Type of the IdP Client used for the user authentication type: local The active IdP client is local by default. Following are the IdP Clients available for WSO2 SP: Local IdP Client External IdP Client","title":"User Management via the IdP Client Interface"},{"location":"admin/user-Management-via-the-IdP-Client-Interface/#local-idp-client","text":"The local IdP Client interacts with the file-based user store that is defined in the SP_HOME /conf/ PROFILE /deployment.yaml file under auth.configs namespace as follows: auth.configs: type: 'local' userManager: adminRole: admin userStore: users: - user: username: admin password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: admin The above user and role is added by default.","title":"Local IdP Client"},{"location":"admin/user-Management-via-the-IdP-Client-Interface/#parameters","text":"The parameters used in the above configurations are as follows: Note If new users/roles are added and the above default user and role are also needed, the following parameters must be added to the user store along with the added user/role. Parameter Default Value Description userManager adminRole admin The name of the role that has administration privileges. userManager userStore users user username admin The username of the user. userManager userStore users user password YWRtaW4= The Base64(UTF-8) encrypted password of the user. userManager userStore users user roles 1 A comma separated list of the IDs of the roles assigned to the user. userManager userStore roles role id 1 The unique ID for the role. userManager userStore roles role admin admin The name of the role. Furthermore, Local IdP Client functionality can be controlled via the properties defined in the SP_HOME /conf/ PROFILE /deployment.yam l file under the auth.configs namespace as shown below. auth.configs: type: local properties: sessionTimeout: 3600 refreshSessionTimeout: 86400 The following are the properties that can be configured for the local IdP provider: Property Default Value Description properties sessiontimeout 3600 The number of seconds for which the session is valid once the user logs in. !!! info The value specified here needs to be greater than 60 seconds because the system checks the user credentials and keeps extending the session every minute until the session timeout is reached. properties refreshSessionTimeout 86400 The number of seconds for which the refresh token used to extend the session is valid. The complete default configuration of the local IdP Client is as follows: auth.configs: type: 'local' properties: sessionTimeout: 3600 refreshSessionTimeout: 86400 userManager: adminRole: admin userStore: users: - user: username: admin password: YWRtaW4= roles: 1 roles: - role: id: 1 displayName: admin","title":"Parameters"},{"location":"admin/user-Management-via-the-IdP-Client-Interface/#external-idp-client","text":"External IdP Client authenticates users by interacting with an external identity provider via OAuth2 and SCIM2 protocols. The user store is maintained by the external identity provider. WSO2 SP authenticates by requesting an access token from the identity provider using the password grant type. Note The identity provider with which WSO2 SP interacts with to authenticate users must be started before the SP server. The auth manager must be configured under the auth.configs namespace as shown below: auth.configs: type: external authManager: adminRole: admin The parameters used in the above configurations areas follows: Parameter Default Value Description userManager adminRole admin The name of the role that has administration privilages. Furthermore, external IdP client functionality can be controlled via the properties defined in the SP_HOME /conf/ PROFILE /deployment.yaml file under the auth.configs namespace as shown below. auth.configs: type: external properties: kmDcrUrl: https://localhost:9443/identity/connect/register kmTokenUrl: https://localhost:9443/oauth2 kmUsername: admin kmPassword: admin idpBaseUrl: https://localhost:9443/scim2 idpUsername: admin idpPassword: admin portalAppContext: portal statusDashboardAppContext: monitoring businessRulesAppContext : business-rules databaseName: WSO2_OAUTH_APP_DB cacheTimeout: 900 baseUrl: https://localhost:9643 grantType: password The following are the properties that can be configured for the external IdP provider: Property Default Value Description kmDcrUrl https://localhost:9443/identity/connect/register The Dynamic Client Registration (DCR) endpoint of the key manager in the IdP. dcrAppOwner kmUsername kmTokenUrl https://localhost:9443/oauth2 The token endpoint of the key manager in the IdP. kmUsername admin The username for the key manager in the IdP. kmPassword admin The password for the key manager in the IdP. idpBaseUrl https://localhost:9443/scim2 The SCIM2 endpoint of the IdP. idpUsername admin The username for the IdP. idpPassword admin The password for the IdP. portalAppContext portal The application context of the Dashboard Portal application in WSO2 SP. statusDashboardAppContext monitoring The application context of the Status Dashboard application in WSO2 SP. businessRulesAppContext business-rules The application context of the Business Rules application in WSO2 SP. databaseName WSO2_OAUTH_APP_DB The name of the wso2.datasource used to store the OAuth application credentials cacheTimeout 900 The cache timeout for the validity period of the token in seconds. baseUrl https://localhost:9643 The base URL to which the token should be redirected after the code returned from the Authorization Code grant type is used to get the token. grantType password The grant type used in the OAuth application token request. spClientId/ portalClientId/ statusDashboardClientId/ businessRulesClientId N/A The client ID of the OAuth App. If no value is specified for this property, the DCR is called to register the application and persist the client ID in the data store. spClientId/ portalClientId/ statusDashboardClientId/ businessRulesClientId N/A The client secret of the OAuth application. If no value is specified for this property, the DCR is called to register the application and persist the client secret in the data store.","title":"External IdP Client"},{"location":"admin/user-Management-via-the-IdP-Client-Interface/#writing-custom-idp-client","text":"When writing a custom IdP client, the following two interfaces must be implemented: IdPClientFactory : This is a factory OSGi service that initialtes the custom IdP client using the properties from IdPClientConfiguration. IdPClient : An interface with functions to provide user authentication and retrieval by the other services.","title":"Writing custom IdP Client"},{"location":"admin/user-management/","text":"User Management Info User management in Stream Processor has the following features, The concept of single user store, which is either local or external. File based user store as the default embedded store. Ability to connect to an external Identity Provider using SCIM2 and OAuth2 protocols. Ability to extend user authentication as per the scenario Follwing sections includes further information and configuration steps for the user mangement in SP, Introduction to User Management User Management via the IdP Client Interface","title":"User Management"},{"location":"admin/user-management/#user-management","text":"Info User management in Stream Processor has the following features, The concept of single user store, which is either local or external. File based user store as the default embedded store. Ability to connect to an external Identity Provider using SCIM2 and OAuth2 protocols. Ability to extend user authentication as per the scenario Follwing sections includes further information and configuration steps for the user mangement in SP, Introduction to User Management User Management via the IdP Client Interface","title":"User Management"},{"location":"admin/viewing-Node-specific-Pages/","text":"Viewing Node-specific Pages When you open the WSO2 SP Status Dashboard, the Node Overview page is displayed by default. To view information specific to a selected worker node, click on the relevant widget. This opens a separate page for the worker node as shown in the example below. Status indicators Thw following gadgets can be viewed for the selected worker. Server General Details View (Example) Description This gadget displays general information relating to the selected worker node. Purpose This allows you to understand the distribution of nodes in terms of the location, the time zone, operating system used etc., and to locate them. Recommended Action In a distributed set up, you can use this information to evaluate the clustered setup and make changes to optimize the benefits of deploying WSO2 SP as a cluster (e.g., making them physically available in different locations to minimize the risk of all the nodes failing at the same time etc.). CPU Usage View (Example) Description This displays the CPU usage of the selected node. Purpose This allows you to observe the CPU usage of a selected node over time. Recommended Action Identify sudden slumps in the CPU usage, and investigate the reasons (e.g., such as authentication errors that result in requests not reaching the SP server). Identify continuous increases in the CPU usage and check whether the node is overloaded. If so, reallocate some of the Siddhi applications deployed in the node. Memory Used View (Example) Description This displays the memory usage of the selected node. Purpose This allows you to observe the memory usage of a selected node over time. Recommended Action Identify sudden slumps in the memory usage, and investigate the reasons (e.g., a reduction in the requests recived due to system failure). If there are continous increases in the memory usage, check whether there is an increase in the requests handled, and whether you have enough memory resources to handle the increased demand. If not, add more memory to the node or reallocate some of the Siddhi applications deployed in the node to other nodes. System Load Average View (Example) Description This displays the system load average for the selected node. Purpose This allows you to observe the system load of the node over time. Recommended Action Observe the trends of the node's system load, and adjust the allocation of resources (e.g., memory) and work load (i.e., the number of Siddhi applications deployed) accordingly. Overall Throughput View (Example) Description This displays the overall throughput of the selected node. Purpose This allows you to assess the performance of the selected node in terms of the throughput over time. Recommended Action Compare the throughput of the node against that of other nodes with the same amount of CPU and memory resources. If there are significant variations, investigate the causes (e.g., the differences in the number of requests received by different Siddhi applications deployed in the nodes). Observe changes in the throughput over time. If there are significant variances, investigate the causes (e.g., whether the node has been unavaialable to receive requests during a given time). Siddhi Applications View (Example) Description This table displays the complete list of Siddhi applications deployed in the selected node. The status is displayed in green for active Siddhi applications, and in red for inactive Siddhi applications. In addition, the following is displayed for each Siddhi application: Age : The age of the Siddhi application in milliseconds. Latency : The time (in milliseconds) taken by the Siddhi application to process one request. Throughput: The number of requests processed by the Siddhi application since it has been active. Memory : The amount of memory consumed by the Siddhi application during its current active session, expressed in milliseconds. Purpose This allows you to assess the performance of each Siddhi application deployed in the selected node. Recommended Action Identify the inactive Siddhi applications that are required to be active and take the appropriate corrective action. Identify Siddhi applications that consume too much memory, and identify ways in which the memory usage can be optimized (e.g., use incremental processing).","title":"Viewing Node-specific Pages"},{"location":"admin/viewing-Node-specific-Pages/#viewing-node-specific-pages","text":"When you open the WSO2 SP Status Dashboard, the Node Overview page is displayed by default. To view information specific to a selected worker node, click on the relevant widget. This opens a separate page for the worker node as shown in the example below.","title":"Viewing Node-specific Pages"},{"location":"admin/viewing-Node-specific-Pages/#status-indicators","text":"Thw following gadgets can be viewed for the selected worker.","title":"Status indicators"},{"location":"admin/viewing-Node-specific-Pages/#server-general-details","text":"View (Example) Description This gadget displays general information relating to the selected worker node. Purpose This allows you to understand the distribution of nodes in terms of the location, the time zone, operating system used etc., and to locate them. Recommended Action In a distributed set up, you can use this information to evaluate the clustered setup and make changes to optimize the benefits of deploying WSO2 SP as a cluster (e.g., making them physically available in different locations to minimize the risk of all the nodes failing at the same time etc.).","title":"Server General Details"},{"location":"admin/viewing-Node-specific-Pages/#cpu-usage","text":"View (Example) Description This displays the CPU usage of the selected node. Purpose This allows you to observe the CPU usage of a selected node over time. Recommended Action Identify sudden slumps in the CPU usage, and investigate the reasons (e.g., such as authentication errors that result in requests not reaching the SP server). Identify continuous increases in the CPU usage and check whether the node is overloaded. If so, reallocate some of the Siddhi applications deployed in the node.","title":"CPU Usage"},{"location":"admin/viewing-Node-specific-Pages/#memory-used","text":"View (Example) Description This displays the memory usage of the selected node. Purpose This allows you to observe the memory usage of a selected node over time. Recommended Action Identify sudden slumps in the memory usage, and investigate the reasons (e.g., a reduction in the requests recived due to system failure). If there are continous increases in the memory usage, check whether there is an increase in the requests handled, and whether you have enough memory resources to handle the increased demand. If not, add more memory to the node or reallocate some of the Siddhi applications deployed in the node to other nodes.","title":"Memory Used"},{"location":"admin/viewing-Node-specific-Pages/#system-load-average","text":"View (Example) Description This displays the system load average for the selected node. Purpose This allows you to observe the system load of the node over time. Recommended Action Observe the trends of the node's system load, and adjust the allocation of resources (e.g., memory) and work load (i.e., the number of Siddhi applications deployed) accordingly.","title":"System Load Average"},{"location":"admin/viewing-Node-specific-Pages/#overall-throughput","text":"View (Example) Description This displays the overall throughput of the selected node. Purpose This allows you to assess the performance of the selected node in terms of the throughput over time. Recommended Action Compare the throughput of the node against that of other nodes with the same amount of CPU and memory resources. If there are significant variations, investigate the causes (e.g., the differences in the number of requests received by different Siddhi applications deployed in the nodes). Observe changes in the throughput over time. If there are significant variances, investigate the causes (e.g., whether the node has been unavaialable to receive requests during a given time).","title":"Overall Throughput"},{"location":"admin/viewing-Node-specific-Pages/#siddhi-applications","text":"View (Example) Description This table displays the complete list of Siddhi applications deployed in the selected node. The status is displayed in green for active Siddhi applications, and in red for inactive Siddhi applications. In addition, the following is displayed for each Siddhi application: Age : The age of the Siddhi application in milliseconds. Latency : The time (in milliseconds) taken by the Siddhi application to process one request. Throughput: The number of requests processed by the Siddhi application since it has been active. Memory : The amount of memory consumed by the Siddhi application during its current active session, expressed in milliseconds. Purpose This allows you to assess the performance of each Siddhi application deployed in the selected node. Recommended Action Identify the inactive Siddhi applications that are required to be active and take the appropriate corrective action. Identify Siddhi applications that consume too much memory, and identify ways in which the memory usage can be optimized (e.g., use incremental processing).","title":"Siddhi Applications"},{"location":"admin/viewing-Statistics-for-Parent-Siddhi-Applications/","text":"Viewing Statistics for Parent Siddhi Applications When you open the WSO2 SP Status Dashboard, the Node Overview page is displayed by default. To view information specific to an active manager, click on the required active manager node in the Distributed Deployments section. This opens a page with parent Siddhi applications deployed in that manager node as shown in the example below. {width=\"900\"} This page provides a summary of information relating to each parent Siddhi application as described in the table below. If a parent Siddhi application is active, it is indicated with a green dot that appears before the name of the Siddhi application. Similarly, an orange dot is displayed for inactive parent Siddhi applications. Detail Description Groups This indicates the number of execution groups of the parent Siddhi application. In the above example, the Testing Siddhi application has only one execution group. Child Apps This indicates the number of child applications of the parent Siddhi application. The number of active child applications is displayed in green, and the number of inactive child applications are displayed in red. Worker Nodes The number displayed in yellow indicates the total number of worker nodes in the resource cluster. In the above example, there are two worker nodes in the cluster. The number displayed in green indicates the number of worker nodes in which the Siddhi application is deployed. In the above example, the Testing parent Siddhi application is deployed only in one worker node although there are two worker nodes in the resource cluster. If you click on a parent Siddhi application, detailed information is displayed as shown below. {width=\"900\"} The following are the widgets displayed. Code View View (Example) Description This displays the queries defined in the Parent Siddhi file of the application. This allows you to check the queries of the Siddhi application if any further investigations are needed based on the kafka diagrams and performance. For detailed instructions to write a Siddhi application, see Converting to a Distributed Streaming Application . For detailed information about the Siddhi logic, see the Siddhi Query Guide . Purpose This allows you to check the queries of the Siddhi application if any further investigations are needed based on the observations of the performance of the distributed cluster to which it belongs. Recommended Action Edit the Siddhi file if any changes that can improve the performance of the Siddhi application are identified. For detailed instructions to write a Siddhi application, see Converting to a Distributed Streaming Application . For detailed information about the Siddhi logic, see the Siddhi Query Guide . Distributed Siddhi App Deployment View (Example) Description This is a graphical representation of how Kafka topics are connected to the child Siddhi applications of the selected parent Siddhi application . Kafka topics are represented by boxes with red margins, and the child applications are represented by boxes with blue margins. Purpose This is displayed for you to understand how the flow of information takes place. Child App Details View (Example) Description This table displays the complete list of child Siddhi applications of the selected parent Siddhi application. The status is displayed in green for active Siddhi applications, and in red for inactive Siddhi applications. In addition, the following is displayed for each Siddhi application: Group Name : The name of the execution group to which the child application belongs. Child App Status : This indicates whether the child application is currently active or not. Worker Node : The HTTPS host and The HTTPS port of the worker node in which the child siddhi application is deployed. Purpose To identify the currently active child applications.","title":"Viewing Statistics for Parent Siddhi Applications"},{"location":"admin/viewing-Statistics-for-Parent-Siddhi-Applications/#viewing-statistics-for-parent-siddhi-applications","text":"When you open the WSO2 SP Status Dashboard, the Node Overview page is displayed by default. To view information specific to an active manager, click on the required active manager node in the Distributed Deployments section. This opens a page with parent Siddhi applications deployed in that manager node as shown in the example below. {width=\"900\"} This page provides a summary of information relating to each parent Siddhi application as described in the table below. If a parent Siddhi application is active, it is indicated with a green dot that appears before the name of the Siddhi application. Similarly, an orange dot is displayed for inactive parent Siddhi applications. Detail Description Groups This indicates the number of execution groups of the parent Siddhi application. In the above example, the Testing Siddhi application has only one execution group. Child Apps This indicates the number of child applications of the parent Siddhi application. The number of active child applications is displayed in green, and the number of inactive child applications are displayed in red. Worker Nodes The number displayed in yellow indicates the total number of worker nodes in the resource cluster. In the above example, there are two worker nodes in the cluster. The number displayed in green indicates the number of worker nodes in which the Siddhi application is deployed. In the above example, the Testing parent Siddhi application is deployed only in one worker node although there are two worker nodes in the resource cluster. If you click on a parent Siddhi application, detailed information is displayed as shown below. {width=\"900\"} The following are the widgets displayed.","title":"Viewing Statistics for Parent Siddhi Applications"},{"location":"admin/viewing-Statistics-for-Parent-Siddhi-Applications/#code-view","text":"View (Example) Description This displays the queries defined in the Parent Siddhi file of the application. This allows you to check the queries of the Siddhi application if any further investigations are needed based on the kafka diagrams and performance. For detailed instructions to write a Siddhi application, see Converting to a Distributed Streaming Application . For detailed information about the Siddhi logic, see the Siddhi Query Guide . Purpose This allows you to check the queries of the Siddhi application if any further investigations are needed based on the observations of the performance of the distributed cluster to which it belongs. Recommended Action Edit the Siddhi file if any changes that can improve the performance of the Siddhi application are identified. For detailed instructions to write a Siddhi application, see Converting to a Distributed Streaming Application . For detailed information about the Siddhi logic, see the Siddhi Query Guide .","title":"Code View"},{"location":"admin/viewing-Statistics-for-Parent-Siddhi-Applications/#distributed-siddhi-app-deployment","text":"View (Example) Description This is a graphical representation of how Kafka topics are connected to the child Siddhi applications of the selected parent Siddhi application . Kafka topics are represented by boxes with red margins, and the child applications are represented by boxes with blue margins. Purpose This is displayed for you to understand how the flow of information takes place.","title":"Distributed Siddhi App Deployment"},{"location":"admin/viewing-Statistics-for-Parent-Siddhi-Applications/#child-app-details","text":"View (Example) Description This table displays the complete list of child Siddhi applications of the selected parent Siddhi application. The status is displayed in green for active Siddhi applications, and in red for inactive Siddhi applications. In addition, the following is displayed for each Siddhi application: Group Name : The name of the execution group to which the child application belongs. Child App Status : This indicates whether the child application is currently active or not. Worker Node : The HTTPS host and The HTTPS port of the worker node in which the child siddhi application is deployed. Purpose To identify the currently active child applications.","title":"Child App Details"},{"location":"admin/viewing-Statistics-for-Siddhi-Applications/","text":"Viewing Statistics for Siddhi Applications When you open the WSO2 SP Status Dashboard, the Node Overview page is displayed by default. To view information specific to a selected worker node, click on the relevant gadget. This opens the page specific to the worker . To view information specific to a Siddhi application deployed in the Siddhi node, click on the relevant Siddhi application in the Siddhi Applications table. This opens a page with information specific to the selected Siddhi application as shown in the example below. {width=\"900\"} The following statistics can be viewed for an individual Siddhi Application. Latency View (Example) Description This displays the latency of the selected Siddhi application. Latency is the time taken to complete processing a single event in the event flow. Purpose This allows you to assess the performance of the selected Siddhi application. Recommended Action If the latency of the Siddhi application is too high, check the Siddhi queries and rewrite them to optimise performance. Overall Throughput View (Example) Description This shows the overall throughput of a selected Siddhi application over time. Purpose This allows you to assess the performance of the selected Siddhi application. Recommended Action If the throughput of a Siddhi application varies greatly overtime, investigate reasons for any slumps in the throughput (e.g., errors in the deployment of the application). If the throughput of the Siddhi application is lower than expected, investigate reasons, and take corrective action to improve the throughput (e.g., check the Siddhi queries in the application and rewrite them with best practices to achieve greater efficiency in the processing of events. Memory Used View (Example) Description This displays the memory usage (In MB) of a selected Siddhi application over time. Purpose This allows you to monitor the memory consumption of individual Siddhi applications. Recommended Action If there are major fluctuations in the memory consumption of a Siddhi application, investigate the reasons (e.g., Whether the Siddhi application has been inactive at any point of time). Code View View (Example) Description This displays the queries defined in the Siddhi file of the application. Purpose This allows you to check the queries of the Siddhi application if any further investigations are needed based on the observations of its latency, throughput and the memory consumption. Recommended Action Edit the Siddhi file if any changes that can improve the performance of the Siddhi application are identified. For detailed instructions to write a Siddhi application, see Creating a Siddhi Application . | For detailed information about the Siddhi logic, see the Siddhi Query Guide . Design View View (Example) Description This displays the graphical view for queries defined in the Siddhi file of the application. Purpose This allows you to check the flow of the queries of the Siddhi application in the graphical way. Recommended Action Edit the Siddhi file if any changes that can improve the performance of the Siddhi application are identified. Siddhi App Component Statistics View (Example) Description This table displays performance statistics related to dfferent components within a selected Siddhi application (e.g., queries). The columns displayed are as follows: Type : The type of the Siddhi application component to which the information displayed in the row applies. The component type can be queries, streams, tables, windows and aggregations. For more information, see Siddhi Application Overview - Common components of a Siddhi application . Name : The name of the Siddhi component within the application to which the information displayed in the row apply. Metric Type : The metric type for which the statistics are displayed. This can be either the latency (in milliseconds), throughput the number of events per second), or the amount of memory consumed (in bytes). The metric types based on which the performance of a Siddhi component is measured depends on the component type. Attribute : The attribute to which the given value applies. Value : The value for the metric type given in the row. Purpose This allows you to carry out a detailed analysis of the performance of a selected Siddhi application and identify components that have a negative impact on the overall performance of the Siddhi application. Recommended Action Identify the componets in a Siddhi application that have a negative impact on the performance, and rewrite them to improve performance. To understand Siddhi concepts in order to rewrite the components, see the Siddhi Query Guide .","title":"Viewing Statistics for Siddhi Applications"},{"location":"admin/viewing-Statistics-for-Siddhi-Applications/#viewing-statistics-for-siddhi-applications","text":"When you open the WSO2 SP Status Dashboard, the Node Overview page is displayed by default. To view information specific to a selected worker node, click on the relevant gadget. This opens the page specific to the worker . To view information specific to a Siddhi application deployed in the Siddhi node, click on the relevant Siddhi application in the Siddhi Applications table. This opens a page with information specific to the selected Siddhi application as shown in the example below. {width=\"900\"} The following statistics can be viewed for an individual Siddhi Application.","title":"Viewing Statistics for Siddhi Applications"},{"location":"admin/viewing-Statistics-for-Siddhi-Applications/#latency","text":"View (Example) Description This displays the latency of the selected Siddhi application. Latency is the time taken to complete processing a single event in the event flow. Purpose This allows you to assess the performance of the selected Siddhi application. Recommended Action If the latency of the Siddhi application is too high, check the Siddhi queries and rewrite them to optimise performance.","title":"Latency"},{"location":"admin/viewing-Statistics-for-Siddhi-Applications/#overall-throughput","text":"View (Example) Description This shows the overall throughput of a selected Siddhi application over time. Purpose This allows you to assess the performance of the selected Siddhi application. Recommended Action If the throughput of a Siddhi application varies greatly overtime, investigate reasons for any slumps in the throughput (e.g., errors in the deployment of the application). If the throughput of the Siddhi application is lower than expected, investigate reasons, and take corrective action to improve the throughput (e.g., check the Siddhi queries in the application and rewrite them with best practices to achieve greater efficiency in the processing of events.","title":"Overall Throughput"},{"location":"admin/viewing-Statistics-for-Siddhi-Applications/#memory-used","text":"View (Example) Description This displays the memory usage (In MB) of a selected Siddhi application over time. Purpose This allows you to monitor the memory consumption of individual Siddhi applications. Recommended Action If there are major fluctuations in the memory consumption of a Siddhi application, investigate the reasons (e.g., Whether the Siddhi application has been inactive at any point of time).","title":"Memory Used"},{"location":"admin/viewing-Statistics-for-Siddhi-Applications/#code-view","text":"View (Example) Description This displays the queries defined in the Siddhi file of the application. Purpose This allows you to check the queries of the Siddhi application if any further investigations are needed based on the observations of its latency, throughput and the memory consumption. Recommended Action Edit the Siddhi file if any changes that can improve the performance of the Siddhi application are identified. For detailed instructions to write a Siddhi application, see Creating a Siddhi Application . | For detailed information about the Siddhi logic, see the Siddhi Query Guide .","title":"Code View"},{"location":"admin/viewing-Statistics-for-Siddhi-Applications/#design-view","text":"View (Example) Description This displays the graphical view for queries defined in the Siddhi file of the application. Purpose This allows you to check the flow of the queries of the Siddhi application in the graphical way. Recommended Action Edit the Siddhi file if any changes that can improve the performance of the Siddhi application are identified.","title":"Design View"},{"location":"admin/viewing-Statistics-for-Siddhi-Applications/#siddhi-app-component-statistics","text":"View (Example) Description This table displays performance statistics related to dfferent components within a selected Siddhi application (e.g., queries). The columns displayed are as follows: Type : The type of the Siddhi application component to which the information displayed in the row applies. The component type can be queries, streams, tables, windows and aggregations. For more information, see Siddhi Application Overview - Common components of a Siddhi application . Name : The name of the Siddhi component within the application to which the information displayed in the row apply. Metric Type : The metric type for which the statistics are displayed. This can be either the latency (in milliseconds), throughput the number of events per second), or the amount of memory consumed (in bytes). The metric types based on which the performance of a Siddhi component is measured depends on the component type. Attribute : The attribute to which the given value applies. Value : The value for the metric type given in the row. Purpose This allows you to carry out a detailed analysis of the performance of a selected Siddhi application and identify components that have a negative impact on the overall performance of the Siddhi application. Recommended Action Identify the componets in a Siddhi application that have a negative impact on the performance, and rewrite them to improve performance. To understand Siddhi concepts in order to rewrite the components, see the Siddhi Query Guide .","title":"Siddhi App Component Statistics"},{"location":"admin/viewing-Statistics/","text":"Viewing Statistics To view the status dashboard, follow the procedure below: Start the dashboard for your worker node by issuing one of the following commands: For Windows: dashboard.bat For Linux : ./dashboard.sh Access the Status Dashboard via the following URL format. https://localhost: SP_DASHBOARD_PORT /sp-status-dashboard e.g., https://0.0.0.0:9643/sp-status-dashboard After login this opens the Status Dashboard with the nodes that you have already added as shown in the example below. I f no nodes are displayed, add the nodes for which you wnt to view statistics by following the steps in Worker Overview - Adding a worker to the dashboard . For a detailed descripion of each page in this dashboard, see the following topics: Node Overview Viewing Node-specific Pages Viewing Worker History Viewing Statistics for Siddhi Applications Viewing Statistics for Parent Siddhi Applications App Overview","title":"Viewing Statistics"},{"location":"admin/viewing-Statistics/#viewing-statistics","text":"To view the status dashboard, follow the procedure below: Start the dashboard for your worker node by issuing one of the following commands: For Windows: dashboard.bat For Linux : ./dashboard.sh Access the Status Dashboard via the following URL format. https://localhost: SP_DASHBOARD_PORT /sp-status-dashboard e.g., https://0.0.0.0:9643/sp-status-dashboard After login this opens the Status Dashboard with the nodes that you have already added as shown in the example below. I f no nodes are displayed, add the nodes for which you wnt to view statistics by following the steps in Worker Overview - Adding a worker to the dashboard . For a detailed descripion of each page in this dashboard, see the following topics: Node Overview Viewing Node-specific Pages Viewing Worker History Viewing Statistics for Siddhi Applications Viewing Statistics for Parent Siddhi Applications App Overview","title":"Viewing Statistics"},{"location":"admin/viewing-Worker-History/","text":"Viewing Worker History This section explains how to view statistics relating to the performance of a selected node for a specific time interval. Login to the Status Dashboard. For detailed instructions, see Monitoring the Stream Processor - Viewing the Status Dashboard. When you login, the Node Overview page is displayed by default. Click on the required node to view information specific to that node. In the page displayed with node-specific information, click one of the following gadgets to open the Metrics page. CPU Usage Memory Used System Load Average Overall Throughput In the Metrics page, click the required time interval. Then the page displays statistics relating to the performance of the selected node applicable to that time period. If you want to view more details, click More Details . As a result, the following additional information is displayed for the node for the selected time period. ****CPU Usage {height=\"250\"} **** ****JVM OS as CPU {height=\"250\"}**** ****JVM Physical Memory {height=\"250\"}**** ****JVM Threads {height=\"250\"}**** **JVM Swap Space {height=\"250\"} **","title":"Viewing Worker History"},{"location":"admin/viewing-Worker-History/#viewing-worker-history","text":"This section explains how to view statistics relating to the performance of a selected node for a specific time interval. Login to the Status Dashboard. For detailed instructions, see Monitoring the Stream Processor - Viewing the Status Dashboard. When you login, the Node Overview page is displayed by default. Click on the required node to view information specific to that node. In the page displayed with node-specific information, click one of the following gadgets to open the Metrics page. CPU Usage Memory Used System Load Average Overall Throughput In the Metrics page, click the required time interval. Then the page displays statistics relating to the performance of the selected node applicable to that time period. If you want to view more details, click More Details . As a result, the following additional information is displayed for the node for the selected time period. ****CPU Usage {height=\"250\"} **** ****JVM OS as CPU {height=\"250\"}**** ****JVM Physical Memory {height=\"250\"}**** ****JVM Threads {height=\"250\"}**** **JVM Swap Space {height=\"250\"} **","title":"Viewing Worker History"},{"location":"admin/working-with-Databases/","text":"Working with Databases WSO2 SP stores product-specific data in the H2 database located in the SP_HOME /wso2/ RunTime /database directory by default. This embedded H2 database is suitable for development, testing, and for some production environments. For most production environments, however, we recommend you to use an industry-standard RDBMS such as Oracle, PostgreSQL, MySQL, MS SQL. Most table schemas are self generated by the feature itself. For others, you can use the scripts provided with WSO2 SP (in the SP_HOME /wso2/ RunTime /dbscripts directory) to install and configure databases, including Oracle, PostgreSQL, MySQL and MS SQL. The following sections explain how to change the default databases and queries: Configuring Datasources Configuring Database Queries","title":"Working with Databases"},{"location":"admin/working-with-Databases/#working-with-databases","text":"WSO2 SP stores product-specific data in the H2 database located in the SP_HOME /wso2/ RunTime /database directory by default. This embedded H2 database is suitable for development, testing, and for some production environments. For most production environments, however, we recommend you to use an industry-standard RDBMS such as Oracle, PostgreSQL, MySQL, MS SQL. Most table schemas are self generated by the feature itself. For others, you can use the scripts provided with WSO2 SP (in the SP_HOME /wso2/ RunTime /dbscripts directory) to install and configure databases, including Oracle, PostgreSQL, MySQL and MS SQL. The following sections explain how to change the default databases and queries: Configuring Datasources Configuring Database Queries","title":"Working with Databases"},{"location":"admin/working-with-Keystores/","text":"Working with Keystores Warning This section is currently a work in progress! A keystore is a repository that stores the cryptographic keys and certificates that are used for various security purposes, such as encrypting sensitive information and for establishing trust between your server and outside parties that connect to your server. The usage of keys and certificates contained in a keystore are explained below. Key pairs : According to public-key cryptography, a key pair (private key and the corresponding public key) is used for encrypting sensitive information and for authenticating the identity of parties that communicate with your server. For example, information that is encrypted in your server using the public key can only be decrypted using the corresponding private key. Therefore, if any party wants to decrypt this encrypted data, they should have the corresponding private key, which is usually kept as a secret (not publicly shared). Digital certificate : When there is a key pair, it is also necessary to have a digital certificate to verify the identity of the keys. Typically, the public key of a key pair is embedded in this digital certificate, which also contains additional information such as the owner, validity, etc. of the keys. For example, if an external party wants to verify the integrity of data or validate the identity of the signer (by validating the digital signature), it is necessary for them to have this digital certificate. Trusted certificates : To establish trust, the digital certificate containing the public key should be signed by a trusted certifying authority (CA). You can generate self-signed certificates for the public key (thereby creating your own certifying authority), or you can get the certificates signed by an external CA. Both types of trusted certificates can be effectively used depending on the sensitivity of the information that is protected by the keys. When the certificate is signed by a reputed CA, all the parties who trust this CA also trust the certificates signed by them. Info Identity and Trust The key pair and the CA-signed certificates in a keystore establishes two security functions in your server: The key pair with the digital certificate is an indication of identity and the CA-signed certificate provides trust to the identity. Since the public key is used to encrypt information, the keystore containing the corresponding private key should always be protected, as it can decrypt the sensitive information. Furthermore, the privacy of the private key is important as it represents its own identity and protects the integrity of data. However, the CA-signed digital certificates should be accessible to outside parties that require to decrypt and use the information. To facilitate this requirement, the certificates must be copied to a separate keystore (called a Truststore), which can then be shared with outside parties. Therefore, in a typical setup, you will have one keystore for identity (containing the private key) that is protected, and a separate keystore for trust (containing CA certificates) that is shared with outside parties. Setting up keystores in WSO2 SP WSO2 SP uses keystores mainly for the following purposes: Authenticating the communication over Secure Sockets Layer (SSL)/Transport Layer Security (TLS) protocols Communication over Secure Sockets Layer (SSL) when invoking Rest APIs. Protecting sensitive information via Cipher Tool. Default keystore settings in WSO2 products WSO2 SP is shipped with two default keystore files stored in the SP_HOME /resources/security/ directory. wso2carbon.jks : This keystore contains a key pair and is used by default in your SP servers for all of the purposes explained above. client-truststore.jks : This is the default trust store, which contains the trusted certificates of the keystore used in SSL communication. By default, the following files provide paths to these keystores: SP_HOME /wso2/ PROFILE /bin/carbon.s h file This script is run when you start an SP server. It contains the following parameters, and makes references to the two files mentioned above by default. Parameter Default Value Description keyStore \"$CARBON_HOME/resources/security/wso2carbon.jks\" \\ This specifies the path to the keystore to be used when running the SP server on a secure network. keyStorePassword \"wso2carbon\" \\ The password to access the keystore trustStore \"$CARBON_HOME/resources/security/client-truststore.jks\" \\ This specifies the path to the trust store to be used when running the server on a secure network. trustStorePassword \"wso2carbon\" \\ The password to access the trust store. SP_HOME /conf/ PROFILE /deployment.yaml file refers to the above keystore and trust store by default for the following configurations: Listener configurations This specifies the key store to be used when WSO2 SP is receiving events via a secure network and the password to access the key store. Databridge configurations This specifies the key store to be used when WSO2 SP is publishing events via databrige using a secure network, and the password to access the key store. Secure vault configurations This specifies the key store to be used when you are configuring a secure vault to protect sensitive information. Note It is recommended to replace this default keystore with a new keystore that has self-signed or CA signed certificates when the products are deployed in production environments. This is because wso2carbon.jks is available with open source WSO2 products, which means anyone can have access to the private key of the default keystore. Managing keystores","title":"Working with Keystores"},{"location":"admin/working-with-Keystores/#working-with-keystores","text":"Warning This section is currently a work in progress! A keystore is a repository that stores the cryptographic keys and certificates that are used for various security purposes, such as encrypting sensitive information and for establishing trust between your server and outside parties that connect to your server. The usage of keys and certificates contained in a keystore are explained below. Key pairs : According to public-key cryptography, a key pair (private key and the corresponding public key) is used for encrypting sensitive information and for authenticating the identity of parties that communicate with your server. For example, information that is encrypted in your server using the public key can only be decrypted using the corresponding private key. Therefore, if any party wants to decrypt this encrypted data, they should have the corresponding private key, which is usually kept as a secret (not publicly shared). Digital certificate : When there is a key pair, it is also necessary to have a digital certificate to verify the identity of the keys. Typically, the public key of a key pair is embedded in this digital certificate, which also contains additional information such as the owner, validity, etc. of the keys. For example, if an external party wants to verify the integrity of data or validate the identity of the signer (by validating the digital signature), it is necessary for them to have this digital certificate. Trusted certificates : To establish trust, the digital certificate containing the public key should be signed by a trusted certifying authority (CA). You can generate self-signed certificates for the public key (thereby creating your own certifying authority), or you can get the certificates signed by an external CA. Both types of trusted certificates can be effectively used depending on the sensitivity of the information that is protected by the keys. When the certificate is signed by a reputed CA, all the parties who trust this CA also trust the certificates signed by them. Info Identity and Trust The key pair and the CA-signed certificates in a keystore establishes two security functions in your server: The key pair with the digital certificate is an indication of identity and the CA-signed certificate provides trust to the identity. Since the public key is used to encrypt information, the keystore containing the corresponding private key should always be protected, as it can decrypt the sensitive information. Furthermore, the privacy of the private key is important as it represents its own identity and protects the integrity of data. However, the CA-signed digital certificates should be accessible to outside parties that require to decrypt and use the information. To facilitate this requirement, the certificates must be copied to a separate keystore (called a Truststore), which can then be shared with outside parties. Therefore, in a typical setup, you will have one keystore for identity (containing the private key) that is protected, and a separate keystore for trust (containing CA certificates) that is shared with outside parties.","title":"Working with Keystores"},{"location":"admin/working-with-Keystores/#setting-up-keystores-in-wso2-sp","text":"WSO2 SP uses keystores mainly for the following purposes: Authenticating the communication over Secure Sockets Layer (SSL)/Transport Layer Security (TLS) protocols Communication over Secure Sockets Layer (SSL) when invoking Rest APIs. Protecting sensitive information via Cipher Tool.","title":"Setting up keystores in WSO2 SP"},{"location":"admin/working-with-Keystores/#default-keystore-settings-in-wso2-products","text":"WSO2 SP is shipped with two default keystore files stored in the SP_HOME /resources/security/ directory. wso2carbon.jks : This keystore contains a key pair and is used by default in your SP servers for all of the purposes explained above. client-truststore.jks : This is the default trust store, which contains the trusted certificates of the keystore used in SSL communication. By default, the following files provide paths to these keystores: SP_HOME /wso2/ PROFILE /bin/carbon.s h file This script is run when you start an SP server. It contains the following parameters, and makes references to the two files mentioned above by default. Parameter Default Value Description keyStore \"$CARBON_HOME/resources/security/wso2carbon.jks\" \\ This specifies the path to the keystore to be used when running the SP server on a secure network. keyStorePassword \"wso2carbon\" \\ The password to access the keystore trustStore \"$CARBON_HOME/resources/security/client-truststore.jks\" \\ This specifies the path to the trust store to be used when running the server on a secure network. trustStorePassword \"wso2carbon\" \\ The password to access the trust store. SP_HOME /conf/ PROFILE /deployment.yaml file refers to the above keystore and trust store by default for the following configurations: Listener configurations This specifies the key store to be used when WSO2 SP is receiving events via a secure network and the password to access the key store. Databridge configurations This specifies the key store to be used when WSO2 SP is publishing events via databrige using a secure network, and the password to access the key store. Secure vault configurations This specifies the key store to be used when you are configuring a secure vault to protect sensitive information. Note It is recommended to replace this default keystore with a new keystore that has self-signed or CA signed certificates when the products are deployed in production environments. This is because wso2carbon.jks is available with open source WSO2 products, which means anyone can have access to the private key of the default keystore.","title":"Default keystore settings in WSO2 products"},{"location":"admin/working-with-Keystores/#managing-keystores","text":"","title":"Managing keystores"},{"location":"admin/working-with-business-rules/","text":"Working with Business Rules In stream processing, there are common use cases for analyzing statistics that involve operations such as calculating the average, minimum, maximum etc., for different endpoints. The Business Rules Manager allows you to define templates and generate business rules from them for different scenarios with common requirements. Watch the following screencast for an introduction to WSO2 SP Business Rules. The following topics cover how to create templates for business rules, create business rules from them and how to manage them. Creating Business Rules Managing Business Rules Creating a Business Rule Template Business Rules Templates Configuring Business Rules Manager Permissions","title":"Working with Business Rules"},{"location":"admin/working-with-business-rules/#working-with-business-rules","text":"In stream processing, there are common use cases for analyzing statistics that involve operations such as calculating the average, minimum, maximum etc., for different endpoints. The Business Rules Manager allows you to define templates and generate business rules from them for different scenarios with common requirements. Watch the following screencast for an introduction to WSO2 SP Business Rules. The following topics cover how to create templates for business rules, create business rules from them and how to manage them. Creating Business Rules Managing Business Rules Creating a Business Rule Template Business Rules Templates Configuring Business Rules Manager Permissions","title":"Working with Business Rules"},{"location":"admin/writing-Custom-Siddhi-Extensions/","text":"Writing Custom Siddhi Extensions Custom extensions can be written in order to apply use case specific logic that is not available in Siddhi out of the box or as an existing extension. There are five types of Siddhi extensions that you can write to cater your specific use cases. These extension archetypes are listed below with their related maven archetypes. You can use these archetypes to generate maven projects for each extension type. siddhi-execution siddhi-io siddhi-map siddhi-script siddhi-store siddhi-execution Siddhi-execution provides following extension types: Function Aggregate Function Stream Function Stream Processor Window You can use one or more from above mentioned extension types and implement according to your requirement. For more information about these extension types, see Siddhi Query Guide - Extensions . To install and implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=org.wso2.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-execution -DarchetypeVersion=1.0.1 -DgroupId=org.wso2.extension.siddhi.execution -Dversion=1.0.0-SNAPSHOT Enter the required execution name in the message that pops up as shown in the example below. Define value for property 'executionType': ML To confirm that all property values are correct, type Y in the console. If not, press N . Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant extension logic. Then build the source code and place the build extension jar in the SP_HOME /lib directory. siddhi-io Siddhi-io provides following extension types: sink source You can use one or more from above mentioned extension types and implement according to your requirement. siddhi-io is generally used to work with IO operations as follows: The Source extension type gets inputs to your Siddhi application. The Sink extension publishes outputs from your Siddhi application. For more information about these extension types, see Siddhi Query Guide - Extensions . To implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=org.wso2.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-io -DarchetypeVersion=1.0.1 -DgroupId=org.wso2.extension.siddhi.io -Dversion=1.0.0-SNAPSHOT Enter the required execution name in the message that pops up as shown in the example below. Define value for property 'typeOf_IO': http To confirm that all property values are correct, type Y in the console. If not, press N . Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant extension logic. Then build the source code and place the build extension jar in the SP_HOME /lib directory. siddhi-map Siddhi-map provides following extension types, Sink Mapper Source Mapper You can use one or more from above mentioned extension types and implement according to your requirement as follows. The Source Mapper maps events to a predefined data format (such as XML, JSON, binary, etc), and publishes them to external endpoints (such as E-mail, TCP, Kafka, HTTP, etc). The Sink Mapper also maps events to a predefined data format, but it does it at the time of publishing events from a Siddhi application. For more information about these extension types, see Siddhi Query Guide - Extensions . To implement the siddhi-map extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=org.wso2.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-map -DarchetypeVersion=1.0.1 -DgroupId=org.wso2.extension.siddhi.map -Dversion=1.0.0-SNAPSHOT Enter the required execution name in the message that pops up as shown in the example below. Define value for property 'typeOf_IO': http To confirm that all property values are correct, type Y in the console. If not, press N . Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant extension logic. Then build the source code and place the build extension jar in the SP_HOME /lib directory. siddhi-script Siddhi-script provides the Script extension type. The script extension type allows you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. For more information about these extension types, see Siddhi Query Guide - Extensions . To implement the siddhi-script extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=org.wso2.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-script -DarchetypeVersion=1.0.1 -DgroupId=org.wso2.extension.siddhi.script -Dversion=1.0.0-SNAPSHOT Enter the required execution name in the message that pops up as shown in the example below. Define value for property 'typeOfScript': To confirm that all property values are correct, type Y in the console. If not, press N . Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant extension logic. Then build the source code and place the build extension jar in the SP_HOME /lib directory. siddhi-store Siddhi-store provides the Store extension type. The Store extension type allows you to work with data/events stored in various data stores through the table abstraction. For more information about these extension types, see Siddhi Query Guide - Extensions . To implement the siddhi-store extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=org.wso2.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-store -DarchetypeVersion=1.0.1 -DgroupId=org.wso2.extension.siddhi.store -Dversion=1.0.0-SNAPSHOT Enter the required execution name in the message that pops up as shown in the example below. Define value for property 'storeType': RDBMS To confirm that all property values are correct, type Y in the console. If not, press N . Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant extension logic. Then build the source code and place the build extension jar in the SP_HOME /lib directory.","title":"Writing Custom Siddhi Extensions"},{"location":"admin/writing-Custom-Siddhi-Extensions/#writing-custom-siddhi-extensions","text":"Custom extensions can be written in order to apply use case specific logic that is not available in Siddhi out of the box or as an existing extension. There are five types of Siddhi extensions that you can write to cater your specific use cases. These extension archetypes are listed below with their related maven archetypes. You can use these archetypes to generate maven projects for each extension type. siddhi-execution siddhi-io siddhi-map siddhi-script siddhi-store","title":"Writing Custom Siddhi Extensions"},{"location":"admin/writing-Custom-Siddhi-Extensions/#siddhi-execution","text":"Siddhi-execution provides following extension types: Function Aggregate Function Stream Function Stream Processor Window You can use one or more from above mentioned extension types and implement according to your requirement. For more information about these extension types, see Siddhi Query Guide - Extensions . To install and implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=org.wso2.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-execution -DarchetypeVersion=1.0.1 -DgroupId=org.wso2.extension.siddhi.execution -Dversion=1.0.0-SNAPSHOT Enter the required execution name in the message that pops up as shown in the example below. Define value for property 'executionType': ML To confirm that all property values are correct, type Y in the console. If not, press N . Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant extension logic. Then build the source code and place the build extension jar in the SP_HOME /lib directory.","title":"siddhi-execution"},{"location":"admin/writing-Custom-Siddhi-Extensions/#siddhi-io","text":"Siddhi-io provides following extension types: sink source You can use one or more from above mentioned extension types and implement according to your requirement. siddhi-io is generally used to work with IO operations as follows: The Source extension type gets inputs to your Siddhi application. The Sink extension publishes outputs from your Siddhi application. For more information about these extension types, see Siddhi Query Guide - Extensions . To implement the siddhi-io extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=org.wso2.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-io -DarchetypeVersion=1.0.1 -DgroupId=org.wso2.extension.siddhi.io -Dversion=1.0.0-SNAPSHOT Enter the required execution name in the message that pops up as shown in the example below. Define value for property 'typeOf_IO': http To confirm that all property values are correct, type Y in the console. If not, press N . Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant extension logic. Then build the source code and place the build extension jar in the SP_HOME /lib directory.","title":"siddhi-io"},{"location":"admin/writing-Custom-Siddhi-Extensions/#siddhi-map","text":"Siddhi-map provides following extension types, Sink Mapper Source Mapper You can use one or more from above mentioned extension types and implement according to your requirement as follows. The Source Mapper maps events to a predefined data format (such as XML, JSON, binary, etc), and publishes them to external endpoints (such as E-mail, TCP, Kafka, HTTP, etc). The Sink Mapper also maps events to a predefined data format, but it does it at the time of publishing events from a Siddhi application. For more information about these extension types, see Siddhi Query Guide - Extensions . To implement the siddhi-map extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=org.wso2.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-map -DarchetypeVersion=1.0.1 -DgroupId=org.wso2.extension.siddhi.map -Dversion=1.0.0-SNAPSHOT Enter the required execution name in the message that pops up as shown in the example below. Define value for property 'typeOf_IO': http To confirm that all property values are correct, type Y in the console. If not, press N . Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant extension logic. Then build the source code and place the build extension jar in the SP_HOME /lib directory.","title":"siddhi-map"},{"location":"admin/writing-Custom-Siddhi-Extensions/#siddhi-script","text":"Siddhi-script provides the Script extension type. The script extension type allows you to write functions in other programming languages and execute them within Siddhi queries. Functions defined via scripts can be accessed in queries similar to any other inbuilt function. For more information about these extension types, see Siddhi Query Guide - Extensions . To implement the siddhi-script extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=org.wso2.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-script -DarchetypeVersion=1.0.1 -DgroupId=org.wso2.extension.siddhi.script -Dversion=1.0.0-SNAPSHOT Enter the required execution name in the message that pops up as shown in the example below. Define value for property 'typeOfScript': To confirm that all property values are correct, type Y in the console. If not, press N . Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant extension logic. Then build the source code and place the build extension jar in the SP_HOME /lib directory.","title":"siddhi-script"},{"location":"admin/writing-Custom-Siddhi-Extensions/#siddhi-store","text":"Siddhi-store provides the Store extension type. The Store extension type allows you to work with data/events stored in various data stores through the table abstraction. For more information about these extension types, see Siddhi Query Guide - Extensions . To implement the siddhi-store extension archetype, follow the procedure below: Issue the following command from your CLI. mvn archetype:generate -DarchetypeGroupId=org.wso2.siddhi.extension.archetype -DarchetypeArtifactId=siddhi-archetype-store -DarchetypeVersion=1.0.1 -DgroupId=org.wso2.extension.siddhi.store -Dversion=1.0.0-SNAPSHOT Enter the required execution name in the message that pops up as shown in the example below. Define value for property 'storeType': RDBMS To confirm that all property values are correct, type Y in the console. If not, press N . Once you perform the above steps, a skeleton source code is created. You need to update this with the relevant extension logic. Then build the source code and place the build extension jar in the SP_HOME /lib directory.","title":"siddhi-store"},{"location":"admin/writing-custom-siddhi-connectors/","text":"Writing Custom Siddhi Connectors","title":"Writing custom siddhi connectors"},{"location":"admin/writing-custom-siddhi-connectors/#writing-custom-siddhi-connectors","text":"","title":"Writing Custom Siddhi Connectors"},{"location":"concepts/concepts/","text":"Concepts Data Streaming Data streaming refers to the continuous transfer of data in high volumes and in high speed. The following are a few examples: Reading information such as the temperature, humidity, etc., per millisecond from a sensor. Capturing the daily transations recorded in an ATM as and when they occur. Tracking information relating to the speed and location of vehicles and other mobile objects. Recording the sales of ononline store in real time. ## Streaming Integration Streaming integration takes place when data from various sources and in different formats flowing at high speed and in high volumes are integrated together. The Stream Processor profile of WSO2 EI performs streaming integration as follows: Allowing downstream applications to access streaming data Different sources of data publish their data via different transports and in different formats. They can disseminate their data in a stream, load it to a data store such as RDBMS, push it into a JMS topic, etc. The Stream Processor profile has the capability to make this data consumable by the different downstream applications, regardless of the transport, the format and the manner in which it is shared. To do make data from a specific source accessible to a downstream application, the Stream Processor may redirect that data to another stream, a data store, a topic etc., that is accessible by that downstream application. To do so, it can communicate with the data source via the relevant transport. The Stream Processor may also do the required transformations to convert the data to a format that can be read by the application. Allowing downstream applications to publish streaming data Once downstream applications process data, you may need to publish the output in different ways. The interfaces to which the output needs to be published may require the data to be delivered via specific transports and presented in specific formats The transport via which the data is published and the data format to use can differ based on the client that receives the output. The Stream Processor profile of WSO2 EI can perform the required transformations to the output so that they are in the format supported by the client, and deliver it via the required transport. It can also direct the output as a stream, to a data store, to a JMS topic etc., as required. To support the above, the Stream Processor profile is shipped with over 60 connectors (i.e.,JMS, HTTP, Kafka, TCP, etc.) that enable it to receive and publish data streams in many formats and protocols, and to store data in both relational and non-relational databases. Event-based messaging An event is a request or a response that adheres to a specific schema. To understand this, consider the example where following information about cash withdrawals from an ATM are captured. Here, the same details are captured for each transaction. In this scenario, each transaction is considered and event, and the details (referred to as atributes) listed above form the event schema. A collection of such events form a stream as depicted in the image below. Event-based messaging is carried out when the all the messages transferred are events that adhere to a specifc event schema. Event-driven integration The Streaming Integration profile allows WSO2 EI to carry out event-driven integration via event based messaging. The stream processing capabilities of the Streaming integratiion profile allows you to detect simple occurrences (e.g.,The number of requests for a specific service exceeding the target), as well as complex events such as a sequence of related events or patterns. Once an occurence, sequence, or pattern is detected, it often requires a response. This response can be a simple response such as the genration of an alert, or the initiation of a complex integration flow. e.g., The following diagram depicts how the streaming integration profile can detect a credit card fraud, and triggers mnultiple actions in response. Stateful integration Stateful integration takes place when the integration flow retains information about the previous requests it handled in memory. This information is taken into account when making decisions in the future. WSO2 EI achieves this via the Stream Processor profile. To understand how stateful integration is useful in real world scenarios, consider an example where you need to throttle requests for a service if it receives more than a fixed number of requests during a defined time span. To do this, the system needs to be aware of the number of requests already received within the defined span of time.","title":"Concepts"},{"location":"concepts/concepts/#concepts","text":"","title":"Concepts"},{"location":"concepts/concepts/#data-streaming","text":"Data streaming refers to the continuous transfer of data in high volumes and in high speed. The following are a few examples: Reading information such as the temperature, humidity, etc., per millisecond from a sensor. Capturing the daily transations recorded in an ATM as and when they occur. Tracking information relating to the speed and location of vehicles and other mobile objects. Recording the sales of ononline store in real time. ## Streaming Integration Streaming integration takes place when data from various sources and in different formats flowing at high speed and in high volumes are integrated together. The Stream Processor profile of WSO2 EI performs streaming integration as follows: Allowing downstream applications to access streaming data Different sources of data publish their data via different transports and in different formats. They can disseminate their data in a stream, load it to a data store such as RDBMS, push it into a JMS topic, etc. The Stream Processor profile has the capability to make this data consumable by the different downstream applications, regardless of the transport, the format and the manner in which it is shared. To do make data from a specific source accessible to a downstream application, the Stream Processor may redirect that data to another stream, a data store, a topic etc., that is accessible by that downstream application. To do so, it can communicate with the data source via the relevant transport. The Stream Processor may also do the required transformations to convert the data to a format that can be read by the application. Allowing downstream applications to publish streaming data Once downstream applications process data, you may need to publish the output in different ways. The interfaces to which the output needs to be published may require the data to be delivered via specific transports and presented in specific formats The transport via which the data is published and the data format to use can differ based on the client that receives the output. The Stream Processor profile of WSO2 EI can perform the required transformations to the output so that they are in the format supported by the client, and deliver it via the required transport. It can also direct the output as a stream, to a data store, to a JMS topic etc., as required. To support the above, the Stream Processor profile is shipped with over 60 connectors (i.e.,JMS, HTTP, Kafka, TCP, etc.) that enable it to receive and publish data streams in many formats and protocols, and to store data in both relational and non-relational databases.","title":"Data Streaming"},{"location":"concepts/concepts/#event-based-messaging","text":"An event is a request or a response that adheres to a specific schema. To understand this, consider the example where following information about cash withdrawals from an ATM are captured. Here, the same details are captured for each transaction. In this scenario, each transaction is considered and event, and the details (referred to as atributes) listed above form the event schema. A collection of such events form a stream as depicted in the image below. Event-based messaging is carried out when the all the messages transferred are events that adhere to a specifc event schema.","title":"Event-based messaging"},{"location":"concepts/concepts/#event-driven-integration","text":"The Streaming Integration profile allows WSO2 EI to carry out event-driven integration via event based messaging. The stream processing capabilities of the Streaming integratiion profile allows you to detect simple occurrences (e.g.,The number of requests for a specific service exceeding the target), as well as complex events such as a sequence of related events or patterns. Once an occurence, sequence, or pattern is detected, it often requires a response. This response can be a simple response such as the genration of an alert, or the initiation of a complex integration flow. e.g., The following diagram depicts how the streaming integration profile can detect a credit card fraud, and triggers mnultiple actions in response.","title":"Event-driven integration"},{"location":"concepts/concepts/#stateful-integration","text":"Stateful integration takes place when the integration flow retains information about the previous requests it handled in memory. This information is taken into account when making decisions in the future. WSO2 EI achieves this via the Stream Processor profile. To understand how stateful integration is useful in real world scenarios, consider an example where you need to throttle requests for a service if it receives more than a fixed number of requests during a defined time span. To do this, the system needs to be aware of the number of requests already received within the defined span of time.","title":"Stateful integration"},{"location":"develop/creating-GDPR-Compliant-Siddhi-Applications/","text":"Creating GDPR Compliant Siddhi Applications The obfuscation/removal of such PII (Personally Identifiable Information) can be handled in WSO2 Stream Processor by Siddhi Applications that can either modify or remove records that contain the PII. These Siddhi Applications can be written in a way to match the original queries that captured data for persistence so that the same data can be modified or removed as required. For more information about writing Siddhi Queries, see Siddhi Query Guide . The following sections explain how obfuscation/deletion of sensitive data can be managed via Siddhi queries in a custom Siddhi application developed based on a specific user case. Obfuscating PII Deleting PII Obfuscating PII Let's consider a Siddhi application that includes the following store definition to persist streaming data. define table customerTable (customerId string, customerName string, entryVector int); In this example, the customer ID is considered PII, and a customer with the XXX ID wants that ID to be hidden in the system so that he/she cannot be personally identified with it. Therefore, you need to obfuscate the value for the customerId attribute. This can be done by creating an algorithm to create a hashed value or a pseudonym to replace a specific value for the customerId attribute. Let's consider that such an algorithm exists (e.g., as a function named anonymize ). To invoke this function, you need to add a new query to the Siddhi application as shown in the sample below. define table customerTable (customerId string, customerName string, entryVector int); define stream UpdateStream (customerId string); from UpdateStream select * update customerTable set customerTable.customerName = anonymize(customerTable.customerName) on customerTable.customerId == XXX; In the above Siddhi application, the query in bold is triggered when a new event is received in the UpdateStream stream where the value for the customerId attribute is XXX. Once it is triggered, the XXX customer ID is replaced with a pseudonym. For more information about writing custom functions, see Siddhi Query Guide - Writing Custom Extensions . Deleting PII Let's assume that the customer ID in the scenario described above needs to be deleted. To do this, you can write a Siddhi query to delete the value for the customerId attribute when is equal to XXX as shown below. define table customerTable (customerId string, customerName string, entryVector int); define stream DeleteStream (customerId string); from DeleteStream delete customerTable on customerTable.customerId == customerId; In the above Siddhi application, the query in bold is triggered when a new event is received in the DeleteStream stream where the value for the customerId attribute is XXX. Once it is triggered, the XXX customer ID is deleted. For more information about the Delete operator used here, see Siddhi Query Guide - Delete .","title":"Creating GDPR Compliant Siddhi Applications"},{"location":"develop/creating-GDPR-Compliant-Siddhi-Applications/#creating-gdpr-compliant-siddhi-applications","text":"The obfuscation/removal of such PII (Personally Identifiable Information) can be handled in WSO2 Stream Processor by Siddhi Applications that can either modify or remove records that contain the PII. These Siddhi Applications can be written in a way to match the original queries that captured data for persistence so that the same data can be modified or removed as required. For more information about writing Siddhi Queries, see Siddhi Query Guide . The following sections explain how obfuscation/deletion of sensitive data can be managed via Siddhi queries in a custom Siddhi application developed based on a specific user case. Obfuscating PII Deleting PII","title":"Creating GDPR Compliant Siddhi Applications"},{"location":"develop/creating-GDPR-Compliant-Siddhi-Applications/#obfuscating-pii","text":"Let's consider a Siddhi application that includes the following store definition to persist streaming data. define table customerTable (customerId string, customerName string, entryVector int); In this example, the customer ID is considered PII, and a customer with the XXX ID wants that ID to be hidden in the system so that he/she cannot be personally identified with it. Therefore, you need to obfuscate the value for the customerId attribute. This can be done by creating an algorithm to create a hashed value or a pseudonym to replace a specific value for the customerId attribute. Let's consider that such an algorithm exists (e.g., as a function named anonymize ). To invoke this function, you need to add a new query to the Siddhi application as shown in the sample below. define table customerTable (customerId string, customerName string, entryVector int); define stream UpdateStream (customerId string); from UpdateStream select * update customerTable set customerTable.customerName = anonymize(customerTable.customerName) on customerTable.customerId == XXX; In the above Siddhi application, the query in bold is triggered when a new event is received in the UpdateStream stream where the value for the customerId attribute is XXX. Once it is triggered, the XXX customer ID is replaced with a pseudonym. For more information about writing custom functions, see Siddhi Query Guide - Writing Custom Extensions .","title":"Obfuscating PII"},{"location":"develop/creating-GDPR-Compliant-Siddhi-Applications/#deleting-pii","text":"Let's assume that the customer ID in the scenario described above needs to be deleted. To do this, you can write a Siddhi query to delete the value for the customerId attribute when is equal to XXX as shown below. define table customerTable (customerId string, customerName string, entryVector int); define stream DeleteStream (customerId string); from DeleteStream delete customerTable on customerTable.customerId == customerId; In the above Siddhi application, the query in bold is triggered when a new event is received in the DeleteStream stream where the value for the customerId attribute is XXX. Once it is triggered, the XXX customer ID is deleted. For more information about the Delete operator used here, see Siddhi Query Guide - Delete .","title":"Deleting PII"},{"location":"develop/creating-a-Siddhi-Application/","text":"Creating a Siddhi Application Siddhi applications are files that define the Siddhi logic to process the events sent to the Streaming Integrator. They are written in the Siddhi Query Language using the Streaming Integrator Studio . A Siddhi file contains the following configurations: Configuration Description Stream A logical series of events ordered in time with a uniquely identifiable name, and set of defined attributes with specific data types defining its schema. Source This consumes data from external sources (such as TCP , Kafka , HTTP , etc) in the form of events, then converts each event (that can be in XML , JSON , binary , etc. format) to a Siddhi event, and passes that to a stream for processing. Sink This takes events arriving at a stream, maps them to a predefined data format (such as XML , JSON, binary , etc), and publishes them to external endpoints (such as E-mail , TCP , Kafka , HTTP , etc). Executional Element An executional element can be one of the following: Stateless query: Queries that only consider currently incoming events when generating an output. e.g., filters Stateful query: Queries that consider both currently incoming events as well as past events when generating an output. e.g., windows, sequences, patterns, etc. Partitions: Collections of stream definitions and Siddhi queries separated from each other within a Siddhi application for the purpose of processing events in parallel and in isolation A Siddhi application can be created from the source view or the design view of the Streaming Integrator Studio. Creating a Siddhi application in the source view To create a Siddhi application via the source view of the Streaming Integrator Studio, follow the steps below: Start the Streaming Integrator Studio by navigating to the SI_TOOLING_HOME /bin directory and issue one of the following commands: For Windows: streaming-integrator-tooling.bat For Linux: ./streaming-integrator-tooling.sh The Stream Processor Studio opens as shown below. Click New to start defining a new Siddhi application. A new file opens as shown below. Add the following sample Siddhi application to the file. @App:name(\"SweetProductionAnalysis\") @Source(type = 'tcp', context='SweetProductionData', @map(type='binary')) define stream SweetProductionStream (name string, amount double); @sink(type='log', @map(type='json')) define stream ProductionAlertStream (name string, amount double); from SweetProductionStream select * insert into ProductionAlertStream; Info Note the following in this Siddhi application Configuration Description Stream This stream contains two stream configurations: SweetProductionStream define stream SweetProductionStream (name string, amount double); This is the input stream that defines the schema based on which events are selected to be processed by the SweetProductionAnalysis Siddhi application. Events received via the source in this application are directed to this stream. ProductionAlertStream define stream ProductionAlertStream (name string, amount double); This is the output stream from which the sink configured in this application takes events to be published as the output. Source @Source(type = tcp , context= SweetProductionData , @map(type= binary )) This source configuration has the following sections: @Source(type = \u2018tcp\u2019, context= SweetProductionData This configuration defines tcp as the transport via which events are received to be processed by the SweetProductionAnalysis Siddhi application. @map(type= binary )) This configuration defines the input mapping. In this scenario, Binary Mapper is used which converts input events into binary events and feeds them into siddhi. The source types and map types are available as Siddhi extensions, and you can find via the operator finder as follows: Click the Operator Finder icon to open the Operator Finder. Move the cursor to the location in the Siddhi application where you want to add the source. Search for the required transport type. Once it appears in the search results, click the Add to Source icon on it. Similarly, search for the mapping type you want to include in the source configuration, and add it. The source annotation is now displayed as follows. You can add the other properties as required, and save your changes. Sink @sink(type= log , @map(type= json )) This sink configuration has the following sections: @sink(type= log ) This configuration defines log as the transport via which the processed events are published from the ProductionAlertStream output stream. Log sink simply publishes events into the console. @map(type= json )) This configuration defines the output mapping. Events are published with the json mapping type. Json mapper converts the events in the ProductionAlertStream to the Json format. You can select the sink type and the map type from the Operator Finder . Executional Elements from SweetProductionStream select * insert into ProductionAlertStream; This is where the logic of the siddhi app is defined. In this scenario, all the events received in the SweetProductionStream input stream are inserted into the ProductionAlertStream output stream. To save this Siddhi application, click File , and then click Save . By default siddhi applications are saved in the SI_HOME /wso2/editor/deployment/workspace directory. To export the Siddhi application to your preferred location, click File , and then click Export File . To see a graphical view of the event flow you defined in your Siddhi application, click Design View . The event flow is displayed as follows. {width=\"900\"} Creating a Siddhi application in the design view To create a Siddhi application via the design view of the WSO2 SP Stream Processor Studio, follow the steps below: Start the Streaming Integrator Studio by navigating to the SI_TOOLING_HOME /bin directory and issue one of the following commands: For Windows: streaming-integrator-tooling.bat For Linux: ./streaming-integrator-tooling.sh The Stream Processor Studio opens as shown below. Click New to start defining a new Siddhi application. A new file opens as shown below. To open the design view, click Design View . To define the input stream into which the events to be processed via the Siddhi application should be received, drag and drop the stream icon (shown below) into the grid. Once the stream component is added to the grid, move the cursor over it, and then click on the settings icon as shown below. As as result, the Stream Configuration form opens as follows. Fill this form as follows to define a stream named SweetProductionStream with two attributes named name and amount : In the Name field, enter SweetProductionStream . In the Attributes table, enter two attributes as follows. You can click +Attribute to add a new row in the table to define a new attribute. Attribute Name Attribute Type name string amount double Click Submit to save the new stream definition. As a result, the stream is displayed on the grid with the SweetProductionStream label as shown below. To define the output stream to which the processed events need to be directed, drag and drop the stream icon again. Place it after the SweetProductionStream stream. This stream should be named ProductionAlertStream and have the following attributes. Attribute Name Attribute Type name string totalProduction long To add the source from which events are received, drag and drop the source icon (shown below) into the grid. The source is an input to the SweetProductionStream input stream component. Therefore, place this source component to the left of the input stream component in the grid. Once you add the source component, draw a line from it to the SweetProductionStream input stream component by dragging the cursor as demonstrated below. Click the settings icon on the source component you added to open the Source Configuration form. Then enter information as follows. In the Source Type field, select tcp . For this example, assume that events are received in the binary format. To indicate that events are expected to be converted from this format, select binary in the Map Type field. To indicate the context, select the context check box and enter SweetProductionData in the field that appears below. Click Submit. To add a query that defines the execution logic, drag and drop the projection query icon (shown below) to the grid. The query uses the events in the SweetProductionStream input stream as inputs and directs the processed events (which are its output) to the ProductionAlertStream output stream. Therefore, create two connections as demonstrated below. To define the execution logic, move the cursor over the query in the grid, and click on the settings icon that appears. This opens the Query Configuration form. Enter information in it as follows: Enter a name for the query in the Name field. In this example, let's enter query as the name. In order to specify how each user defined attribute in the input stream is converted to generate the output events, select User Defined Attributes in the Select field. As a result, the User Defined Attributes table appears. The As column of this table displays the attributes of the output stream. To derive the value for each attribute, enter required expressions/values in the Expression column as explained below. The value for name can be derived from the input stream without any further processing. Therefore, enter name as the expression for the name attribute. To derive the value for the totalProduction attribute, the sum of the values for the amount attribute of input events need to be calculated. Therefore, enter the expression as follows to apply the sum() Siddhi function to the amount attribute. sum(amount) Leave the default values of the Output section unchanged. Click Submit to save the information. To add a sink to publish the output events that are directed to the ProductionAlertStream output stream, drag and drop the sink icon (shown below) into the grid. Draw an arrow from the ProductionAlertStream output stream to the sink component to connect them. Click the settings icon on the sink component you added to open the Sink Configuration form. Then enter information as follows. 1. In this example, let's assume that output needs to be generated as logs in the console. To indicate this, select log in the Sink Type field. In the Map Type field, select the format in which the output must be generated. For this example, let's select json . Click Submit to save the information. To align the Siddhi components that you have added to the grid, click Edit and then click Auto-Align . As a result, all the components are horizontally aligned as shown below. Click Source View . The siddhi application is displayed as follows. Click File and then click Save as . The Save to Workspace dialog box appears. In the File Name field, enter SweetProductionAnalysis and click Save .","title":"Creating Siddhi Applications"},{"location":"develop/creating-a-Siddhi-Application/#creating-a-siddhi-application","text":"Siddhi applications are files that define the Siddhi logic to process the events sent to the Streaming Integrator. They are written in the Siddhi Query Language using the Streaming Integrator Studio . A Siddhi file contains the following configurations: Configuration Description Stream A logical series of events ordered in time with a uniquely identifiable name, and set of defined attributes with specific data types defining its schema. Source This consumes data from external sources (such as TCP , Kafka , HTTP , etc) in the form of events, then converts each event (that can be in XML , JSON , binary , etc. format) to a Siddhi event, and passes that to a stream for processing. Sink This takes events arriving at a stream, maps them to a predefined data format (such as XML , JSON, binary , etc), and publishes them to external endpoints (such as E-mail , TCP , Kafka , HTTP , etc). Executional Element An executional element can be one of the following: Stateless query: Queries that only consider currently incoming events when generating an output. e.g., filters Stateful query: Queries that consider both currently incoming events as well as past events when generating an output. e.g., windows, sequences, patterns, etc. Partitions: Collections of stream definitions and Siddhi queries separated from each other within a Siddhi application for the purpose of processing events in parallel and in isolation A Siddhi application can be created from the source view or the design view of the Streaming Integrator Studio.","title":"Creating a Siddhi Application"},{"location":"develop/creating-a-Siddhi-Application/#creating-a-siddhi-application-in-the-source-view","text":"To create a Siddhi application via the source view of the Streaming Integrator Studio, follow the steps below: Start the Streaming Integrator Studio by navigating to the SI_TOOLING_HOME /bin directory and issue one of the following commands: For Windows: streaming-integrator-tooling.bat For Linux: ./streaming-integrator-tooling.sh The Stream Processor Studio opens as shown below. Click New to start defining a new Siddhi application. A new file opens as shown below. Add the following sample Siddhi application to the file. @App:name(\"SweetProductionAnalysis\") @Source(type = 'tcp', context='SweetProductionData', @map(type='binary')) define stream SweetProductionStream (name string, amount double); @sink(type='log', @map(type='json')) define stream ProductionAlertStream (name string, amount double); from SweetProductionStream select * insert into ProductionAlertStream; Info Note the following in this Siddhi application Configuration Description Stream This stream contains two stream configurations: SweetProductionStream define stream SweetProductionStream (name string, amount double); This is the input stream that defines the schema based on which events are selected to be processed by the SweetProductionAnalysis Siddhi application. Events received via the source in this application are directed to this stream. ProductionAlertStream define stream ProductionAlertStream (name string, amount double); This is the output stream from which the sink configured in this application takes events to be published as the output. Source @Source(type = tcp , context= SweetProductionData , @map(type= binary )) This source configuration has the following sections: @Source(type = \u2018tcp\u2019, context= SweetProductionData This configuration defines tcp as the transport via which events are received to be processed by the SweetProductionAnalysis Siddhi application. @map(type= binary )) This configuration defines the input mapping. In this scenario, Binary Mapper is used which converts input events into binary events and feeds them into siddhi. The source types and map types are available as Siddhi extensions, and you can find via the operator finder as follows: Click the Operator Finder icon to open the Operator Finder. Move the cursor to the location in the Siddhi application where you want to add the source. Search for the required transport type. Once it appears in the search results, click the Add to Source icon on it. Similarly, search for the mapping type you want to include in the source configuration, and add it. The source annotation is now displayed as follows. You can add the other properties as required, and save your changes. Sink @sink(type= log , @map(type= json )) This sink configuration has the following sections: @sink(type= log ) This configuration defines log as the transport via which the processed events are published from the ProductionAlertStream output stream. Log sink simply publishes events into the console. @map(type= json )) This configuration defines the output mapping. Events are published with the json mapping type. Json mapper converts the events in the ProductionAlertStream to the Json format. You can select the sink type and the map type from the Operator Finder . Executional Elements from SweetProductionStream select * insert into ProductionAlertStream; This is where the logic of the siddhi app is defined. In this scenario, all the events received in the SweetProductionStream input stream are inserted into the ProductionAlertStream output stream. To save this Siddhi application, click File , and then click Save . By default siddhi applications are saved in the SI_HOME /wso2/editor/deployment/workspace directory. To export the Siddhi application to your preferred location, click File , and then click Export File . To see a graphical view of the event flow you defined in your Siddhi application, click Design View . The event flow is displayed as follows. {width=\"900\"}","title":"Creating a Siddhi application in the source view"},{"location":"develop/creating-a-Siddhi-Application/#creating-a-siddhi-application-in-the-design-view","text":"To create a Siddhi application via the design view of the WSO2 SP Stream Processor Studio, follow the steps below: Start the Streaming Integrator Studio by navigating to the SI_TOOLING_HOME /bin directory and issue one of the following commands: For Windows: streaming-integrator-tooling.bat For Linux: ./streaming-integrator-tooling.sh The Stream Processor Studio opens as shown below. Click New to start defining a new Siddhi application. A new file opens as shown below. To open the design view, click Design View . To define the input stream into which the events to be processed via the Siddhi application should be received, drag and drop the stream icon (shown below) into the grid. Once the stream component is added to the grid, move the cursor over it, and then click on the settings icon as shown below. As as result, the Stream Configuration form opens as follows. Fill this form as follows to define a stream named SweetProductionStream with two attributes named name and amount : In the Name field, enter SweetProductionStream . In the Attributes table, enter two attributes as follows. You can click +Attribute to add a new row in the table to define a new attribute. Attribute Name Attribute Type name string amount double Click Submit to save the new stream definition. As a result, the stream is displayed on the grid with the SweetProductionStream label as shown below. To define the output stream to which the processed events need to be directed, drag and drop the stream icon again. Place it after the SweetProductionStream stream. This stream should be named ProductionAlertStream and have the following attributes. Attribute Name Attribute Type name string totalProduction long To add the source from which events are received, drag and drop the source icon (shown below) into the grid. The source is an input to the SweetProductionStream input stream component. Therefore, place this source component to the left of the input stream component in the grid. Once you add the source component, draw a line from it to the SweetProductionStream input stream component by dragging the cursor as demonstrated below. Click the settings icon on the source component you added to open the Source Configuration form. Then enter information as follows. In the Source Type field, select tcp . For this example, assume that events are received in the binary format. To indicate that events are expected to be converted from this format, select binary in the Map Type field. To indicate the context, select the context check box and enter SweetProductionData in the field that appears below. Click Submit. To add a query that defines the execution logic, drag and drop the projection query icon (shown below) to the grid. The query uses the events in the SweetProductionStream input stream as inputs and directs the processed events (which are its output) to the ProductionAlertStream output stream. Therefore, create two connections as demonstrated below. To define the execution logic, move the cursor over the query in the grid, and click on the settings icon that appears. This opens the Query Configuration form. Enter information in it as follows: Enter a name for the query in the Name field. In this example, let's enter query as the name. In order to specify how each user defined attribute in the input stream is converted to generate the output events, select User Defined Attributes in the Select field. As a result, the User Defined Attributes table appears. The As column of this table displays the attributes of the output stream. To derive the value for each attribute, enter required expressions/values in the Expression column as explained below. The value for name can be derived from the input stream without any further processing. Therefore, enter name as the expression for the name attribute. To derive the value for the totalProduction attribute, the sum of the values for the amount attribute of input events need to be calculated. Therefore, enter the expression as follows to apply the sum() Siddhi function to the amount attribute. sum(amount) Leave the default values of the Output section unchanged. Click Submit to save the information. To add a sink to publish the output events that are directed to the ProductionAlertStream output stream, drag and drop the sink icon (shown below) into the grid. Draw an arrow from the ProductionAlertStream output stream to the sink component to connect them. Click the settings icon on the sink component you added to open the Sink Configuration form. Then enter information as follows. 1. In this example, let's assume that output needs to be generated as logs in the console. To indicate this, select log in the Sink Type field. In the Map Type field, select the format in which the output must be generated. For this example, let's select json . Click Submit to save the information. To align the Siddhi components that you have added to the grid, click Edit and then click Auto-Align . As a result, all the components are horizontally aligned as shown below. Click Source View . The siddhi application is displayed as follows. Click File and then click Save as . The Save to Workspace dialog box appears. In the File Name field, enter SweetProductionAnalysis and click Save .","title":"Creating a Siddhi application in the design view"},{"location":"develop/debugging_siddhi_applications/","text":"Debugging a Siddhi Application WSO2 Streaming Integrator Studio allows debugging tasks to be carried out to ensure that the Siddhi applications you create and deploy are validated before they are run on an actual production environment. To debug a Siddhi application, you can run it in the debug mode, apply debug point and then run event simulation so that the specific debug points are analyzed. To run a Siddhi application in the debug mode, follow the procedure below: Info A Siddhi application can be run in the debug mode only in the source view. Start the Streaming Integrator Studio following the instructions in Starting Stream Integration Studio . You are directed to the welcome-page. In this scenario, let's use the existing sample Siddhi application named ReceiveAndCount to demostrate the debugging functionality. To open this Siddhi application, click on the sample. The ReceiveAndCount Siddhi application opens inca new tab. In order to debug the Siddhi file, you need to first save it in the workspace directory. To do this, click File => Save . In the Save to Workspace dialog box that appears, click Save . To run the Siddhi application in the debug mode, click Run => Debug . !!! info This menu option is enabled only when the Siddhi file is saved in the workspace directory as specified in the previous step. As a result, the following log is printed in the console. Also, another console tab is opened with debug options as shown below. Apply debug points for the required queries. To mark a debug point, you need to click on the left of the required line number so that it is marked with a dot as shown in the image below. Info You can only mark lines with from or insert into statements as debug points. Simulate one or more events for the SweetProductionStream stream in the Siddhi application. The first line that is marked as a debug point is highlighted as shown below when they are hit. Info For detailed instructions to simulate events, see the following sections: Simulating a Single Event Simulating Multiple Events via CS Files Simulating Multiple Events via Databases Generating Random Data Two viewing options are provided under both Event State and the Query State sections of the Debug tab for each debug point hit as shown above. To expand the tree and understand the details of the processed attributes and their values etc., click the following icon for the relevant query. When you observe the details, note that the value for outputData in the Event State section is null. This is because the debug point is still at beginning of the query. Also note that the value calculated via the count() function is still displayed as 0 in the Query State section. The following icons are displayed in the Debug tab of the console: Icon Description Click this to proceed from the current debug point to the next available debug point. If there is no debug point marked after the current debug point, the existing debug point continues to be displayed in the tab. Click this to proceed from the current debug point even if no debug point exists after it. Once you navigate to next debug point and see the details by clicking the plus signs as mentioned above you can further analyze the processed attributes and its values as shown below. Note that after the count() aggregate function, a value of 1 has been calculated.","title":"Debugging Siddhi Applications"},{"location":"develop/debugging_siddhi_applications/#debugging-a-siddhi-application","text":"WSO2 Streaming Integrator Studio allows debugging tasks to be carried out to ensure that the Siddhi applications you create and deploy are validated before they are run on an actual production environment. To debug a Siddhi application, you can run it in the debug mode, apply debug point and then run event simulation so that the specific debug points are analyzed. To run a Siddhi application in the debug mode, follow the procedure below: Info A Siddhi application can be run in the debug mode only in the source view. Start the Streaming Integrator Studio following the instructions in Starting Stream Integration Studio . You are directed to the welcome-page. In this scenario, let's use the existing sample Siddhi application named ReceiveAndCount to demostrate the debugging functionality. To open this Siddhi application, click on the sample. The ReceiveAndCount Siddhi application opens inca new tab. In order to debug the Siddhi file, you need to first save it in the workspace directory. To do this, click File => Save . In the Save to Workspace dialog box that appears, click Save . To run the Siddhi application in the debug mode, click Run => Debug . !!! info This menu option is enabled only when the Siddhi file is saved in the workspace directory as specified in the previous step. As a result, the following log is printed in the console. Also, another console tab is opened with debug options as shown below. Apply debug points for the required queries. To mark a debug point, you need to click on the left of the required line number so that it is marked with a dot as shown in the image below. Info You can only mark lines with from or insert into statements as debug points. Simulate one or more events for the SweetProductionStream stream in the Siddhi application. The first line that is marked as a debug point is highlighted as shown below when they are hit. Info For detailed instructions to simulate events, see the following sections: Simulating a Single Event Simulating Multiple Events via CS Files Simulating Multiple Events via Databases Generating Random Data Two viewing options are provided under both Event State and the Query State sections of the Debug tab for each debug point hit as shown above. To expand the tree and understand the details of the processed attributes and their values etc., click the following icon for the relevant query. When you observe the details, note that the value for outputData in the Event State section is null. This is because the debug point is still at beginning of the query. Also note that the value calculated via the count() function is still displayed as 0 in the Query State section. The following icons are displayed in the Debug tab of the console: Icon Description Click this to proceed from the current debug point to the next available debug point. If there is no debug point marked after the current debug point, the existing debug point continues to be displayed in the tab. Click this to proceed from the current debug point even if no debug point exists after it. Once you navigate to next debug point and see the details by clicking the plus signs as mentioned above you can further analyze the processed attributes and its values as shown below. Note that after the count() aggregate function, a value of 1 has been calculated.","title":"Debugging a Siddhi Application"},{"location":"develop/deploying-Streaming-Applications/","text":"Deploying Streaming Applications A Siddhi application ( .siddhi file ) can be deployed to run that on production. There are two modes deployment Standard deployment Here we deploy Siddhi Apps in worker node (In Single node, Minimum HA Deployments) Fully distributed deployment Here we deploy Siddhi Apps in manager node (Only in Fully Distributed Deployment) Refer below sections to get to know how to deploy on the above deployments: Standard Deployment Fully Distributed Deployment Standard Deployment To start a worker node in single node mode, issue one of the following commands: For Windows: worker.bat For Linux : ./worker.sh Siddhi applications can be deployed to the worker node by using one of the following methods: Using Stream Processor Studio This involves deploying the Siddhi applications via the Stream Processor studio once you create and save them. To do this, follow the substeps below. !!! tip This method allows you to deploy multiple Siddhi applications to mutiple servers at once. Open the Stream Processor Studio. For detailed instructions, see Stream Processor Studio Overview . In the top menu bar, click Deploy and then click Deploy to Server . {width=\"850\"} If you want to deploy all the Siddhi applications saved in the Stream Processor Studio, select the check box for the Workspace directory as shown in the example below. If not, select the check boxes of the required Siddhi applications. {height=\"250\"} Add the servers to which you want to deploy the Siddhi applications as follows: {width=\"771\"} In the Host field, enter the host of the server. In the Port field, enter the port of the server. In the User Name field enter the user name to log in to the server. In the Password field, enter the password to log in to the server. Click Add to add the server. Repeat the above substeps to specify all the servers to which the Siddhi applications you selected need to be deployed. Once you have added all the required servers, click Deploy . As a result, a log appears to indicate whether the selected Siddhi applications are successfully deployed as shown in the example below. {width=\"771\"} Deploying manually This involves dropping the .siddhi file in to the SP_HOME /wso2/worker/deployment/siddhi-files/ directory before or after starting the worker node. Via REST API This involves sending a \"POST\" request to http://:/siddhi-apps with the Siddhi App included in the body of the request. Refer Stream Processor REST API Guide for more information on using WSO2 Strean Processor APIs. When a Siddhi application is successfully deployed, a message similar to the following example appears in the startup logs. To configure a Minimum HA deployment refer Minimum High Availability Deployment documentation. Fully Distributed Deployment To successfully set up, configure and run Siddhi applications in a fully distributed environment refer Fully Distributed Deployment documentation. Info If you need to deploy streaming applications in a Docker environment, they need to be exported as Docker artifacts. For mo re information, see Exporting Siddhi Files as Docker Artifacts .","title":"Deploying Streaming Applications"},{"location":"develop/deploying-Streaming-Applications/#deploying-streaming-applications","text":"A Siddhi application ( .siddhi file ) can be deployed to run that on production. There are two modes deployment Standard deployment Here we deploy Siddhi Apps in worker node (In Single node, Minimum HA Deployments) Fully distributed deployment Here we deploy Siddhi Apps in manager node (Only in Fully Distributed Deployment) Refer below sections to get to know how to deploy on the above deployments: Standard Deployment Fully Distributed Deployment","title":"Deploying Streaming Applications"},{"location":"develop/deploying-Streaming-Applications/#standard-deployment","text":"To start a worker node in single node mode, issue one of the following commands: For Windows: worker.bat For Linux : ./worker.sh Siddhi applications can be deployed to the worker node by using one of the following methods: Using Stream Processor Studio This involves deploying the Siddhi applications via the Stream Processor studio once you create and save them. To do this, follow the substeps below. !!! tip This method allows you to deploy multiple Siddhi applications to mutiple servers at once. Open the Stream Processor Studio. For detailed instructions, see Stream Processor Studio Overview . In the top menu bar, click Deploy and then click Deploy to Server . {width=\"850\"} If you want to deploy all the Siddhi applications saved in the Stream Processor Studio, select the check box for the Workspace directory as shown in the example below. If not, select the check boxes of the required Siddhi applications. {height=\"250\"} Add the servers to which you want to deploy the Siddhi applications as follows: {width=\"771\"} In the Host field, enter the host of the server. In the Port field, enter the port of the server. In the User Name field enter the user name to log in to the server. In the Password field, enter the password to log in to the server. Click Add to add the server. Repeat the above substeps to specify all the servers to which the Siddhi applications you selected need to be deployed. Once you have added all the required servers, click Deploy . As a result, a log appears to indicate whether the selected Siddhi applications are successfully deployed as shown in the example below. {width=\"771\"} Deploying manually This involves dropping the .siddhi file in to the SP_HOME /wso2/worker/deployment/siddhi-files/ directory before or after starting the worker node. Via REST API This involves sending a \"POST\" request to http://:/siddhi-apps with the Siddhi App included in the body of the request. Refer Stream Processor REST API Guide for more information on using WSO2 Strean Processor APIs. When a Siddhi application is successfully deployed, a message similar to the following example appears in the startup logs. To configure a Minimum HA deployment refer Minimum High Availability Deployment documentation.","title":"Standard Deployment"},{"location":"develop/deploying-Streaming-Applications/#fully-distributed-deployment","text":"To successfully set up, configure and run Siddhi applications in a fully distributed environment refer Fully Distributed Deployment documentation. Info If you need to deploy streaming applications in a Docker environment, they need to be exported as Docker artifacts. For mo re information, see Exporting Siddhi Files as Docker Artifacts .","title":"Fully Distributed Deployment"},{"location":"develop/designing_siddhi_applications/","text":"Designing Siddhi Applications","title":"Designing siddhi applications"},{"location":"develop/designing_siddhi_applications/#designing-siddhi-applications","text":"","title":"Designing Siddhi Applications"},{"location":"develop/developing-Streaming-Applications/","text":"Developing Streaming Applications The following topics cover information on writing Siddhi Streaming Applications for WSO2 Stream Processor: Siddhi Application Overview Collecting Events Processing Streaming Events Storage Integration Complex Event Processing Machine Learning Incremental Analysis Publishing Events Converting to a Distributed Streaming Application Fault Handling","title":"Developing Streaming Applications"},{"location":"develop/developing-Streaming-Applications/#developing-streaming-applications","text":"The following topics cover information on writing Siddhi Streaming Applications for WSO2 Stream Processor: Siddhi Application Overview Collecting Events Processing Streaming Events Storage Integration Complex Event Processing Machine Learning Incremental Analysis Publishing Events Converting to a Distributed Streaming Application Fault Handling","title":"Developing Streaming Applications"},{"location":"develop/exporting-Siddhi-Files-as-Docker-Artifacts/","text":"Exporting Siddhi Files as Docker Artifacts The Stream Processor Studio allows you to export one or more selected Siddhi files as Docker artifacts in order to make it possible to run those Siddhi applications in a Docker environment. To export Siddhi files as Docker artifacts, follow the steps below: Tip Before you begin: The exported Docker artifacts use docker-compose for this purpose. Therefore to run the artifacts, you need to install the following in the running environment. Docker Docker Compose Configuring Docker in SP By default, WSO2 SP uses the latest docker image hosted in wso2.docker.com . If you want to use the different version of an image, follow the steps below: Update the Configurations for Docker export section of the SP_HOME /conf/editor/deployment.yaml file as follows. # Configurations for Docker export feature docker.configs: productVersion: PREFERED_PRODUCT_VERSION You need to replace the PREFERED_PRODUCT_VERSION with your preferred image version e.g., 4.3.0. Restart the WSO2 SP server. Info This feature also uses the following configuration files with the content as shown below: docker-compose.editor.yaml version: '2.3' services: editor: image: docker.wso2.com/wso2sp-editor:{{PRODUCT_VERSION}} container_name: wso2sp-editor ports: - \"9390:9390\" healthcheck: test: [\"CMD\", \"nc\", \"-z\",\"localhost\", \"9390\"] interval: 10s timeout: 120s retries: 5 volumes: - ./siddhi-files:/home/wso2carbon/wso2-artifact-volume/wso2/editor/deployment/workspace docker-compose.worker.yml version: '2.3' services: editor: image: docker.wso2.com/wso2sp-worker:{{PRODUCT_VERSION}} container_name: wso2sp-worker ports: - \"9090:9090\" healthcheck: test: [\"CMD\", \"nc\", \"-z\",\"localhost\", \"9090\"] interval: 10s timeout: 120s retries: 5 volumes: - ./siddhi-files:/home/wso2carbon/wso2-artifact-volume/wso2/worker/deployment/siddhi-files Exporting Siddhi files To export multiple Siddhi files, follow the steps below: Start and access the Stream Processor Studio as instructed in Stream Processor Studio Overview - Starting the Stream Processor Studio . Click File , and then click Export as Docker . {width=\"258\"} As a result, the Export as Docker dialog box opens as follows. {width=\"480\"} In the Profile field, select either Editor or Worker depending on the profile in which the Siddhi file you want to export is located. Under Files to be included , expand the workspace directory to view all the Siddhi applications that are available to be exported. {height=\"250\"} Select the speific Siddhi applications you want to export by selecting the relevant check boxes. If you want to select all the Siddhi applications, select the check box for the workspace directory. Click Export . The Siddhi applications are exported as docker artifacts in a zip file to the default location in your machine, based on your operating system and browser settings.","title":"Exporting Siddhi Files as Docker Artifacts"},{"location":"develop/exporting-Siddhi-Files-as-Docker-Artifacts/#exporting-siddhi-files-as-docker-artifacts","text":"The Stream Processor Studio allows you to export one or more selected Siddhi files as Docker artifacts in order to make it possible to run those Siddhi applications in a Docker environment. To export Siddhi files as Docker artifacts, follow the steps below: Tip Before you begin: The exported Docker artifacts use docker-compose for this purpose. Therefore to run the artifacts, you need to install the following in the running environment. Docker Docker Compose","title":"Exporting Siddhi Files as Docker Artifacts"},{"location":"develop/exporting-Siddhi-Files-as-Docker-Artifacts/#configuring-docker-in-sp","text":"By default, WSO2 SP uses the latest docker image hosted in wso2.docker.com . If you want to use the different version of an image, follow the steps below: Update the Configurations for Docker export section of the SP_HOME /conf/editor/deployment.yaml file as follows. # Configurations for Docker export feature docker.configs: productVersion: PREFERED_PRODUCT_VERSION You need to replace the PREFERED_PRODUCT_VERSION with your preferred image version e.g., 4.3.0. Restart the WSO2 SP server. Info This feature also uses the following configuration files with the content as shown below: docker-compose.editor.yaml version: '2.3' services: editor: image: docker.wso2.com/wso2sp-editor:{{PRODUCT_VERSION}} container_name: wso2sp-editor ports: - \"9390:9390\" healthcheck: test: [\"CMD\", \"nc\", \"-z\",\"localhost\", \"9390\"] interval: 10s timeout: 120s retries: 5 volumes: - ./siddhi-files:/home/wso2carbon/wso2-artifact-volume/wso2/editor/deployment/workspace docker-compose.worker.yml version: '2.3' services: editor: image: docker.wso2.com/wso2sp-worker:{{PRODUCT_VERSION}} container_name: wso2sp-worker ports: - \"9090:9090\" healthcheck: test: [\"CMD\", \"nc\", \"-z\",\"localhost\", \"9090\"] interval: 10s timeout: 120s retries: 5 volumes: - ./siddhi-files:/home/wso2carbon/wso2-artifact-volume/wso2/worker/deployment/siddhi-files","title":"Configuring Docker in SP"},{"location":"develop/exporting-Siddhi-Files-as-Docker-Artifacts/#exporting-siddhi-files","text":"To export multiple Siddhi files, follow the steps below: Start and access the Stream Processor Studio as instructed in Stream Processor Studio Overview - Starting the Stream Processor Studio . Click File , and then click Export as Docker . {width=\"258\"} As a result, the Export as Docker dialog box opens as follows. {width=\"480\"} In the Profile field, select either Editor or Worker depending on the profile in which the Siddhi file you want to export is located. Under Files to be included , expand the workspace directory to view all the Siddhi applications that are available to be exported. {height=\"250\"} Select the speific Siddhi applications you want to export by selecting the relevant check boxes. If you want to select all the Siddhi applications, select the check box for the workspace directory. Click Export . The Siddhi applications are exported as docker artifacts in a zip file to the default location in your machine, based on your operating system and browser settings.","title":"Exporting Siddhi files"},{"location":"develop/exporting-Siddhi-Files/","text":"Exporting Siddhi Files The Siddhi files you create in the Stream Processor Studio are saved in the SP_HOME /wso2/editor/deployment/workspace directory by default. If you want to save a Siddhi file in a different location, you can export it to the required location. You can also enable Siddhi applications to be run in a Docker environment by exporting them as Docker artifacts. For detailed information about the different purposes for whcih the Siddhi applications are exported and the methods you can adopt to export them, see the following sections. Exporting a Single Siddhi File Exporting Siddhi Files as Docker Artifacts","title":"Exporting Siddhi Files"},{"location":"develop/exporting-Siddhi-Files/#exporting-siddhi-files","text":"The Siddhi files you create in the Stream Processor Studio are saved in the SP_HOME /wso2/editor/deployment/workspace directory by default. If you want to save a Siddhi file in a different location, you can export it to the required location. You can also enable Siddhi applications to be run in a Docker environment by exporting them as Docker artifacts. For detailed information about the different purposes for whcih the Siddhi applications are exported and the methods you can adopt to export them, see the following sections. Exporting a Single Siddhi File Exporting Siddhi Files as Docker Artifacts","title":"Exporting Siddhi Files"},{"location":"develop/exporting-a-Single-Siddhi-File/","text":"Exporting a Single Siddhi File If you want to copy a specific Siddhi application to multiple SP or other SP based WSO2 product instances, you can export it as follows: Start WSO2 SP in the editor mode and access the Stream Processor Studio. For detailed instructions, see Starting Stream Processor Studio . The Stream Processor Studio opens as shown below. {width=\"900\"} Open the Siddhi application you want to export. You can click Open and select a Siddhi application that is already saved in the SP_HOME /wso2/editor/deployment/workspace directory. Click File => Export File . This opens the native file browser opens as shown below. {width=\"600\"} Browse for the required directory path and click Save . Info This functionality differs based on the web browser you are using and its settings. e.g., if you have set a default download location and disabled the Ask where to save each file before downloading feature as shown below, the file is downloaded to the default location without prompting you for any further input.","title":"Exporting a Single Siddhi File"},{"location":"develop/exporting-a-Single-Siddhi-File/#exporting-a-single-siddhi-file","text":"If you want to copy a specific Siddhi application to multiple SP or other SP based WSO2 product instances, you can export it as follows: Start WSO2 SP in the editor mode and access the Stream Processor Studio. For detailed instructions, see Starting Stream Processor Studio . The Stream Processor Studio opens as shown below. {width=\"900\"} Open the Siddhi application you want to export. You can click Open and select a Siddhi application that is already saved in the SP_HOME /wso2/editor/deployment/workspace directory. Click File => Export File . This opens the native file browser opens as shown below. {width=\"600\"} Browse for the required directory path and click Save . Info This functionality differs based on the web browser you are using and its settings. e.g., if you have set a default download location and disabled the Ask where to save each file before downloading feature as shown below, the file is downloaded to the default location without prompting you for any further input.","title":"Exporting a Single Siddhi File"},{"location":"develop/ndex/","text":"SP440 (Stream Processor 4.4.0) Available Pages: Stream Processor Studio Overview Working with the Design View Creating a Siddhi Application Testing a Siddhi Application Simulating Events Simulating a Single Event Simulating Multiple Events via CSV Files Simulating Multiple Events via Databases Generating Random Data Debugging a Siddhi Application Exporting Siddhi Files Exporting a Single Siddhi File Exporting Siddhi Files as Docker Artifacts","title":"SP440 (Stream Processor 4.4.0)"},{"location":"develop/ndex/#sp440-stream-processor-440","text":"","title":"SP440 (Stream Processor 4.4.0)"},{"location":"develop/ndex/#available-pages","text":"Stream Processor Studio Overview Working with the Design View Creating a Siddhi Application Testing a Siddhi Application Simulating Events Simulating a Single Event Simulating Multiple Events via CSV Files Simulating Multiple Events via Databases Generating Random Data Debugging a Siddhi Application Exporting Siddhi Files Exporting a Single Siddhi File Exporting Siddhi Files as Docker Artifacts","title":"Available Pages:"},{"location":"develop/siddhi-Application-Overview/","text":"Siddhi Application Overview A Siddhi application (.siddhi) file is the deployment artifact containing the Stream Processing logic for WSO2 Stream Processor. The format of a Siddhi application is as follows: @App:name(\"ReceiveAndCount\") @App:description('Receive events via HTTP transport and view the output on the console') /* Sample Siddhi App block comment */ -- Sample Siddhi App line comment @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream TotalCountStream (totalCount long); -- Count the incoming events @info(name='query1') from SweetProductionStream select count() as totalCount insert into TotalCountStream; Basic information about Siddhi applications Following are some important things to note about Siddhi applications: The file name of each Siddhi application must be equal to the name specified via the @App:name() annotation. e.g., In the sample Siddhi application given above, the application name is ReceiveAndCount . Therefore, the Siddhi file name must be ReceiveAndCount.Siddhi . It is optional to provide a description via the @App:description() annotation. The definitions of the required streams, windows, tables, triggers and aggregations need to be included before the Siddhi queries. e.g., In the above sample Siddhi file, the streams (lines 14 and 17) are defined before the queries (lines 21-23). Siddhi can infer the definition of the streams. It is not required to define all the streams. However, if annotations need to be added to a stream, that stream must be defined. In the above sample, lines 4-6 nd 8 demonstrate how to nclude comments within Siddhi applications. For more information about Siddhi applications, see Siddhi Application at Siddhi Streaming SQL Guide . Common elements of a Siddhi application This section explains the common types of definitions and queries that are included in Siddhi application: Queries Queries define the logical processing and selections that must be executed for streaming events. They consume from the pre-defined streams/ windows/ tables/ aggregations, process them in a streaming manner, and insert the output to another stream, window or table. For more information about Siddhi queries, see Queries at Siddhi Streaming SQL Guide . Streams Streams are one of the core elements of a stream processing application. A stream is a logical series of events ordered in time with a uniquely identifiable name and set of defined attributes with specific data types defining its schema. In Siddhi, streams are defined by giving it a name and the set of attributes it contains. Lines 14 and 17 of the above sample are examples of defined streams. For more information on Siddhi streams, see Streams at Siddhi Streaming SQL Guide . Tables A table is a collection of events that can be used to store streaming data. The capability to store events in a table allows you to query for stored events later or process them again with a different stream. The generic table concept holds here as well, however, Siddhi tables also support numerous table specific data manipulations such as defining primary keys, indexing, etc. For more information on Siddhi tables, see Storage Integration and Tables at Siddhi Streaming SQL Guide . Windows Windows allow you to retain a collection of streaming events based on a time duration (time window), or a given number of events (length window). It allows you to process events that fall into the defined window or expire from it. For more information on Siddhi windows, see Windows at Siddhi Streaming SQL Guide . Aggregations Aggregation allows you to aggregate streaming events for different time granularities. The time granularities supported are seconds, minutes, hours, days, months and years. Aggregations such as sum, min, avg can be calculated for the desired duration(s) via Siddhi aggregation. For more information on Siddhi aggregations, see Aggregations at Siddhi Streaming SQL Guide . The elements mentioned above work together in a Siddhi application to form an event flow. To understand how the elements os a Siddhi application are interconnected, you can view the design view of a Siddhi application. For more information, see Stream Processor Studio Overview .","title":"Siddhi Application Overview"},{"location":"develop/siddhi-Application-Overview/#siddhi-application-overview","text":"A Siddhi application (.siddhi) file is the deployment artifact containing the Stream Processing logic for WSO2 Stream Processor. The format of a Siddhi application is as follows: @App:name(\"ReceiveAndCount\") @App:description('Receive events via HTTP transport and view the output on the console') /* Sample Siddhi App block comment */ -- Sample Siddhi App line comment @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='json')) define stream SweetProductionStream (name string, amount double); @sink(type='log') define stream TotalCountStream (totalCount long); -- Count the incoming events @info(name='query1') from SweetProductionStream select count() as totalCount insert into TotalCountStream;","title":"Siddhi Application Overview"},{"location":"develop/siddhi-Application-Overview/#basic-information-about-siddhi-applications","text":"Following are some important things to note about Siddhi applications: The file name of each Siddhi application must be equal to the name specified via the @App:name() annotation. e.g., In the sample Siddhi application given above, the application name is ReceiveAndCount . Therefore, the Siddhi file name must be ReceiveAndCount.Siddhi . It is optional to provide a description via the @App:description() annotation. The definitions of the required streams, windows, tables, triggers and aggregations need to be included before the Siddhi queries. e.g., In the above sample Siddhi file, the streams (lines 14 and 17) are defined before the queries (lines 21-23). Siddhi can infer the definition of the streams. It is not required to define all the streams. However, if annotations need to be added to a stream, that stream must be defined. In the above sample, lines 4-6 nd 8 demonstrate how to nclude comments within Siddhi applications. For more information about Siddhi applications, see Siddhi Application at Siddhi Streaming SQL Guide .","title":"Basic information about Siddhi applications"},{"location":"develop/siddhi-Application-Overview/#common-elements-of-a-siddhi-application","text":"This section explains the common types of definitions and queries that are included in Siddhi application:","title":"Common elements of a Siddhi application"},{"location":"develop/siddhi-Application-Overview/#queries","text":"Queries define the logical processing and selections that must be executed for streaming events. They consume from the pre-defined streams/ windows/ tables/ aggregations, process them in a streaming manner, and insert the output to another stream, window or table. For more information about Siddhi queries, see Queries at Siddhi Streaming SQL Guide .","title":"Queries"},{"location":"develop/siddhi-Application-Overview/#streams","text":"Streams are one of the core elements of a stream processing application. A stream is a logical series of events ordered in time with a uniquely identifiable name and set of defined attributes with specific data types defining its schema. In Siddhi, streams are defined by giving it a name and the set of attributes it contains. Lines 14 and 17 of the above sample are examples of defined streams. For more information on Siddhi streams, see Streams at Siddhi Streaming SQL Guide .","title":"Streams"},{"location":"develop/siddhi-Application-Overview/#tables","text":"A table is a collection of events that can be used to store streaming data. The capability to store events in a table allows you to query for stored events later or process them again with a different stream. The generic table concept holds here as well, however, Siddhi tables also support numerous table specific data manipulations such as defining primary keys, indexing, etc. For more information on Siddhi tables, see Storage Integration and Tables at Siddhi Streaming SQL Guide .","title":"Tables"},{"location":"develop/siddhi-Application-Overview/#windows","text":"Windows allow you to retain a collection of streaming events based on a time duration (time window), or a given number of events (length window). It allows you to process events that fall into the defined window or expire from it. For more information on Siddhi windows, see Windows at Siddhi Streaming SQL Guide .","title":"Windows"},{"location":"develop/siddhi-Application-Overview/#aggregations","text":"Aggregation allows you to aggregate streaming events for different time granularities. The time granularities supported are seconds, minutes, hours, days, months and years. Aggregations such as sum, min, avg can be calculated for the desired duration(s) via Siddhi aggregation. For more information on Siddhi aggregations, see Aggregations at Siddhi Streaming SQL Guide . The elements mentioned above work together in a Siddhi application to form an event flow. To understand how the elements os a Siddhi application are interconnected, you can view the design view of a Siddhi application. For more information, see Stream Processor Studio Overview .","title":"Aggregations"},{"location":"develop/simulating-Events/","text":"Simulating Events Simulating events involves simulating predefined event streams. These event stream definitions have stream attributes. You can use event simulator to create events by assigning values to the defined stream attributes and send them as events. This is useful for debugging and monitoring the event receivers and publishers, execution plans and event formatters. Function REST API Saving a simulation configuration Single Event Simulation : POST http:// SP_HOST : API_PORT /simulation/single Multiple Event Simulation: POST http:// SP_HOST : API_PORT /simulation/feed Editing a simulation configuration Single Event Simulation : PUT http:// SP_HOST : API_PORT /simulation/single Multiple Event Simulation: PUT http:// SP_HOST : API_PORT /simulation/feed Deleting a simulation configuration Single Event Simulation : DELETE http:// SP_HOST : API_PORT /simulation/single Multiple Event Simulation: DELETE http:// SP_HOST : API_PORT /simulation/feed Retrieving a simulation configuration Single Event Simulation : GET http:// SP_HOST : API_PORT /simulation/single Multiple Event Simulation: GET http:// SP_HOST : API_PORT /simulation/feed Uploading a CSV file POST http:// SP_HOST : API_PORT /simulation/feed Editing and uploaded CSV file PUT -F 'file=@/{path to csv file}' http:// SP_HOST : API_PORT /simulation/files/{fileName}?fileName={fileName} Deleting an uploaded CSV file DELETE http:// SP_HOST : API_PORT /simulation/files/{fileName} Pausing an event simulation POST http:// SP_HOST : API_PORT / simulation/feed/{simulationName}/?action=pause Resuming an event simulation POST http:// SP_HOST : API_PORT / simulation/feed/{simulationName}/?action=resume Stopping an event simulation DELETE http:// SP_HOST : API_PORT /simulation/feed/{simulationName} The following sections cover how events can be simulated. Saving a simulation configuration Editing a simulation configuration Deleting a simulation configuration Retrieving a simulation configuration Uploading a CSV file Editing an uploaded CSV file Deleting an uploaded CSV file Pausing an event simulation Resuming an event simulation Stopping an event simulation Saving a simulation configuration To simulate events for WSO2 SP, you should first save the event simulator configuration in the SP_HOME /deployment/simulator/simulationConfigs directory by sending a POST request to a REST API as described below. REST API The REST API to be called depends on the type of event simulation you are carrying out as shown in the table below. Event Simulation Type REST API Simulating a single event POST http:// SP_HOST : API_PORT /simulation/single Simulating a multiple events POST http:// SP_HOST : API_PORT /simulation/feed/ Sample cURL command curl -X POST \\ http://localhost:9390/simulation/feed/ \\ -H 'content-type: text/plain' \\ -d '{ \"properties\": { \"simulationName\": \"simulationPrimitive\", \"startTimestamp\": \"\", \"endTimestamp\": \"\", \"noOfEvents\": \"\", \"description\": \"\", \"timeInterval\": \"1000\" }, \"sources\": [ { \"siddhiAppName\": \"TestExecutionPlan\", \"streamName\": \"FooStream\", \"timestampInterval\": \"1000\", \"simulationType\": \"RANDOM_DATA_SIMULATION\", \"attributeConfiguration\": [ { \"type\": \"PRIMITIVE_BASED\", \"primitiveType\": \"STRING\", \"length\": \"5\" }, { \"type\": \"PRIMITIVE_BASED\", \"primitiveType\": \"INT\", \"min\": \"0\", \"max\": \"999\" } ] } ] }' Sample output { status : CREATED , message : Successfully uploaded simulation configuration 'simulationPrimitive' } REST API response 200 if the simulation configuration is successfully saved. 409 if a simulation configuration with the specified name already exists. 400 if the configuration provided is not in a valid JSON format. For descriptions of the HTTP status codes, see HTTP Status Codes . Editing a simulation configuration To edit a simulation configuration that is already saved, a PUT request should be sent to a REST API as explained below. REST API The REST API to be called depends on the type of event simulation you are carrying out as shown in the table below. Event Simulation Type REST API Simulating a single event PUT http:// SP_HOST : API_PORT /simulation/single Simulating a multiple events PUT http:// SP_HOST : API_PORT /simulation/feed/{feed name} Sample cURL command curl -X PUT \\ http://localhost:9390/simulation/feed/simulationPrimitive \\ -H 'content-type: text/plain' \\ -d '{ \"properties\": { \"simulationName\": \"updatedSimulationPrimitive\", \"startTimestamp\": \"\", \"endTimestamp\": \"\", \"noOfEvents\": \"10\", \"description\": \"Updating the simulation configuration\", \"timeInterval\": \"1000\" }, \"sources\": [ { \"siddhiAppName\": \"TestExecutionPlan\", \"streamName\": \"FooStream\", \"timestampInterval\": \"1000\", \"simulationType\": \"RANDOM_DATA_SIMULATION\", \"attributeConfiguration\": [ { \"type\": \"PRIMITIVE_BASED\", \"primitiveType\": \"STRING\", \"length\": \"5\" }, { \"type\": \"PRIMITIVE_BASED\", \"primitiveType\": \"INT\", \"min\": \"0\", \"max\": \"999\" } ] } ] }' Sample output { status : OK , message : Successfully updated simulation configuration 'simulationPrimitive'. } REST API response 200 if the simulation configuration is successfully updated. 404 if the file specified does not exist in the SP_HOME /wso2/editor/deployment/simulation-configs directory. 400 if the file specified is not a CSV file, or if the file does not exist in the path specified. 403 if the size of the file specified exceeds the maximum size allowed. For descriptions of the HTTP status codes, see HTTP Status Codes . Deleting a simulation configuration To delete an event simulation file that is already saved in the SP_HOME /wso2/editor/deployment/simulation-configs directory, a DELETE request should be sent to a REST API as explained below. REST API The REST API to be called depends on the type of event simulation you are carrying out as shown in the table below. Event Simulation Type REST API Simulating a single event DELETE http:// SP_HOST : API_PORT /simulation/single Simulating a multiple events DELETE http:// SP_HOST : API_PORT /simulation/feed/ Sample cURL command curl -X DELETE 'http://localhost:9390/simulation/feed/simulationPrimitive' Sample output { status : OK , message : Successfully deleted simulation configuration 'simulationPrimitive' } REST API response 200 if the simulation configuration is successfully deleted. 404 if the file specified does not exist in the SP_HOME /wso2/editor/deployment/simulation-configs directory. For descriptions of the HTTP status codes, see HTTP Status Codes . Retrieving a simulation configuration To view a simulation configuration saved in the SP_HOME /wso2/editor/deployment/simulation-configs directory via the CLI, a GET request should be sent to a REST API as explained below. REST API The REST API to be called depends on the type of event simulation you are carrying out as shown in the table below. Event Simulation Type REST API Simulating a single event GET http:// SP_HOST : API_PORT /simulation/single Simulating a multiple events GET http:// SP_HOST : API_PORT /simulation/feed/ Sample cURL command curl -X GET 'http://localhost:9390/simulation/feed/simulationPrimitive' Sample output { status : OK , message : {\\ Simulation configuration\\ :{\\ sources\\ :[{\\ timestampInterval\\ :\\ 1000\\ ,\\ simulationType\\ :\\ RANDOM_DATA_SIMULATION\\ ,\\ attributeConfiguration\\ :[{\\ length\\ :\\ 5\\ ,\\ type\\ :\\ PRIMITIVE_BASED\\ ,\\ primitiveType\\ :\\ STRING\\ },{\\ min\\ :\\ 0\\ ,\\ max\\ :\\ 999\\ ,\\ type\\ :\\ PRIMITIVE_BASED\\ ,\\ primitiveType\\ :\\ INT\\ }],\\ streamName\\ :\\ FooStream\\ ,\\ siddhiAppName\\ :\\ TestExecutionPlan\\ }],\\ properties\\ :{\\ simulationName\\ :\\ simulationPrimitive\\ ,\\ description\\ :\\ Updating the simulation configuration\\ ,\\ timeInterval\\ :\\ 1000\\ ,\\ endTimestamp\\ :\\ \\ ,\\ startTimestamp\\ :\\ \\ ,\\ noOfEvents\\ :\\ 10\\ }}} } REST API Response 200 if the simulation configuration is successfully retrieved. 404 if the file specified does not exist in the SP_HOME /wso2/editor/deployment/simulation-configs directory. For descriptions of the HTTP status codes, see HTTP Status Codes . Uploading a CSV file To simulate events from a CSV file, the required CSV file needs to exist in the \\ SP_HOME>/wso2/editor/deployment/csv-files directory. REST API A POST request should be sent to the following API. POST http:// SP_HOST : API_PORT /simulation/feed Sample cURL command curl -X POST \\ http://localhost:9390/simulation/feed/ \\ -H 'content-type: text/plain' \\ -d '{ \"properties\": { \"simulationName\": \"FeedSimulationTest\", \"startTimestamp\": \"\", \"endTimestamp\": \"\", \"noOfEvents\": \"\", \"description\": \"\", \"timeInterval\": \"1000\" }, \"sources\": [ { \"siddhiAppName\": \"TestExecutionPlan\", \"streamName\": \"FooStream\", \"timestampInterval\": \"1000\", \"simulationType\": \"CSV_SIMULATION\", \"fileName\": \"myEvents.csv\", \"delimiter\": \",\", \"isOrdered\": true, \"indices\": \"0,1\" } ] }' Sample output { status : CREATED , message : Successfully uploaded simulation configuration 'FeedSimulationTest' } REST API response 200 if the CSV file is successfully uploaded. 409 if a CSV file with the file name specified already exists in the \\ SP_HOME>/wso2/editor/deployment/csv-files directory. 400 if the specified file is not a CSV file or if the specified file path is not valid. 403 if the size of the file specified exceeds the maximum file size allowed. For descriptions of the HTTP status codes, see HTTP Status Codes . Editing an uploaded CSV file This section explains how to edit a CSV file that is already uploaded to the SP_HOME /wso2/editor/deployment/csv-files directory. REST API A PUT request should be sent to the following API. PUT -F 'file=@/{path to csv file}' http:// SP_HOST : API_PORT /simulation/files/{fileName}?fileName={fileName} Sample cURL command curl -X PUT -F 'file=@/home/nadeeka/Desktop/editedMyEvents.csv' http://localhost:9390/simulation/files/myEvents.csv?fileName=myEvents.csv Sample output { status : OK , message : Successfully updated CSV file 'myEvents.csv' with file ' editedMyEvents.csv'. } REST API response 200 if the CSV file is successfully updated. 404 if the specified CSV file does not exist in the SP_HOME /deployment/simulator/csvFiles directory. For descriptions of the HTTP status codes, see HTTP Status Codes . Deleting an uploaded CSV file This section explains how to delete a CSV file that is already uploaded to the SP_HOME /wso2/editor/deployment/csv-files directory. REST API A DELETE request should be sent to the following API. DELETE http:// SP_HOST : API_PORT /simulation/files/{fileName} Sample cURL command curl -X DELETE http://localhost:9390/simulation/files/myEvents.csv Sample output { status : OK , message : Successfully deleted file 'myEvents.csv' } REST API response 200 if the CSV file is successfully deleted. 404 if the specified CSV file does not exist in the SP_HOME /wso2/editor/deployment/csv-files directory. Pausing an event simulation This section explains how to pause an event simulation that has already started. REST API A POST request should be sent to the following API. POST http:// SP_HOST : API_PORT /simulation/feed/{simulationName}/?action=pause Sample cURL command curl -X POST http://localhost:9390/simulation/feed/simulationPrimitive/?action=pause Sample output { status : OK , message : Successfully paused event simulation 'simulationPrimitive'. } REST API response 200 if the event simulation is successfully paused. 409 if the event simulation is already paused. Resuming an event simulation This section explains how to resume an event simulation that has already paused. REST API A POST request should be sent to the following API POST http:// SP_HOST : API_PORT /simulation/feed/{ simulationName }/?action=resume Sample cURL command curl -X POST http://localhost:9390/simulation/feed/simulationPrimitive/?action=resume Sample output { status : OK , message : Successfully resumed event simulation 'simulationPrimitive'. } REST API response 200 if the event simulation is successfully resumed. Stopping an event simulation This section explains how to stop an event simulation. REST API A POST request should be sent to the following API POST http:// SP_HOST : API_PORT /simulation/feed/{ simulationName }/?action=stop Sample cURL command curl -X POST http://localhost:9390/simulation/feed/simulationPrimitive/?action=stop Sample output { status : OK , message : Successfully stopped event simulation 'simulationPrimitive'. } REST API response 200 if the event simulation is successfully stoped.","title":"Simulating Events"},{"location":"develop/simulating-Events/#simulating-events","text":"Simulating events involves simulating predefined event streams. These event stream definitions have stream attributes. You can use event simulator to create events by assigning values to the defined stream attributes and send them as events. This is useful for debugging and monitoring the event receivers and publishers, execution plans and event formatters. Function REST API Saving a simulation configuration Single Event Simulation : POST http:// SP_HOST : API_PORT /simulation/single Multiple Event Simulation: POST http:// SP_HOST : API_PORT /simulation/feed Editing a simulation configuration Single Event Simulation : PUT http:// SP_HOST : API_PORT /simulation/single Multiple Event Simulation: PUT http:// SP_HOST : API_PORT /simulation/feed Deleting a simulation configuration Single Event Simulation : DELETE http:// SP_HOST : API_PORT /simulation/single Multiple Event Simulation: DELETE http:// SP_HOST : API_PORT /simulation/feed Retrieving a simulation configuration Single Event Simulation : GET http:// SP_HOST : API_PORT /simulation/single Multiple Event Simulation: GET http:// SP_HOST : API_PORT /simulation/feed Uploading a CSV file POST http:// SP_HOST : API_PORT /simulation/feed Editing and uploaded CSV file PUT -F 'file=@/{path to csv file}' http:// SP_HOST : API_PORT /simulation/files/{fileName}?fileName={fileName} Deleting an uploaded CSV file DELETE http:// SP_HOST : API_PORT /simulation/files/{fileName} Pausing an event simulation POST http:// SP_HOST : API_PORT / simulation/feed/{simulationName}/?action=pause Resuming an event simulation POST http:// SP_HOST : API_PORT / simulation/feed/{simulationName}/?action=resume Stopping an event simulation DELETE http:// SP_HOST : API_PORT /simulation/feed/{simulationName} The following sections cover how events can be simulated. Saving a simulation configuration Editing a simulation configuration Deleting a simulation configuration Retrieving a simulation configuration Uploading a CSV file Editing an uploaded CSV file Deleting an uploaded CSV file Pausing an event simulation Resuming an event simulation Stopping an event simulation","title":"Simulating Events"},{"location":"develop/simulating-Events/#saving-a-simulation-configuration","text":"To simulate events for WSO2 SP, you should first save the event simulator configuration in the SP_HOME /deployment/simulator/simulationConfigs directory by sending a POST request to a REST API as described below.","title":"Saving a simulation configuration"},{"location":"develop/simulating-Events/#rest-api","text":"The REST API to be called depends on the type of event simulation you are carrying out as shown in the table below. Event Simulation Type REST API Simulating a single event POST http:// SP_HOST : API_PORT /simulation/single Simulating a multiple events POST http:// SP_HOST : API_PORT /simulation/feed/","title":"REST API"},{"location":"develop/simulating-Events/#sample-curl-command","text":"curl -X POST \\ http://localhost:9390/simulation/feed/ \\ -H 'content-type: text/plain' \\ -d '{ \"properties\": { \"simulationName\": \"simulationPrimitive\", \"startTimestamp\": \"\", \"endTimestamp\": \"\", \"noOfEvents\": \"\", \"description\": \"\", \"timeInterval\": \"1000\" }, \"sources\": [ { \"siddhiAppName\": \"TestExecutionPlan\", \"streamName\": \"FooStream\", \"timestampInterval\": \"1000\", \"simulationType\": \"RANDOM_DATA_SIMULATION\", \"attributeConfiguration\": [ { \"type\": \"PRIMITIVE_BASED\", \"primitiveType\": \"STRING\", \"length\": \"5\" }, { \"type\": \"PRIMITIVE_BASED\", \"primitiveType\": \"INT\", \"min\": \"0\", \"max\": \"999\" } ] } ] }'","title":"Sample cURL command"},{"location":"develop/simulating-Events/#sample-output","text":"{ status : CREATED , message : Successfully uploaded simulation configuration 'simulationPrimitive' }","title":"Sample output"},{"location":"develop/simulating-Events/#rest-api-response","text":"200 if the simulation configuration is successfully saved. 409 if a simulation configuration with the specified name already exists. 400 if the configuration provided is not in a valid JSON format. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"REST API response"},{"location":"develop/simulating-Events/#editing-a-simulation-configuration","text":"To edit a simulation configuration that is already saved, a PUT request should be sent to a REST API as explained below.","title":"Editing a simulation configuration"},{"location":"develop/simulating-Events/#rest-api_1","text":"The REST API to be called depends on the type of event simulation you are carrying out as shown in the table below. Event Simulation Type REST API Simulating a single event PUT http:// SP_HOST : API_PORT /simulation/single Simulating a multiple events PUT http:// SP_HOST : API_PORT /simulation/feed/{feed name}","title":"REST API"},{"location":"develop/simulating-Events/#sample-curl-command_1","text":"curl -X PUT \\ http://localhost:9390/simulation/feed/simulationPrimitive \\ -H 'content-type: text/plain' \\ -d '{ \"properties\": { \"simulationName\": \"updatedSimulationPrimitive\", \"startTimestamp\": \"\", \"endTimestamp\": \"\", \"noOfEvents\": \"10\", \"description\": \"Updating the simulation configuration\", \"timeInterval\": \"1000\" }, \"sources\": [ { \"siddhiAppName\": \"TestExecutionPlan\", \"streamName\": \"FooStream\", \"timestampInterval\": \"1000\", \"simulationType\": \"RANDOM_DATA_SIMULATION\", \"attributeConfiguration\": [ { \"type\": \"PRIMITIVE_BASED\", \"primitiveType\": \"STRING\", \"length\": \"5\" }, { \"type\": \"PRIMITIVE_BASED\", \"primitiveType\": \"INT\", \"min\": \"0\", \"max\": \"999\" } ] } ] }'","title":"Sample cURL command"},{"location":"develop/simulating-Events/#sample-output_1","text":"{ status : OK , message : Successfully updated simulation configuration 'simulationPrimitive'. }","title":"Sample output"},{"location":"develop/simulating-Events/#rest-api-response_1","text":"200 if the simulation configuration is successfully updated. 404 if the file specified does not exist in the SP_HOME /wso2/editor/deployment/simulation-configs directory. 400 if the file specified is not a CSV file, or if the file does not exist in the path specified. 403 if the size of the file specified exceeds the maximum size allowed. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"REST API response"},{"location":"develop/simulating-Events/#deleting-a-simulation-configuration","text":"To delete an event simulation file that is already saved in the SP_HOME /wso2/editor/deployment/simulation-configs directory, a DELETE request should be sent to a REST API as explained below.","title":"Deleting a simulation configuration"},{"location":"develop/simulating-Events/#rest-api_2","text":"The REST API to be called depends on the type of event simulation you are carrying out as shown in the table below. Event Simulation Type REST API Simulating a single event DELETE http:// SP_HOST : API_PORT /simulation/single Simulating a multiple events DELETE http:// SP_HOST : API_PORT /simulation/feed/","title":"REST API"},{"location":"develop/simulating-Events/#sample-curl-command_2","text":"curl -X DELETE 'http://localhost:9390/simulation/feed/simulationPrimitive'","title":"Sample cURL command"},{"location":"develop/simulating-Events/#sample-output_2","text":"{ status : OK , message : Successfully deleted simulation configuration 'simulationPrimitive' }","title":"Sample output"},{"location":"develop/simulating-Events/#rest-api-response_2","text":"200 if the simulation configuration is successfully deleted. 404 if the file specified does not exist in the SP_HOME /wso2/editor/deployment/simulation-configs directory. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"REST API response"},{"location":"develop/simulating-Events/#retrieving-a-simulation-configuration","text":"To view a simulation configuration saved in the SP_HOME /wso2/editor/deployment/simulation-configs directory via the CLI, a GET request should be sent to a REST API as explained below.","title":"Retrieving a simulation configuration"},{"location":"develop/simulating-Events/#rest-api_3","text":"The REST API to be called depends on the type of event simulation you are carrying out as shown in the table below. Event Simulation Type REST API Simulating a single event GET http:// SP_HOST : API_PORT /simulation/single Simulating a multiple events GET http:// SP_HOST : API_PORT /simulation/feed/","title":"REST API"},{"location":"develop/simulating-Events/#sample-curl-command_3","text":"curl -X GET 'http://localhost:9390/simulation/feed/simulationPrimitive'","title":"Sample cURL command"},{"location":"develop/simulating-Events/#sample-output_3","text":"{ status : OK , message : {\\ Simulation configuration\\ :{\\ sources\\ :[{\\ timestampInterval\\ :\\ 1000\\ ,\\ simulationType\\ :\\ RANDOM_DATA_SIMULATION\\ ,\\ attributeConfiguration\\ :[{\\ length\\ :\\ 5\\ ,\\ type\\ :\\ PRIMITIVE_BASED\\ ,\\ primitiveType\\ :\\ STRING\\ },{\\ min\\ :\\ 0\\ ,\\ max\\ :\\ 999\\ ,\\ type\\ :\\ PRIMITIVE_BASED\\ ,\\ primitiveType\\ :\\ INT\\ }],\\ streamName\\ :\\ FooStream\\ ,\\ siddhiAppName\\ :\\ TestExecutionPlan\\ }],\\ properties\\ :{\\ simulationName\\ :\\ simulationPrimitive\\ ,\\ description\\ :\\ Updating the simulation configuration\\ ,\\ timeInterval\\ :\\ 1000\\ ,\\ endTimestamp\\ :\\ \\ ,\\ startTimestamp\\ :\\ \\ ,\\ noOfEvents\\ :\\ 10\\ }}} }","title":"Sample output"},{"location":"develop/simulating-Events/#rest-api-response_3","text":"200 if the simulation configuration is successfully retrieved. 404 if the file specified does not exist in the SP_HOME /wso2/editor/deployment/simulation-configs directory. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"REST API Response"},{"location":"develop/simulating-Events/#uploading-a-csv-file","text":"To simulate events from a CSV file, the required CSV file needs to exist in the \\ SP_HOME>/wso2/editor/deployment/csv-files directory.","title":"Uploading a CSV file"},{"location":"develop/simulating-Events/#rest-api_4","text":"A POST request should be sent to the following API. POST http:// SP_HOST : API_PORT /simulation/feed","title":"REST API"},{"location":"develop/simulating-Events/#sample-curl-command_4","text":"curl -X POST \\ http://localhost:9390/simulation/feed/ \\ -H 'content-type: text/plain' \\ -d '{ \"properties\": { \"simulationName\": \"FeedSimulationTest\", \"startTimestamp\": \"\", \"endTimestamp\": \"\", \"noOfEvents\": \"\", \"description\": \"\", \"timeInterval\": \"1000\" }, \"sources\": [ { \"siddhiAppName\": \"TestExecutionPlan\", \"streamName\": \"FooStream\", \"timestampInterval\": \"1000\", \"simulationType\": \"CSV_SIMULATION\", \"fileName\": \"myEvents.csv\", \"delimiter\": \",\", \"isOrdered\": true, \"indices\": \"0,1\" } ] }'","title":"Sample cURL command"},{"location":"develop/simulating-Events/#sample-output_4","text":"{ status : CREATED , message : Successfully uploaded simulation configuration 'FeedSimulationTest' }","title":"Sample output"},{"location":"develop/simulating-Events/#rest-api-response_4","text":"200 if the CSV file is successfully uploaded. 409 if a CSV file with the file name specified already exists in the \\ SP_HOME>/wso2/editor/deployment/csv-files directory. 400 if the specified file is not a CSV file or if the specified file path is not valid. 403 if the size of the file specified exceeds the maximum file size allowed. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"REST API response"},{"location":"develop/simulating-Events/#editing-an-uploaded-csv-file","text":"This section explains how to edit a CSV file that is already uploaded to the SP_HOME /wso2/editor/deployment/csv-files directory.","title":"Editing an uploaded CSV file"},{"location":"develop/simulating-Events/#rest-api_5","text":"A PUT request should be sent to the following API. PUT -F 'file=@/{path to csv file}' http:// SP_HOST : API_PORT /simulation/files/{fileName}?fileName={fileName}","title":"REST API"},{"location":"develop/simulating-Events/#sample-curl-command_5","text":"curl -X PUT -F 'file=@/home/nadeeka/Desktop/editedMyEvents.csv' http://localhost:9390/simulation/files/myEvents.csv?fileName=myEvents.csv","title":"Sample cURL command"},{"location":"develop/simulating-Events/#sample-output_5","text":"{ status : OK , message : Successfully updated CSV file 'myEvents.csv' with file ' editedMyEvents.csv'. }","title":"Sample output"},{"location":"develop/simulating-Events/#rest-api-response_5","text":"200 if the CSV file is successfully updated. 404 if the specified CSV file does not exist in the SP_HOME /deployment/simulator/csvFiles directory. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"REST API response"},{"location":"develop/simulating-Events/#deleting-an-uploaded-csv-file","text":"This section explains how to delete a CSV file that is already uploaded to the SP_HOME /wso2/editor/deployment/csv-files directory.","title":"Deleting an uploaded CSV file"},{"location":"develop/simulating-Events/#rest-api_6","text":"A DELETE request should be sent to the following API. DELETE http:// SP_HOST : API_PORT /simulation/files/{fileName}","title":"REST API"},{"location":"develop/simulating-Events/#sample-curl-command_6","text":"curl -X DELETE http://localhost:9390/simulation/files/myEvents.csv","title":"Sample cURL command"},{"location":"develop/simulating-Events/#sample-output_6","text":"{ status : OK , message : Successfully deleted file 'myEvents.csv' }","title":"Sample output"},{"location":"develop/simulating-Events/#rest-api-response_6","text":"200 if the CSV file is successfully deleted. 404 if the specified CSV file does not exist in the SP_HOME /wso2/editor/deployment/csv-files directory.","title":"REST API response"},{"location":"develop/simulating-Events/#pausing-an-event-simulation","text":"This section explains how to pause an event simulation that has already started.","title":"Pausing an event simulation"},{"location":"develop/simulating-Events/#rest-api_7","text":"A POST request should be sent to the following API. POST http:// SP_HOST : API_PORT /simulation/feed/{simulationName}/?action=pause","title":"REST API"},{"location":"develop/simulating-Events/#sample-curl-command_7","text":"curl -X POST http://localhost:9390/simulation/feed/simulationPrimitive/?action=pause","title":"Sample cURL command"},{"location":"develop/simulating-Events/#sample-output_7","text":"{ status : OK , message : Successfully paused event simulation 'simulationPrimitive'. }","title":"Sample output"},{"location":"develop/simulating-Events/#rest-api-response_7","text":"200 if the event simulation is successfully paused. 409 if the event simulation is already paused.","title":"REST API response"},{"location":"develop/simulating-Events/#resuming-an-event-simulation","text":"This section explains how to resume an event simulation that has already paused.","title":"Resuming an event simulation"},{"location":"develop/simulating-Events/#rest-api_8","text":"A POST request should be sent to the following API POST http:// SP_HOST : API_PORT /simulation/feed/{ simulationName }/?action=resume","title":"REST API"},{"location":"develop/simulating-Events/#sample-curl-command_8","text":"curl -X POST http://localhost:9390/simulation/feed/simulationPrimitive/?action=resume","title":"Sample cURL command"},{"location":"develop/simulating-Events/#sample-output_8","text":"{ status : OK , message : Successfully resumed event simulation 'simulationPrimitive'. }","title":"Sample output"},{"location":"develop/simulating-Events/#rest-api-response_8","text":"200 if the event simulation is successfully resumed.","title":"REST API response"},{"location":"develop/simulating-Events/#stopping-an-event-simulation","text":"This section explains how to stop an event simulation.","title":"Stopping an event simulation"},{"location":"develop/simulating-Events/#rest-api_9","text":"A POST request should be sent to the following API POST http:// SP_HOST : API_PORT /simulation/feed/{ simulationName }/?action=stop","title":"REST API"},{"location":"develop/simulating-Events/#sample-curl-command_9","text":"curl -X POST http://localhost:9390/simulation/feed/simulationPrimitive/?action=stop","title":"Sample cURL command"},{"location":"develop/simulating-Events/#sample-output_9","text":"{ status : OK , message : Successfully stopped event simulation 'simulationPrimitive'. }","title":"Sample output"},{"location":"develop/simulating-Events/#rest-api-response_9","text":"200 if the event simulation is successfully stoped.","title":"REST API response"},{"location":"develop/stream-Processor-Studio-Overview/","text":"Stream Processor Studio Overview The Stream Processor Studio is a developer tool that is shipped with WSO2 SP to develop Siddhi applications. It allows provides two interfaces to develop Siddhi applications From source view : This allows you to write Siddhi applications in the Siddhi Query Language. This supports auto-completion, tracking syntax errors and debugging. From design view :This interface visualizes the event flow of a Siddhi application, and allows you to compose the applications by dragging and dropping Siddhi components to a graph. Once a Siddhi application is created, you can simulate events via the Stream Processor Studio to test whether it works as expected. You can also run the Siddhi application in the debug mode to detect errors in the Stream Processor logic. Starting Stream Processor Studio (#start-integration-studio) To start and access the Stream Processor Studio, follow the steps below: Start the Stream Processor Studio by issuing one of the following commands from the SP_HOME /bin directory. For Windows: editor.bat For Linux: ./ editor.sh Access the Stream Processor Studio via the http://localhost:/editor URL. The Stream Processor Studio opens as shown below. !!! info The default URL is ` http://localhost:9390/editor ` Welcome Page {width=\"900\"} The Welcome to the Stream Processor Studio Tour Guide is open by deault. You can take a tour by following the instructions on the dialog box, or close it and proceed to explore the Stream Processor Studio on your own. You can also access this dialog box by clicking Tools => Tour Guide . Once you close the dialog box, you can try the following: New Click this to open a new untitled Siddhi file. **Open Click this to open a Siddhi file that is already saved in the workspace folder of the Stream Processor Studio. If the file is already opened in a new tab, clicking **Open does not open it again. The default path to the workspace directory is SP_Home /wso2/editor/deployment . Try out samples The pre-created samples provided out of the box are listed in this section. When you click on a sample, it opens in a new tab without a title. More Samples Click this to view the complete list of samples in the samples directory. This allows you to access samples other than the ones hat are displayed by default is the Try out samples section. When you click on a sample, it opens in a new tab without a title. Quick links This section provides links to more resources. Menu items This section explains the options that are available in the File , Edit and Run menus. File menu Items The File menu includes the following options. {width=\"258\"} New Click this to open a new untitled Siddhi file. **Open File ** Click this to open a Siddhi file that is already saved in the workspace directory of the Stream Processor Studio. If the file is already opened in a new tab, clicking this menu item does not open it again in another tab. The default path to the workspace directory is SP_Home /wso2/editor/deployment . When a Siddhi file is opened, its source view is displayed by default. To view a design view where the elements of the Siddhi application are graphically represented, click Design View . As a result, a graphical view of the Siddhi application is displayed as shown in the following example. {width=\"900\"} - Import Sample Click this to import a sample from the samples diretory to a new tab. The sample opens in an untitled Siddhi file. Once you save it, it can be accessed from the workspace directory. - Save Click this to save an edited or new file to the workspace directory. - Save As Click this if you want to save an existing saved file with a different name. If you click this for an untitled Siddhi file, the normal save operation is executed (i.e., same operation carried out when you click Save ). - Import File Click this to open a file from a system location. This file is opened in a new tab in the saved state with the same file name with which it is imported. - Export File Click this to export a saved file to a system location. This is only applicable to Siddhi application tabs that are in a saved state. Export as Docker !!! tip The exported Docker artifacts use docker-compose for this purpose. Therefore to run the artifacts, you need to install the following in the running environment. - Docker - Docker Compose Click this to export one or more selected Siddhi applications in a Docker container. Once you click on this menu item, the Export as Docker dialog box appears. {width=\"500\"} Select the relevant check boxes to indicate which Siddhi files you need to export. If you select the check box for workspace , all the Siddhi applications in the workspace directory are exported. For detailed information, see Exporting Siddhi Files . **Close File ** Click this to close a currently active Siddhi application that is already open in a tab. **Close All Files ** Click this to close all the Siddhi files that are currently open. **Delete File ** Click this to delete the currently active Siddhi file from the workspace directory. Only Siddhi files that are already saved can be deleted. Settings Click this to change the theme and the font size used in the Stream Processor Studio. The default theme is Twilight . Edit menu Items The Edit menu includes the following options. Undo Click this to undo the last edit made to the Siddhi application that you are currently editing. Only unsaved edits can be undone. Redo Click this to redo the edit that was last undone in the Siddhi application that you are currently editing. The redo operation can be carried out only if you have not saved the Siddhi application after you undid the change. **Find ** Click this to search for a specific string in the currently active Siddhi application tab. **Find and Replace ** Click this to search for a specific string in the currently active Siddhi application tab, and replace it with another string. **Reformat Code ** Click this to reformat the Siddhi queries in the Siddhi application you are currently creating/editing in the source view . !!! info This menu option is only visible when you are working in the [source view](#StreamProcessorStudioOverview-SourceView) . **Auto-Align ** Click this to horizontally align all the Siddhi components in a Siddhi application that you are creating/editing in the design view . !!! info This menu option is only visible when you are working in the [design view](#StreamProcessorStudioOverview-DesignView) . Run menu Items The Run menu includes the following options. Run Click this to start the Siddhi application in the Run mode. Only saved Siddhi applications can be run. !!! info This menu option is enabled only when a Siddhi application is being created/edited in the [source view](#StreamProcessorStudioOverview-SourceView) . Debug Click this to start the Siddhi application in the Debug mode. Only saved Siddhi applications can be run in this mode. !!! info This menu option is enabled only when a Siddhi application is being created/edited in the [source view](#StreamProcessorStudioOverview-SourceView) . Stop Click this to stop a Siddhi application that is already started in either the Run or Debug mode. Tools menu items The Tools menu provides access to the following tools that are shipped with the Stream Processor Studio. {width=\"301\"} **File Explorer ** The file explorer. This is also avaible in the Side Panel . ****Event Simulator **** Simulation can be carried out in two ways: Single Simulation Feed Simulation For detailed information about event simulation, see Simulating Events . The event simulator can also be accessed from the Side Panel . Console This is an output console that provides feedback on various user activities carried out on the Stream Processor Studio. It is accesible from the Side Panel . **Sample Event Generator ** This opens the Sample Event Generator as follows. {width=\"865\"} Here, you can generate sample events for a selected stream within a selected Siddhi application in a specified format. Siddhi Store Query ** This opens the **Siddhi Store Query dialog box. {height=\"250\"} Here, you can select a Siddhi application, and then enter a query to manipulate the store in which that Siddhi Application saves data. You can enter queries that can update record, insert/update records, retrieve records and delete records. For more information about actions you can carry out for stores, see Managing Stored Data via REST APIs . Tour Guide ** This opens a dialog box named **Welcome to the Stream Processor Studio Tour Guide which guides you to understand Stream Processor Studio. When you start the Stream Processor Studio and access it, this dialog box is open by default. Deploy menu items The Deploy menu has the following option to select one or more Siddhi applications and deploy them to one or more WSO2 SP servers. For more information, see Deploying Streaming Applications . {width=\"301\"} Side Panel File Explorer This provides a view of all the files saved as shown in the example above. Event Simulator {width=\"400\"} Simulation can be carried out in two ways: Single Simulation Feed Simulation For detailed information about event simulation, see Simulating Events . Output Console {width=\"1000\"} This provides feedback on various user activities carried out on the Stream Processor Studio. Operator Finder {height=\"250\"} Click the Operator Finder icon to search for the Siddhi extensions that you want to use in your Siddhi applications. For the complete list of Siddhi extensions that you can search for via the Operator Finder, see Siddhi Extensions . For detailed instructions to find and use a Siddhi extension via the Operator Finder demonstrated with an example, see Creating a Siddhi Application . Toolbar Run icon Click this to start a currently open Siddhi application in the Run mode. This icon is enabled only for saved Siddhi applications. Debug icon Click this to start a currently open Siddhi application in the Debug mode. This icon is enabled only for saved Siddhi applications. Stop icon Click this to stop a Siddhi application that is currently running in either the Run or Debug mode. Revert icon Click this to revert the unsaved changes in the Siddhi application that is currently being created/edited.","title":"Stream Processor Studio Overview"},{"location":"develop/stream-Processor-Studio-Overview/#stream-processor-studio-overview","text":"The Stream Processor Studio is a developer tool that is shipped with WSO2 SP to develop Siddhi applications. It allows provides two interfaces to develop Siddhi applications From source view : This allows you to write Siddhi applications in the Siddhi Query Language. This supports auto-completion, tracking syntax errors and debugging. From design view :This interface visualizes the event flow of a Siddhi application, and allows you to compose the applications by dragging and dropping Siddhi components to a graph. Once a Siddhi application is created, you can simulate events via the Stream Processor Studio to test whether it works as expected. You can also run the Siddhi application in the debug mode to detect errors in the Stream Processor logic.","title":"Stream Processor Studio Overview"},{"location":"develop/stream-Processor-Studio-Overview/#starting-stream-processor-studio","text":"(#start-integration-studio) To start and access the Stream Processor Studio, follow the steps below: Start the Stream Processor Studio by issuing one of the following commands from the SP_HOME /bin directory. For Windows: editor.bat For Linux: ./ editor.sh Access the Stream Processor Studio via the http://localhost:/editor URL. The Stream Processor Studio opens as shown below. !!! info The default URL is ` http://localhost:9390/editor `","title":"Starting Stream Processor Studio"},{"location":"develop/stream-Processor-Studio-Overview/#welcome-page","text":"{width=\"900\"} The Welcome to the Stream Processor Studio Tour Guide is open by deault. You can take a tour by following the instructions on the dialog box, or close it and proceed to explore the Stream Processor Studio on your own. You can also access this dialog box by clicking Tools => Tour Guide . Once you close the dialog box, you can try the following: New Click this to open a new untitled Siddhi file. **Open Click this to open a Siddhi file that is already saved in the workspace folder of the Stream Processor Studio. If the file is already opened in a new tab, clicking **Open does not open it again. The default path to the workspace directory is SP_Home /wso2/editor/deployment . Try out samples The pre-created samples provided out of the box are listed in this section. When you click on a sample, it opens in a new tab without a title. More Samples Click this to view the complete list of samples in the samples directory. This allows you to access samples other than the ones hat are displayed by default is the Try out samples section. When you click on a sample, it opens in a new tab without a title. Quick links This section provides links to more resources.","title":"Welcome Page"},{"location":"develop/stream-Processor-Studio-Overview/#menu-items","text":"This section explains the options that are available in the File , Edit and Run menus.","title":"Menu items"},{"location":"develop/stream-Processor-Studio-Overview/#file-menu-items","text":"The File menu includes the following options. {width=\"258\"} New Click this to open a new untitled Siddhi file. **Open File ** Click this to open a Siddhi file that is already saved in the workspace directory of the Stream Processor Studio. If the file is already opened in a new tab, clicking this menu item does not open it again in another tab. The default path to the workspace directory is SP_Home /wso2/editor/deployment . When a Siddhi file is opened, its source view is displayed by default. To view a design view where the elements of the Siddhi application are graphically represented, click Design View . As a result, a graphical view of the Siddhi application is displayed as shown in the following example. {width=\"900\"} - Import Sample Click this to import a sample from the samples diretory to a new tab. The sample opens in an untitled Siddhi file. Once you save it, it can be accessed from the workspace directory. - Save Click this to save an edited or new file to the workspace directory. - Save As Click this if you want to save an existing saved file with a different name. If you click this for an untitled Siddhi file, the normal save operation is executed (i.e., same operation carried out when you click Save ). - Import File Click this to open a file from a system location. This file is opened in a new tab in the saved state with the same file name with which it is imported. - Export File Click this to export a saved file to a system location. This is only applicable to Siddhi application tabs that are in a saved state. Export as Docker !!! tip The exported Docker artifacts use docker-compose for this purpose. Therefore to run the artifacts, you need to install the following in the running environment. - Docker - Docker Compose Click this to export one or more selected Siddhi applications in a Docker container. Once you click on this menu item, the Export as Docker dialog box appears. {width=\"500\"} Select the relevant check boxes to indicate which Siddhi files you need to export. If you select the check box for workspace , all the Siddhi applications in the workspace directory are exported. For detailed information, see Exporting Siddhi Files . **Close File ** Click this to close a currently active Siddhi application that is already open in a tab. **Close All Files ** Click this to close all the Siddhi files that are currently open. **Delete File ** Click this to delete the currently active Siddhi file from the workspace directory. Only Siddhi files that are already saved can be deleted. Settings Click this to change the theme and the font size used in the Stream Processor Studio. The default theme is Twilight .","title":"File menu Items"},{"location":"develop/stream-Processor-Studio-Overview/#edit-menu-items","text":"The Edit menu includes the following options. Undo Click this to undo the last edit made to the Siddhi application that you are currently editing. Only unsaved edits can be undone. Redo Click this to redo the edit that was last undone in the Siddhi application that you are currently editing. The redo operation can be carried out only if you have not saved the Siddhi application after you undid the change. **Find ** Click this to search for a specific string in the currently active Siddhi application tab. **Find and Replace ** Click this to search for a specific string in the currently active Siddhi application tab, and replace it with another string. **Reformat Code ** Click this to reformat the Siddhi queries in the Siddhi application you are currently creating/editing in the source view . !!! info This menu option is only visible when you are working in the [source view](#StreamProcessorStudioOverview-SourceView) . **Auto-Align ** Click this to horizontally align all the Siddhi components in a Siddhi application that you are creating/editing in the design view . !!! info This menu option is only visible when you are working in the [design view](#StreamProcessorStudioOverview-DesignView) .","title":"Edit menu Items"},{"location":"develop/stream-Processor-Studio-Overview/#run-menu-items","text":"The Run menu includes the following options. Run Click this to start the Siddhi application in the Run mode. Only saved Siddhi applications can be run. !!! info This menu option is enabled only when a Siddhi application is being created/edited in the [source view](#StreamProcessorStudioOverview-SourceView) . Debug Click this to start the Siddhi application in the Debug mode. Only saved Siddhi applications can be run in this mode. !!! info This menu option is enabled only when a Siddhi application is being created/edited in the [source view](#StreamProcessorStudioOverview-SourceView) . Stop Click this to stop a Siddhi application that is already started in either the Run or Debug mode.","title":"Run menu Items"},{"location":"develop/stream-Processor-Studio-Overview/#tools-menu-items","text":"The Tools menu provides access to the following tools that are shipped with the Stream Processor Studio. {width=\"301\"} **File Explorer ** The file explorer. This is also avaible in the Side Panel . ****Event Simulator **** Simulation can be carried out in two ways: Single Simulation Feed Simulation For detailed information about event simulation, see Simulating Events . The event simulator can also be accessed from the Side Panel . Console This is an output console that provides feedback on various user activities carried out on the Stream Processor Studio. It is accesible from the Side Panel . **Sample Event Generator ** This opens the Sample Event Generator as follows. {width=\"865\"} Here, you can generate sample events for a selected stream within a selected Siddhi application in a specified format. Siddhi Store Query ** This opens the **Siddhi Store Query dialog box. {height=\"250\"} Here, you can select a Siddhi application, and then enter a query to manipulate the store in which that Siddhi Application saves data. You can enter queries that can update record, insert/update records, retrieve records and delete records. For more information about actions you can carry out for stores, see Managing Stored Data via REST APIs . Tour Guide ** This opens a dialog box named **Welcome to the Stream Processor Studio Tour Guide which guides you to understand Stream Processor Studio. When you start the Stream Processor Studio and access it, this dialog box is open by default.","title":"Tools menu items"},{"location":"develop/stream-Processor-Studio-Overview/#deploy-menu-items","text":"The Deploy menu has the following option to select one or more Siddhi applications and deploy them to one or more WSO2 SP servers. For more information, see Deploying Streaming Applications . {width=\"301\"}","title":"Deploy menu items"},{"location":"develop/stream-Processor-Studio-Overview/#side-panel","text":"File Explorer This provides a view of all the files saved as shown in the example above.","title":"Side Panel"},{"location":"develop/stream-Processor-Studio-Overview/#event-simulator","text":"{width=\"400\"} Simulation can be carried out in two ways: Single Simulation Feed Simulation For detailed information about event simulation, see Simulating Events .","title":"Event Simulator"},{"location":"develop/stream-Processor-Studio-Overview/#output-console","text":"{width=\"1000\"} This provides feedback on various user activities carried out on the Stream Processor Studio.","title":"Output Console"},{"location":"develop/stream-Processor-Studio-Overview/#operator-finder","text":"{height=\"250\"} Click the Operator Finder icon to search for the Siddhi extensions that you want to use in your Siddhi applications. For the complete list of Siddhi extensions that you can search for via the Operator Finder, see Siddhi Extensions . For detailed instructions to find and use a Siddhi extension via the Operator Finder demonstrated with an example, see Creating a Siddhi Application .","title":"Operator Finder"},{"location":"develop/stream-Processor-Studio-Overview/#toolbar","text":"Run icon Click this to start a currently open Siddhi application in the Run mode. This icon is enabled only for saved Siddhi applications. Debug icon Click this to start a currently open Siddhi application in the Debug mode. This icon is enabled only for saved Siddhi applications. Stop icon Click this to stop a Siddhi application that is currently running in either the Run or Debug mode. Revert icon Click this to revert the unsaved changes in the Siddhi application that is currently being created/edited.","title":"Toolbar"},{"location":"develop/streaming-integration-studio-overview/","text":"Streaming Integration Studio Overview","title":"Streaming integration studio overview"},{"location":"develop/streaming-integration-studio-overview/#streaming-integration-studio-overview","text":"","title":"Streaming Integration Studio Overview"},{"location":"develop/streaming-integrator-studio-overview/","text":"Streaming Integrator Tooling Overview The Streaming Integrator Tooling is a developer tool that is shipped with the Streaming Integrator to develop Siddhi applications. It allows provides two interfaces to develop Siddhi applications Source View : This allows you to write Siddhi applications in the Siddhi Query Language. This supports auto-completion, tracking syntax errors and debugging. Design View :This interface visualizes the event flow of a Siddhi application, and allows you to compose the applications by dragging and dropping Siddhi components to a graph. Once a Siddhi application is created, you can simulate events via the Streaming Integrator Tooling to test whether it works as expected. You can also run the Siddhi application in the debug mode to detect errors in the Siddhi logic. Starting Streaming Integrator Tooling To start and access the Streaming Integrator Tooling, follow the steps below: Start the Streaming Integrator Tooling by issuing one of the following commands from the SI_HOME /bin directory. For Windows: streaming-integrator-tooling.bat For Linux: ./streaming-integrator-tooling.sh Access the Streaming Integrator Tooling via the http://localhost:/editor URL. The Streaming Integrator Tooling opens as shown below. Info The default URL is http://localhost:9390/editor . Welcome Page The Welcome to the Streaming Integrator Tooling Tour Guide is open by default. You can take a tour by following the instructions on the dialog box, or close it and proceed to explore the Streaming Integrator Tooling on your own. You can also access this dialog box by clicking Tools => Tour Guide . Once you close the dialog box, you can try the following: New Click this to open a new untitled Siddhi file. Open Click this to open a Siddhi file that is already saved in the workspace directory of the Streaming Integrator Tooling. If the file is already opened in a new tab, clicking Open does not open it again. The default path to the workspace directory is SI_Home /wso2/server/deployment . Try out samples The pre-created samples provided out of the box are listed in this section. When you click on a sample, it opens in a new tab without a title. More Samples Click this to view the complete list of samples in the samples directory. This allows you to access samples other than the ones that are displayed by default is the Try out samples section. When you click on a sample, it opens in a new tab without a title. Quick links This section provides links to more resources. Menu items This section explains the options that are available in the File , Edit and Run menus. File menu Items The File menu includes the following options. New Click this to open a new untitled Siddhi file. Open File Click this to open a Siddhi file that is already saved in the workspace directory of the Streaming Integrator Tooling. If the file is already opened in a new tab, clicking Open does not open it again. The default path to the workspace directory is SI_Home /wso2/server/deployment . ` . When a Siddhi file is opened, its source view is displayed by default. To view a design view where the elements of the Siddhi application are graphically represented, click Design View . As a result, a graphical view of the Siddhi application is displayed as shown in the following example. Import Sample Click this to import a sample from the samples diretory to a new tab. The sample opens in an untitled Siddhi file. Once you save it, it can be accessed from the workspace directory. Save Click this to save an edited or new file to the workspace directory. Save As Click this if you want to save an existing saved file with a different name. If you click this for an untitled Siddhi file, the normal save operation is executed (i.e., same operation carried out when you click Save ). Import File Click this to open a file from a system location. This file is opened in a new tab in the saved state with the same file name with which it is imported. Export File Click this to export a saved file to a system location. This is only applicable to Siddhi application tabs that are in a saved state. Export as Docker Tip The exported Docker artifacts use docker-compose for this purpose. Therefore to run the artifacts, you need to install the following in the running environment. + Docker + Docker Compose Click this to export one or more selected Siddhi applications in a Docker container. Once you click on this menu item, the Export as Docker dialog box appears. Select the relevant check boxes to indicate which Siddhi files you need to export. If you select the check box for workspace , all the Siddhi applications in the workspace directory are exported. For detailed information, see Exporting Siddhi Files . Close File Click this to close a currently active Siddhi application that is already open in a tab. Close All Files Click this to close all the Siddhi files that are currently open. Delete File Click this to delete the currently active Siddhi file from the workspace directory. Only Siddhi files that are already saved can be deleted. Settings Click this to change the theme and the font size used in the Streaming Integrator Tooling. The default theme is Twilight . Edit menu Items The Edit menu includes the following options. Undo Click this to undo the last edit made to the Siddhi application that you are currently editing. Only unsaved edits can be undone. Redo Click this to redo the edit that was last undone in the Siddhi application that you are currently editing. The redo operation can be carried out only if you have not saved the Siddhi application after you undid the change. Find Click this to search for a specific string in the currently active Siddhi application tab. Find and Replace Click this to search for a specific string in the currently active Siddhi application tab, and replace it with another string. Reformat Code Click this to reformat the Siddhi queries in the Siddhi application you are currently creating/editing in the source view . Info This menu option is only visible when you are working in the source view . Auto-Align Click this to horizontally align all the Siddhi components in a Siddhi application that you are creating/editing in the design view . Info This menu option is only visible when you are working in the design view . Run menu Items The Run menu includes the following options. Run Click this to start the Siddhi application in the Run mode. Only saved Siddhi applications can be run. Info This menu option is enabled only when a Siddhi application is being created/edited in the source view . Debug Click this to start the Siddhi application in the Debug mode. Only saved Siddhi applications can be run in this mode. Info This menu option is enabled only when a Siddhi application is being created/edited in the source view . Stop Click this to stop a Siddhi application that is already started in either the Run or Debug mode. Tools menu items The Tools menu provides access to the following tools that are shipped with the Streaming Integrator Tooling. File Explorer The file explorer. This is also avaible in the Side Panel . Event Simulator Simulation can be carried out in two ways: Single Simulation Feed Simulation For detailed information about event simulation, see Simulating Events . The event simulator can also be accessed from the Side Panel . Console This is an output console that provides feedback on various user activities carried out on the Streaming Integration Tooling. It is accesible from the Side Panel . Sample Event Generator This opens the Sample Event Generator as follows. Here, you can generate sample events for a selected stream within a selected Siddhi application in a specified format. Siddhi Store Query This opens the Siddhi Store Query dialog box. Here, you can select a Siddhi application, and then enter a query to manipulate the store in which that Siddhi Application saves data. You can enter queries that can update record, insert/update records, retrieve records and delete records. For more information about actions you can carry out for stores, see Managing Stored Data via REST APIs . Tour Guide This opens a dialog box named Welcome to the Streaming Integrator Tooling Tour Guide which guides you to understand Streaming Integrator Tooling. When you start the Streaming Integrator Tooling and access it, this dialog box is open by default. Deploy menu items The Deploy menu has the following option to select one or more Siddhi applications and deploy them to one or more Streaming Integrator servers. For more information, see Deploying Streaming Applications .. Side Panel File Explorer This provides a view of all the files saved as shown in the example above. Event Simulator Simulation can be carried out in two ways: Single Simulation Feed Simulation For detailed information about event simulation, see Simulating Events . Output Console This provides feedback on various user activities carried out on the Streaming Integrator. Operator Finder Click the Operator Finder icon to search for the Siddhi extensions that you want to use in your Siddhi applications. For the complete list of Siddhi extensions that you can search for via the Operator Finder, see Siddhi Extensions . For detailed instructions to find and use a Siddhi extension via the Operator Finder demonstrated with an example, see Creating a Siddhi Application . Toolbar Run icon Click this to start a currently open Siddhi application in the Run mode. This icon is enabled only for saved Siddhi applications. Debug icon Click this to start a currently open Siddhi application in the Debug mode. This icon is enabled only for saved Siddhi applications. Stop icon Click this to stop a Siddhi application that is currently running in either the Run or Debug mode. Revert icon Click this to revert the unsaved changes in the Siddhi application that is currently being created/edited.","title":"Streaming Integrator Tooling Overview"},{"location":"develop/streaming-integrator-studio-overview/#streaming-integrator-tooling-overview","text":"The Streaming Integrator Tooling is a developer tool that is shipped with the Streaming Integrator to develop Siddhi applications. It allows provides two interfaces to develop Siddhi applications Source View : This allows you to write Siddhi applications in the Siddhi Query Language. This supports auto-completion, tracking syntax errors and debugging. Design View :This interface visualizes the event flow of a Siddhi application, and allows you to compose the applications by dragging and dropping Siddhi components to a graph. Once a Siddhi application is created, you can simulate events via the Streaming Integrator Tooling to test whether it works as expected. You can also run the Siddhi application in the debug mode to detect errors in the Siddhi logic.","title":"Streaming Integrator Tooling Overview"},{"location":"develop/streaming-integrator-studio-overview/#starting-streaming-integrator-tooling","text":"To start and access the Streaming Integrator Tooling, follow the steps below: Start the Streaming Integrator Tooling by issuing one of the following commands from the SI_HOME /bin directory. For Windows: streaming-integrator-tooling.bat For Linux: ./streaming-integrator-tooling.sh Access the Streaming Integrator Tooling via the http://localhost:/editor URL. The Streaming Integrator Tooling opens as shown below. Info The default URL is http://localhost:9390/editor .","title":"Starting Streaming Integrator Tooling"},{"location":"develop/streaming-integrator-studio-overview/#welcome-page","text":"The Welcome to the Streaming Integrator Tooling Tour Guide is open by default. You can take a tour by following the instructions on the dialog box, or close it and proceed to explore the Streaming Integrator Tooling on your own. You can also access this dialog box by clicking Tools => Tour Guide . Once you close the dialog box, you can try the following: New Click this to open a new untitled Siddhi file. Open Click this to open a Siddhi file that is already saved in the workspace directory of the Streaming Integrator Tooling. If the file is already opened in a new tab, clicking Open does not open it again. The default path to the workspace directory is SI_Home /wso2/server/deployment . Try out samples The pre-created samples provided out of the box are listed in this section. When you click on a sample, it opens in a new tab without a title. More Samples Click this to view the complete list of samples in the samples directory. This allows you to access samples other than the ones that are displayed by default is the Try out samples section. When you click on a sample, it opens in a new tab without a title. Quick links This section provides links to more resources.","title":"Welcome Page"},{"location":"develop/streaming-integrator-studio-overview/#menu-items","text":"This section explains the options that are available in the File , Edit and Run menus.","title":"Menu items"},{"location":"develop/streaming-integrator-studio-overview/#file-menu-items","text":"The File menu includes the following options. New Click this to open a new untitled Siddhi file. Open File Click this to open a Siddhi file that is already saved in the workspace directory of the Streaming Integrator Tooling. If the file is already opened in a new tab, clicking Open does not open it again. The default path to the workspace directory is SI_Home /wso2/server/deployment . ` . When a Siddhi file is opened, its source view is displayed by default. To view a design view where the elements of the Siddhi application are graphically represented, click Design View . As a result, a graphical view of the Siddhi application is displayed as shown in the following example. Import Sample Click this to import a sample from the samples diretory to a new tab. The sample opens in an untitled Siddhi file. Once you save it, it can be accessed from the workspace directory. Save Click this to save an edited or new file to the workspace directory. Save As Click this if you want to save an existing saved file with a different name. If you click this for an untitled Siddhi file, the normal save operation is executed (i.e., same operation carried out when you click Save ). Import File Click this to open a file from a system location. This file is opened in a new tab in the saved state with the same file name with which it is imported. Export File Click this to export a saved file to a system location. This is only applicable to Siddhi application tabs that are in a saved state. Export as Docker Tip The exported Docker artifacts use docker-compose for this purpose. Therefore to run the artifacts, you need to install the following in the running environment. + Docker + Docker Compose Click this to export one or more selected Siddhi applications in a Docker container. Once you click on this menu item, the Export as Docker dialog box appears. Select the relevant check boxes to indicate which Siddhi files you need to export. If you select the check box for workspace , all the Siddhi applications in the workspace directory are exported. For detailed information, see Exporting Siddhi Files . Close File Click this to close a currently active Siddhi application that is already open in a tab. Close All Files Click this to close all the Siddhi files that are currently open. Delete File Click this to delete the currently active Siddhi file from the workspace directory. Only Siddhi files that are already saved can be deleted. Settings Click this to change the theme and the font size used in the Streaming Integrator Tooling. The default theme is Twilight .","title":"File menu Items"},{"location":"develop/streaming-integrator-studio-overview/#edit-menu-items","text":"The Edit menu includes the following options. Undo Click this to undo the last edit made to the Siddhi application that you are currently editing. Only unsaved edits can be undone. Redo Click this to redo the edit that was last undone in the Siddhi application that you are currently editing. The redo operation can be carried out only if you have not saved the Siddhi application after you undid the change. Find Click this to search for a specific string in the currently active Siddhi application tab. Find and Replace Click this to search for a specific string in the currently active Siddhi application tab, and replace it with another string. Reformat Code Click this to reformat the Siddhi queries in the Siddhi application you are currently creating/editing in the source view . Info This menu option is only visible when you are working in the source view . Auto-Align Click this to horizontally align all the Siddhi components in a Siddhi application that you are creating/editing in the design view . Info This menu option is only visible when you are working in the design view .","title":"Edit menu Items"},{"location":"develop/streaming-integrator-studio-overview/#run-menu-items","text":"The Run menu includes the following options. Run Click this to start the Siddhi application in the Run mode. Only saved Siddhi applications can be run. Info This menu option is enabled only when a Siddhi application is being created/edited in the source view . Debug Click this to start the Siddhi application in the Debug mode. Only saved Siddhi applications can be run in this mode. Info This menu option is enabled only when a Siddhi application is being created/edited in the source view . Stop Click this to stop a Siddhi application that is already started in either the Run or Debug mode.","title":"Run menu Items"},{"location":"develop/streaming-integrator-studio-overview/#tools-menu-items","text":"The Tools menu provides access to the following tools that are shipped with the Streaming Integrator Tooling. File Explorer The file explorer. This is also avaible in the Side Panel . Event Simulator Simulation can be carried out in two ways: Single Simulation Feed Simulation For detailed information about event simulation, see Simulating Events . The event simulator can also be accessed from the Side Panel . Console This is an output console that provides feedback on various user activities carried out on the Streaming Integration Tooling. It is accesible from the Side Panel . Sample Event Generator This opens the Sample Event Generator as follows. Here, you can generate sample events for a selected stream within a selected Siddhi application in a specified format. Siddhi Store Query This opens the Siddhi Store Query dialog box. Here, you can select a Siddhi application, and then enter a query to manipulate the store in which that Siddhi Application saves data. You can enter queries that can update record, insert/update records, retrieve records and delete records. For more information about actions you can carry out for stores, see Managing Stored Data via REST APIs . Tour Guide This opens a dialog box named Welcome to the Streaming Integrator Tooling Tour Guide which guides you to understand Streaming Integrator Tooling. When you start the Streaming Integrator Tooling and access it, this dialog box is open by default.","title":"Tools menu items"},{"location":"develop/streaming-integrator-studio-overview/#deploy-menu-items","text":"The Deploy menu has the following option to select one or more Siddhi applications and deploy them to one or more Streaming Integrator servers. For more information, see Deploying Streaming Applications ..","title":"Deploy menu items"},{"location":"develop/streaming-integrator-studio-overview/#side-panel","text":"File Explorer This provides a view of all the files saved as shown in the example above.","title":"Side Panel"},{"location":"develop/streaming-integrator-studio-overview/#event-simulator","text":"Simulation can be carried out in two ways: Single Simulation Feed Simulation For detailed information about event simulation, see Simulating Events .","title":"Event Simulator"},{"location":"develop/streaming-integrator-studio-overview/#output-console","text":"This provides feedback on various user activities carried out on the Streaming Integrator.","title":"Output Console"},{"location":"develop/streaming-integrator-studio-overview/#operator-finder","text":"Click the Operator Finder icon to search for the Siddhi extensions that you want to use in your Siddhi applications. For the complete list of Siddhi extensions that you can search for via the Operator Finder, see Siddhi Extensions . For detailed instructions to find and use a Siddhi extension via the Operator Finder demonstrated with an example, see Creating a Siddhi Application .","title":"Operator Finder"},{"location":"develop/streaming-integrator-studio-overview/#toolbar","text":"Run icon Click this to start a currently open Siddhi application in the Run mode. This icon is enabled only for saved Siddhi applications. Debug icon Click this to start a currently open Siddhi application in the Debug mode. This icon is enabled only for saved Siddhi applications. Stop icon Click this to stop a Siddhi application that is currently running in either the Run or Debug mode. Revert icon Click this to revert the unsaved changes in the Siddhi application that is currently being created/edited.","title":"Toolbar"},{"location":"develop/testing-a-Siddhi-Application/","text":"Testing Siddhi Applications The Streaming Integrator allows the following tasks to be carried out to ensure that the Siddhi applications you create and deploy are validated before they are run in an actual production environment. Validate Siddhi applications that are written in the Streaming Integrator Studio. Run Siddhi applications that were written in the Streaming Integrator Studio in either Run or Debug mode. Simulate events to test the Siddhi applications and analyze events that are received and sent. This allows you to analyze the status of each query within a Siddhi application at different execution points. Validating a Siddhi application To validate a Siddhi application, follow the procedure below: Start and access the Streaming Integrator Studio. For detailed instructions, see Starting Stream Integration Studio . In this example, let's use an existing sample as an example. Click on the ReceiveAndCount sample to open it. Sample opens in a new tab. This sample does not have errors, and therefore, no errors are displayed in the editor. To create an error for demonstration purposes, change the count() function in the query1 query to countNew() as shown below. @info(name='query1') from SweetProductionStream select countNew() as totalCount insert into TotalCountStream; ` Now, the editor indicates that there is a syntax error. If you move the cursor over the error icon, it indicates that countNew is an invalid function name as shown below. Running or debugging a Siddhi application You can run or debug a Siddhi application to verify whether the logic you have written is correct. To start a Siddhi application in the run/debug mode, follow the procedure below: Start and access the Streaming Integrator Studio. For detailed instructions, see Starting Stream Integration Studio . For this example, click the existing sample ReceiveAndCount . It opens in a new untitled tab. Save the Siddhi file so that you can run it in the Run or Debug mode. To save it, click File => Save . Once the file is saved, you can see the Run and Debug menu options enabled as shown below. To start the application in Run mode, click Run => Run . This logs the following output in the console. Start the application in the Debug mode, click Run => Debug . As a result, the following mesage is logged in the console. You can also note that another console tab is opened with debug options. To create an error for demonstration purposes, change the count() function in the query1 query to countNew() , and save. Then click Run => Run . As a result, the following output is logged in the console. Simulating events This section demonstrates how to test a Siddhi application via event simulation. Event simulation involves simulating predefined event streams. These event stream definitions have stream attributes. You can use event simulator to create events by assigning values to the defined stream attributes and send them as events. This is useful for testing Siddhi applications in order to evaluate whether they function as expected Events can be simulated in the following methods: Simulating a single event Simulating multiple events via CSV files Simulating multiple events via databases Generating random events Tip Before you simulate events for a Siddhi application, you need to run or debug it. Therefore, before you try this section, see Running or debugging a Siddhi application . Simulating a single event This section demonstrates how to simulate a single event to be processed via the Streaming Integrator. Tip Before simulating events, a Siddhi application should be deployed. To simulate a single event, follow the steps given below. Access the Streaming Integrator Studio via the http://localhost:/editor URL. The Streaming Integrator Studio opens as shown below. Info The default URL is http://localhost:9090/editor . Click the Event Simulator icon in the left pane of the editor to open the Single Simulation panel. It opens the left panel for event simulation as follows. Enter Information in the Single Simulation panel as described below. In the Siddhi App Name field, select a currently deployed Siddhi application. In the Stream Name field, select the event stream for which you want to simulate events. The list displays all the event streams defined in the selected Siddhi application. If you want to simulate the event for a specific time different to the current time, enter a valid timestamp in the Timestamp field. To select a timestamp, click the time and calendar icon next to the Timestamp field. Then select the required date, hour, minute, second and millisecond. Click Done to select the time stamp entered. If you want to select the current time, you can click Now . Enter values for the attributes of the selected stream. Click Send to start sending the event. The simulated event is logged similar to the sample log given below. Simulating multiple events via CSV files This section explains how to generate multiple events via CSV files to be analyzed via the Streaming Integrator. Tip Before simulating events, a Siddhi application should be deployed. To simulate multiple events from a CSV file, follow the steps given below. Access the Streaming Integrator Studio via the http://localhost:/editor URL. The Streaming Integrator Studio opens as shown below. Info The default URL is http://localhost:9090/editor . Click the Event Simulator icon in the left pane of the editor. In the event simulation left panel that opens, click on the Feed Simulation tab. To create a new simulation, click Create . This opens the following panel. Enter values for the displayed fields as follows. In the Simulation Name field, enter a name for the event simulation. In the Description field, enter a description for the event simulation. If you want to receive events only during a specific time interval, enter that time interval in the Time Interval field. Click Advanced Configurations if you want to enter detailed specifications to filter events from the CSV file. Then enter information as follows. If you want to include only events that belong to a specific time interval in the simulation feed, enter the start time and the end time in the Starting Event's Timestamp and Ending Event's Timestamp fields respectively. To select a timestamp, click the time and calendar icon next to the field. Then select the required date, hour, minute, second and millisecond. Click Done to select the time stamp entered. If you want to select the current time, you can click Now . If you want to restrict the event simulation feed to a specific number of events, enter the required number in the No of Events field. In the Simulation Source field, select CSV File . Click Add Simulation Source to open the following section. In the Siddhi App Name field, select the required Siddhi application. Then more fields as shown below. Enter details as follows: In the Stream Name field, select the stream for which you want to simulate events. All the streams defined in the Siddhi App you selected are available in the list. In the CSV File field, select an available CSV file. If no CSV files are currently uploaded, select Upload File from the list. This opens the Upload File dialog box. Click Choose File and browse for the CSV file you want to upload. Then click Upload . In the Delimiter field, enter the character you want to use in order to separate the attribute values in each row of the CSV file. If you want to enter more detailed specificiations, click Advanced Configuration . Then enter details as follows. To use the index value as the event timestamp, select the Timestamp Index option. Then enter the relevant index. If you want to increase the value of the timestamp for each new event, select the Increment event time by(ms) option. Then enter the number of milliseconds by which you want to increase the timestamp of each event. If you want the events to arrive in order based on the timestamp, select Yes under the Timestamp Interval option. Click Save to save the information relating to the CSV file. The name os the CSV file appears in the Feed Simulation tab in the left panel. To simulate a CSV file that is uploaded and visible in the Feed Simulation tab in the left panel, click on the arrow to its right. The simulated events are logged n the output console. Simulating multiple events via databases This section explains how to generate multiple events via databases to be analyzed via the Streaming Integrator. Tip Before simulating events via databases: - A Siddhi application must be created. - The database from which you want to simulate events must be already configured for the Streaming Integrator. To simulate multiple events from a database, follow the procedure below: Access the Streaming Integrator Studio via the http://localhost:/editor URL. The Streaming Integrator Studio opens as shown below. Info The default URL is http://localhost:9090/editor . Click the Event Simulator icon in the left pane of the editor. Click the Feed tab to open the Feed Simulation panel. To create a new simulation, click Create . This opens the following panel. Enter values for the displayed fields as follows. In the Simulation Name field, enter a name for the event simulation. In the Description field, enter a description for the event simulation. If you want to simulate events at time intervals of a specific length, enter that length in milliseconds in the Time Interval(ms) field. If you want to enter more advanced conditions to simulate the events, click Advanced Configurations . As a result, the following section is displayed. {width=\"442\" height=\"191\"} Then enter details as follows. If you want to include only events that belong to a specific time interval in the simulation feed, enter the start time and the end time in the Starting Event's Timestamp and Ending Event's Timestamp fields respectively. To select a timestamp, click the time and calendar icon next to the field. Then select the required date, hour, minute, second and millisecond. Click Done to select the time stamp entered. If you want to select the current time, you can click Now . If you want to restrict the event simulation feed to a specific number of events, enter the required number in the No of Events field. In the Simulation Source field, select Database . To connect to a new database, click Add Simulation Source to open the following section. Enter information as follows: Field Description Siddhi App Name Select the Siddhi Application in which the event stream for which you want to simulate events is defined. Stream Name Select the event stream for which you want to simulate events. All the streams defined in the Siddhi Application you selected are available to be selected. Data Source The JDBC URL to be used to access the required database. Driver Class The driver class name of the selected database. Username The username that must be used to access the database. Password The password that must be used to access the database. Once you have entered the above information, click Connect to Database . If the datasource is correctly configured, the following is displayed to indicate that Streaming Integrator can successfully connect to the database. To use the index value as the event timestamp, select the Timestamp Index option. Then enter the relevant index. If you want the vents in the CSV file to be sorted based on the timestamp, select the Yes option under CSV File is Ordered by Timestamp . To increase the timestamp of the published events, select the Timestamp Interval option. Then enter the number of milliseconds by which you want to increase the timestamp of each event. Click Save . This adds the fed simulation you created as an active simulation in the Feed Simulation tab of the left panel as shown below. Click on the play button of this simulation to open the Run or Debug dialog box. If you want to run the Siddhi application you previously selected and simulate events for it, select Run . If you want to simulate events in the Debug mode, select Debug . Once you have selected the required mode, click Start Simulation . A message appears to inform you that the feed simulation started successfully. Similarly, when the simulation is completed, a message appears to inform you that the event simulation has finished. Generating random events This section explains how to generate random data to be analyzed via the Streaming Integrator. Tip Before simulating events, a Siddhi application should be deployed. To simulate random events, follow the steps given below: Access the Streaming Integrator Studio via the http://localhost:/editor URL. The Streaming Integrator Studio opens as shown below. Info The default URL is http://localhost:9090/editor . Click the Event Simulator icon in the left pane of the editor. Click the Feed tab to open the Feed Simulation panel. To create a new simulation, click Create . This opens the following panel. Enter values for the displayed fields as follows. In the Simulation Name field, enter a name for the event simulation. In the Description field, enter a description for the event simulation. If you want to include only events that belong to a specific time interval in the simulation feed, enter the start time and the end time in the Starting Event's Timestamp and Ending Event's Timestamp fields respectively. To select a timestamp, click the time and calendar icon next to the field. Then select the required date, hour, minute, second and millisecond. Click Done to select the time stamp entered. If you want to select the current time, you can click Now . If you want to restrict the event simulation feed to a specific number of events, enter the required number in the No of Events field. If you want to receive events only during a specific time interval, enter that time interval in the Time Interval field. In the Simulation Source field, select Random . If the random simulation source from which you want to simulate events does not already exist in the Feed Simulation pane, click Add New to open the following section. Enter information relating to the random source as follows: In the Siddhi App Name field, s elect the name of the Siddhi App with the event stream for which the events are simulated. In the Stream Name field, select the event stream for which you want to simulate events. All the streams defined in the Siddhi Application you selected are available to be selected. In the Timestamp Interval field, enter the number of milliseconds by which you want to increase the timestamp of each event. To enter values for the stream attributes, follow the instructions below. To enter a custom value for a stream attribute, select Custom data based from the list. When you select this value, data field in which the required value can be entered appears as shown in the example below. {width=\"235\" height=\"88\"} To enter a primitive based value, select Primitive based from the list. The information to be entered varies depending on the data type of the attribute. The following table explains the information you need to enter when you select Primitive based for each data type. Data Type Values to enter STRING Specify a length in the Length field that appears. This results in a value of the specified length being auto-generated. FLOAT or DOUBLE The value generated for the attribute is based on the following values specified. Min : The minimum value. Max : The maximum value. Precision : The precise value. The number of decimals included in the auto-generated values are the same as that of the value specified here. INT or LONG The value generated for the attribute is based on the following values specified. Min : The minimum value. Max : The maximum value. BOOL No further information is required because true and false values are randomly generated. To randomly assign values based on a pre-defined set of meaningful values, select Property based from the list. When you select this value, a field in which the set of available values are listed appears as shown in the example below. To assign a regex value, select Regex based from the list. Click Save to save the simulation information. The saved random simulation appears in the Feed tab of the left panel. To simulate events, click the arrow to the right os the saved simulation (shown in the example below). The simulated events are logged in the CLI as shown in the extract below.","title":"Testing a Siddhi Application"},{"location":"develop/testing-a-Siddhi-Application/#testing-siddhi-applications","text":"The Streaming Integrator allows the following tasks to be carried out to ensure that the Siddhi applications you create and deploy are validated before they are run in an actual production environment. Validate Siddhi applications that are written in the Streaming Integrator Studio. Run Siddhi applications that were written in the Streaming Integrator Studio in either Run or Debug mode. Simulate events to test the Siddhi applications and analyze events that are received and sent. This allows you to analyze the status of each query within a Siddhi application at different execution points.","title":"Testing Siddhi Applications"},{"location":"develop/testing-a-Siddhi-Application/#validating-a-siddhi-application","text":"To validate a Siddhi application, follow the procedure below: Start and access the Streaming Integrator Studio. For detailed instructions, see Starting Stream Integration Studio . In this example, let's use an existing sample as an example. Click on the ReceiveAndCount sample to open it. Sample opens in a new tab. This sample does not have errors, and therefore, no errors are displayed in the editor. To create an error for demonstration purposes, change the count() function in the query1 query to countNew() as shown below. @info(name='query1') from SweetProductionStream select countNew() as totalCount insert into TotalCountStream; ` Now, the editor indicates that there is a syntax error. If you move the cursor over the error icon, it indicates that countNew is an invalid function name as shown below.","title":"Validating a Siddhi application"},{"location":"develop/testing-a-Siddhi-Application/#running-or-debugging-a-siddhi-application","text":"You can run or debug a Siddhi application to verify whether the logic you have written is correct. To start a Siddhi application in the run/debug mode, follow the procedure below: Start and access the Streaming Integrator Studio. For detailed instructions, see Starting Stream Integration Studio . For this example, click the existing sample ReceiveAndCount . It opens in a new untitled tab. Save the Siddhi file so that you can run it in the Run or Debug mode. To save it, click File => Save . Once the file is saved, you can see the Run and Debug menu options enabled as shown below. To start the application in Run mode, click Run => Run . This logs the following output in the console. Start the application in the Debug mode, click Run => Debug . As a result, the following mesage is logged in the console. You can also note that another console tab is opened with debug options. To create an error for demonstration purposes, change the count() function in the query1 query to countNew() , and save. Then click Run => Run . As a result, the following output is logged in the console.","title":"Running or debugging a Siddhi application"},{"location":"develop/testing-a-Siddhi-Application/#simulating-events","text":"This section demonstrates how to test a Siddhi application via event simulation. Event simulation involves simulating predefined event streams. These event stream definitions have stream attributes. You can use event simulator to create events by assigning values to the defined stream attributes and send them as events. This is useful for testing Siddhi applications in order to evaluate whether they function as expected Events can be simulated in the following methods: Simulating a single event Simulating multiple events via CSV files Simulating multiple events via databases Generating random events Tip Before you simulate events for a Siddhi application, you need to run or debug it. Therefore, before you try this section, see Running or debugging a Siddhi application .","title":"Simulating events"},{"location":"develop/testing-a-Siddhi-Application/#simulating-a-single-event","text":"This section demonstrates how to simulate a single event to be processed via the Streaming Integrator. Tip Before simulating events, a Siddhi application should be deployed. To simulate a single event, follow the steps given below. Access the Streaming Integrator Studio via the http://localhost:/editor URL. The Streaming Integrator Studio opens as shown below. Info The default URL is http://localhost:9090/editor . Click the Event Simulator icon in the left pane of the editor to open the Single Simulation panel. It opens the left panel for event simulation as follows. Enter Information in the Single Simulation panel as described below. In the Siddhi App Name field, select a currently deployed Siddhi application. In the Stream Name field, select the event stream for which you want to simulate events. The list displays all the event streams defined in the selected Siddhi application. If you want to simulate the event for a specific time different to the current time, enter a valid timestamp in the Timestamp field. To select a timestamp, click the time and calendar icon next to the Timestamp field. Then select the required date, hour, minute, second and millisecond. Click Done to select the time stamp entered. If you want to select the current time, you can click Now . Enter values for the attributes of the selected stream. Click Send to start sending the event. The simulated event is logged similar to the sample log given below.","title":"Simulating a single event"},{"location":"develop/testing-a-Siddhi-Application/#simulating-multiple-events-via-csv-files","text":"This section explains how to generate multiple events via CSV files to be analyzed via the Streaming Integrator. Tip Before simulating events, a Siddhi application should be deployed. To simulate multiple events from a CSV file, follow the steps given below. Access the Streaming Integrator Studio via the http://localhost:/editor URL. The Streaming Integrator Studio opens as shown below. Info The default URL is http://localhost:9090/editor . Click the Event Simulator icon in the left pane of the editor. In the event simulation left panel that opens, click on the Feed Simulation tab. To create a new simulation, click Create . This opens the following panel. Enter values for the displayed fields as follows. In the Simulation Name field, enter a name for the event simulation. In the Description field, enter a description for the event simulation. If you want to receive events only during a specific time interval, enter that time interval in the Time Interval field. Click Advanced Configurations if you want to enter detailed specifications to filter events from the CSV file. Then enter information as follows. If you want to include only events that belong to a specific time interval in the simulation feed, enter the start time and the end time in the Starting Event's Timestamp and Ending Event's Timestamp fields respectively. To select a timestamp, click the time and calendar icon next to the field. Then select the required date, hour, minute, second and millisecond. Click Done to select the time stamp entered. If you want to select the current time, you can click Now . If you want to restrict the event simulation feed to a specific number of events, enter the required number in the No of Events field. In the Simulation Source field, select CSV File . Click Add Simulation Source to open the following section. In the Siddhi App Name field, select the required Siddhi application. Then more fields as shown below. Enter details as follows: In the Stream Name field, select the stream for which you want to simulate events. All the streams defined in the Siddhi App you selected are available in the list. In the CSV File field, select an available CSV file. If no CSV files are currently uploaded, select Upload File from the list. This opens the Upload File dialog box. Click Choose File and browse for the CSV file you want to upload. Then click Upload . In the Delimiter field, enter the character you want to use in order to separate the attribute values in each row of the CSV file. If you want to enter more detailed specificiations, click Advanced Configuration . Then enter details as follows. To use the index value as the event timestamp, select the Timestamp Index option. Then enter the relevant index. If you want to increase the value of the timestamp for each new event, select the Increment event time by(ms) option. Then enter the number of milliseconds by which you want to increase the timestamp of each event. If you want the events to arrive in order based on the timestamp, select Yes under the Timestamp Interval option. Click Save to save the information relating to the CSV file. The name os the CSV file appears in the Feed Simulation tab in the left panel. To simulate a CSV file that is uploaded and visible in the Feed Simulation tab in the left panel, click on the arrow to its right. The simulated events are logged n the output console.","title":"Simulating multiple events via CSV files"},{"location":"develop/testing-a-Siddhi-Application/#simulating-multiple-events-via-databases","text":"This section explains how to generate multiple events via databases to be analyzed via the Streaming Integrator. Tip Before simulating events via databases: - A Siddhi application must be created. - The database from which you want to simulate events must be already configured for the Streaming Integrator. To simulate multiple events from a database, follow the procedure below: Access the Streaming Integrator Studio via the http://localhost:/editor URL. The Streaming Integrator Studio opens as shown below. Info The default URL is http://localhost:9090/editor . Click the Event Simulator icon in the left pane of the editor. Click the Feed tab to open the Feed Simulation panel. To create a new simulation, click Create . This opens the following panel. Enter values for the displayed fields as follows. In the Simulation Name field, enter a name for the event simulation. In the Description field, enter a description for the event simulation. If you want to simulate events at time intervals of a specific length, enter that length in milliseconds in the Time Interval(ms) field. If you want to enter more advanced conditions to simulate the events, click Advanced Configurations . As a result, the following section is displayed. {width=\"442\" height=\"191\"} Then enter details as follows. If you want to include only events that belong to a specific time interval in the simulation feed, enter the start time and the end time in the Starting Event's Timestamp and Ending Event's Timestamp fields respectively. To select a timestamp, click the time and calendar icon next to the field. Then select the required date, hour, minute, second and millisecond. Click Done to select the time stamp entered. If you want to select the current time, you can click Now . If you want to restrict the event simulation feed to a specific number of events, enter the required number in the No of Events field. In the Simulation Source field, select Database . To connect to a new database, click Add Simulation Source to open the following section. Enter information as follows: Field Description Siddhi App Name Select the Siddhi Application in which the event stream for which you want to simulate events is defined. Stream Name Select the event stream for which you want to simulate events. All the streams defined in the Siddhi Application you selected are available to be selected. Data Source The JDBC URL to be used to access the required database. Driver Class The driver class name of the selected database. Username The username that must be used to access the database. Password The password that must be used to access the database. Once you have entered the above information, click Connect to Database . If the datasource is correctly configured, the following is displayed to indicate that Streaming Integrator can successfully connect to the database. To use the index value as the event timestamp, select the Timestamp Index option. Then enter the relevant index. If you want the vents in the CSV file to be sorted based on the timestamp, select the Yes option under CSV File is Ordered by Timestamp . To increase the timestamp of the published events, select the Timestamp Interval option. Then enter the number of milliseconds by which you want to increase the timestamp of each event. Click Save . This adds the fed simulation you created as an active simulation in the Feed Simulation tab of the left panel as shown below. Click on the play button of this simulation to open the Run or Debug dialog box. If you want to run the Siddhi application you previously selected and simulate events for it, select Run . If you want to simulate events in the Debug mode, select Debug . Once you have selected the required mode, click Start Simulation . A message appears to inform you that the feed simulation started successfully. Similarly, when the simulation is completed, a message appears to inform you that the event simulation has finished.","title":"Simulating multiple events via databases"},{"location":"develop/testing-a-Siddhi-Application/#generating-random-events","text":"This section explains how to generate random data to be analyzed via the Streaming Integrator. Tip Before simulating events, a Siddhi application should be deployed. To simulate random events, follow the steps given below: Access the Streaming Integrator Studio via the http://localhost:/editor URL. The Streaming Integrator Studio opens as shown below. Info The default URL is http://localhost:9090/editor . Click the Event Simulator icon in the left pane of the editor. Click the Feed tab to open the Feed Simulation panel. To create a new simulation, click Create . This opens the following panel. Enter values for the displayed fields as follows. In the Simulation Name field, enter a name for the event simulation. In the Description field, enter a description for the event simulation. If you want to include only events that belong to a specific time interval in the simulation feed, enter the start time and the end time in the Starting Event's Timestamp and Ending Event's Timestamp fields respectively. To select a timestamp, click the time and calendar icon next to the field. Then select the required date, hour, minute, second and millisecond. Click Done to select the time stamp entered. If you want to select the current time, you can click Now . If you want to restrict the event simulation feed to a specific number of events, enter the required number in the No of Events field. If you want to receive events only during a specific time interval, enter that time interval in the Time Interval field. In the Simulation Source field, select Random . If the random simulation source from which you want to simulate events does not already exist in the Feed Simulation pane, click Add New to open the following section. Enter information relating to the random source as follows: In the Siddhi App Name field, s elect the name of the Siddhi App with the event stream for which the events are simulated. In the Stream Name field, select the event stream for which you want to simulate events. All the streams defined in the Siddhi Application you selected are available to be selected. In the Timestamp Interval field, enter the number of milliseconds by which you want to increase the timestamp of each event. To enter values for the stream attributes, follow the instructions below. To enter a custom value for a stream attribute, select Custom data based from the list. When you select this value, data field in which the required value can be entered appears as shown in the example below. {width=\"235\" height=\"88\"} To enter a primitive based value, select Primitive based from the list. The information to be entered varies depending on the data type of the attribute. The following table explains the information you need to enter when you select Primitive based for each data type. Data Type Values to enter STRING Specify a length in the Length field that appears. This results in a value of the specified length being auto-generated. FLOAT or DOUBLE The value generated for the attribute is based on the following values specified. Min : The minimum value. Max : The maximum value. Precision : The precise value. The number of decimals included in the auto-generated values are the same as that of the value specified here. INT or LONG The value generated for the attribute is based on the following values specified. Min : The minimum value. Max : The maximum value. BOOL No further information is required because true and false values are randomly generated. To randomly assign values based on a pre-defined set of meaningful values, select Property based from the list. When you select this value, a field in which the set of available values are listed appears as shown in the example below. To assign a regex value, select Regex based from the list. Click Save to save the simulation information. The saved random simulation appears in the Feed tab of the left panel. To simulate events, click the arrow to the right os the saved simulation (shown in the example below). The simulated events are logged in the CLI as shown in the extract below.","title":"Generating random events"},{"location":"develop/working-with-design-view/","text":"","title":"Working with design view"},{"location":"develop/working-with-the-Design-View/","text":"Working with the Design View This section provides an overview of the design view of the Streaming Integrator Tooling. Accesing the Design View To open the design view of the Streaming Integrator Tooling: Start the Stream Processor Studio and log in with your credentials. For detailed instructions, see Streaming Integrator Tooling Overview - Starting Streaming Integrator Tooling . Click New and open a new Siddhi file, or click Open and open an existing Siddhi file. Click Design View to open the Design View. The design view opens as shown in the example below. It consists of a grid to which you can drag and drop the Siddhi components represented by icons displayed in the left panel to design a Siddhi application. Adding Siddhi components To add a Siddhi component to the Siddhi application that you are creating/editing in the design view, click on the relevant icon in the left pane, and then drag and drop it to the grid as demonstrated in the example below. Once you add a Siddhi component, you can configure it as required. To configure a Siddhi component, click the settings icon on the component. This opens a form with parameters related to the relevant component. The following is the complete list of Siddhi components that you can add to the grid of the design view when you create a Siddhi application. Stream Icon Description A stream represents a logical series of events ordered in time. For a detailed description of streams, see Siddhi Query Guide - Stream . Form To configure the stream, click the settings icon on the stream component you added to the grid. Then enter values as follows: Stream Name : A unique name for the stream. This should be specified in title caps, and without spaces (e.g., ProductionDataStream ). Attributes : Attributes of streams are specified as name and type pairs in the Attributes table. Example The details entered in the above form creates a stream configuration as follows: define stream SweetProductionStream (amount double , name string); Source Sources Projection queries Filter queries Window queries Join queries Target Sinks Projection queries Filter queries Window queries Join queries Source Icon Description A source receives events in the specified transport and in the specified format. For more information, see Siddhi Query Guide - Source . Form To configure the source, click the settings icon on the source component you added to the grid. This opens a form where you can enter the following information: !!! info To access the form in which you can configure a source, you must first connect the source as the source (input) object to a stream component. Source Type : This specifies the transport type via which the events are received. The value should be entered in lower case (e.g., t cp ). The other parameters displayed for the source depends on the source type selected. Map Type : This specifies the format in which you want to receive the events (e.g., xml ). The other parameters displayed for the map depends on the map type selected. If you want to add more configurations to the mapping, click Customized Options and set the required properties and key value pairs. Map Attribute as Key/Value Pairs : If this check box is selected, you can define custom mapping by entering key value pairs. You can add as many key value pairs as required under this check box. Example The details entered in the above form creates a source configuration as follows: @source (type = tcp , @map (type = json , @attributes (name = $.sweet , amount = $.batch.count ))) Source No connection can start from another Siddhi component and link to a source because a source is the point from which events selected into the event flow of the Siddhi application start. Target Streams Sink Icon Description A sink publishes events via the specified transport and in the specified format. For more information, see Siddhi Query Guide - Sink . Form To configure the sink, click the settings icon on the sink component you added to the grid. !!! info To access the form in which you can configure a sink, you must first connect the sinke as the target object to a stream component. Sink Type : This specifies the transport via which the sink publishes processed events. The value should be entered in lower case (e.g., log ). Map Type : This specifies the format in which you want to publish the events (e.g., passThrough ). The other parameters displayed for the map depends on the map type selected. If you want to add more configurations to the mapping, click Customized Options and set the required properties and key value pairs. Map Attribute as Key/Value Pairs : If this check box is selected, you can define custom mapping by entering key value pairs. You can add as many key value pairs as required under this check box. Example The details entered in the above form creates a sink configuration as follows: @sink(type = log , prefix = Sweet Totals: ) Source Streams Target N/A A sink cannot be followed by another Siddhi component because it represents the last stage of the event flow where the results of the processing carried out by the Siddhi application are communicated via the required interface. Table Icon Description A table is a stored version of an stream or a table of events. For more information, see Siddhi Query Guide - Table . Form To configure the table, click the settings icon on the table component you added to the grid. Name : This field specifies unique name for the table. This should be specified in title caps, and without spaces (e.g., ProductionDataTable ). Attributes : Attributes of tables are specified as name and type pairs in the Attributes table. To add a new attribute, click +Attribute . Store Type : This specifies the specific database type in which you want to stopre data or whether the data is to be stored in-memory. Once the store type is selected, select an option to indicate whether the datastore needs to be defined inline, whether you want to use a datasource defined in the SP_HOME /conf/worker/deployment.yaml file, or connected to a JNDI resource. For more information, see Defining Tables for Physical Stores . The other parameters configured under Store Type depend on the store type you select. Annotations : This section allows you to specify the table attributes you want to use as the primary key and indexes via the @primarykey and @index annotations. For more information, see Defining Data Tables . If you want to add any other custom annotations to your table definition, click +Annotation to define them. Example The details entered in the above form creates a table definition as follows: @store(type = rdbms , datasource = SweetProductionDB ) define table ShipmentDetails (name string, supplier string, amount double ); Source Projection queries Window queries Filter queries Join queries Target Projection queries Window queries Filter queries Join queries Window Icon Description This icon represents a window definition that can be shared across multiple queries. For more information, see Siddhi Query Guide - (Defined) Window . Form To configure the window, click the settings icon on the window component you added to the grid, and update the following information. Name : This field specifies a unique name for the window. PascalCase is used for window names as a convention. Attributes : Attributes of windows are specified as name and type pairs in the Attributes table. Window Type : This specifies the function of the window (i.e., the window type such as time , length , frequent etc.). The window types supported include time , timeBatch , timeLength , length , lengthBatch , sort , frequent , lossyFrequent , cron , externalTime , externalTimeBatch . Parameters : This section allows you to define one or more parameters for the window definition based on the window type you entered in the Window Type field. Annotations : If you want to add any other custom annotations to your window definition, click +Annotation to define them. Example The details entered in the above form creates a window definition as follows: define window FiveMinTempWindow (roomNo int , temp double ) time ( 5 min ) output all events ; Source Projection queries Window queries Filter queries Join queries Target Projection queries Window queries Filter queries Join queries Trigger Icon Description A trigger allows you to generate events periodically. For more information, see Siddhi Query Guide - Trigger . Form To configure the trigger, click the settings icon on the trigger component you added to the grid, and update the following information. Name : A unique name for the trigger. Trigger Criteria : This specifies the criteria based on which the trigger is activated. Possible values are as follows: start : Select this to trigger events when the SP server has started. every : Select this to specify a time interval at which events should be triggered. cron-expression : Select this to enter a cron expression based on which the events can be triggered. For more information about cron expressions, see the quartz-scheduler . Example The details entered in the above orm creates a trigger definition as follows: define trigger FiveMinTriggerStream at every 5 min ; Source N/A Target Projection queries Window queries Filter queries Join queries Aggregation Icon Description Incremental aggregation allows you to obtain aggregates in an incremental manner for a specified set of time periods. For more information, see Siddhi Query Guide - Incremental Aggregation . !!! tip Before you add an aggregation: Make sure that you have already added the stream with the events to which the aggregation is applied is already defined. Form To configure the aggregation, click the settings icon on the aggregation component you added to the grid, and update the following information. Aggregation Meta Information : In this section, define a unique name for the aggregation in the Name field, and specify the stream from which the input information is taken to perform the aggregations. You can also select the optional annotations you want to use in the aggregation definition by selecting the relevant check boxes. For more information about configuring the annotations once you select them, see Incremental Analysis . Projection : This section specifies the attributes to be included in the aggregation query. In the Select field, you can select All attributes to perform the aggregation for all the attributes of the stream specified under Input , or select User Defined Attributes to select specific attributes. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Aggregation Criteria : Here, you can specify the time values based on which the aggregates are calculated. Example The details entered in the above form creates an aggregation definition as follows: define aggregation TradeAggregation from TradeStream select symbol, avg (price) as avgPrice, sum (price) as total group by symbol aggregate by timestamp every seconds .. .years; Source N/A Target Join queries Function Icon Description The function icon represents Script in Siddhi Query Language . It allows you to write functions in other programming languages and execute them within Siddhi queries. A function component in a Siddhi application is not connected to ther Siddhi components in the design UI. However, the configuration of one or more Query components can include a reference to it. Form To configure the function, click the settings icon on the function component you added to the grid, and update the following information. Name : A unique name for the function. Script Type : The language in which the function is written. Return Value : The data format of the value that is generated as the output via the function. Script Body : This is a free text field to write the function in the specified script type. Example The details entered in the above form creates a function definition as follows: define function concatFN[JAVASCRIPT] return string { var str1 = data [ 0 ]; var str2 = data [ 1 ]; var str3 = data [ 2 ]; var responce = str1 + str2 + str3; return responce; }; Projection Query Icon Description !!! tip Before you add a projection query: You need to add and configure the following: The input stream with the events to be processed by the query. The output stream to which the events processed by the query are directed. This icon represents a query to project the events in an input stream to an output stream. This invoves selectng the attributes to be included in the output, renaming attributes, introducing constant values, and using mathematical and/or logical expressions. For more information, see Siddhi Query Guide - Query Projection . Form Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the projection query, click the settings icon on the projection query component you added to the grid, and update the following information. Query Meta Information : This section specifies the stream to be considered as the input stream with the events to which the query needs to be applied. The input stream connected to the query as the source is automatically displayed. Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events.| Example The details entered in the above form creates a query as follows: from TradeStream select symbol, avg (price) as averagePrice, sum (volume) as total insert all events into OutputStream; Source Streams Tables Triggers Windows Target Streams Tables Windows Filter Query Icon Description !!! tip Before you add a filter query: You need to add and configure the following: The input stream with the events to be processed by the query. The output stream to which the events processed by the query are directed. A filter query filters information in an input stream based on a given condition. For more information, see Siddhi Query Guide - Filters . Form Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the filter query, click the settings icon on the filter query component you added to the grid, and update the following information. By default, the Stream Handler check box is selected, and a stream handler of the filter type is available under it to indicate that the query is a filter. Expand this stream handler, and enter the condition based on which the information needs to be filtered. !!! info A Siddhi application can have multiple stream handlers. To add another stream handler, click the + Stream Handler . Multiple functions, filters and windows can be defined within the same form as stream handlers. Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events. Example The details entered in the above form creates a query with a filter as follows: from TradeStream[ sum (amount) 10000 ] select symbol, avg (price) as averagePrice, sum (amount) as total insert all events into OutputStream; Source Streams Tables Triggers Windows Target Streams Tables Windows Window Query Icon Description !!! tip Before you add a window query: You need to add and configure the following: The input stream with the events to be processed by the query. The output stream to which the events processed by the query are directed. Window queries include a window to select a subset of events to be processed based on a specific criterion. For more information, see Siddhi Query Guide - (Defined) Window . Form Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the window query, click the settings icon on the window query component you added to the grid, and update the following information. By default, the Stream Handler check box is selected, and a stream handler of the window type is available under it to indicate that the query is a filter. Expand this stream handler, and enter details to determine the window including the window type and the basis on which the subset of events considered by the window is determined (i.e., based on the window type selected). !!! info A Siddhi application can have multiple stream handlers. To add another stream handler, click the + Stream Handler . Multiple functions, filters and windows can be defined within the same form as stream handlers. Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events. Example The details entered in the above Query Configuration form creates a query with a window as follows: from TradeStream#window. time ( 1 month ) select symbol, avg (price) as averagePrice, sum (amount) as total insert all events into OutputStream; Source !!! info A window query can have only one source at a given time. Streams Tables Triggers Windows Target Streams Tables Windows Join Query Icon Description A join query derives a combined result from two streams in real-time based on a specified condition. For more information, see Siddhi Query Guide - Join . Form Once you connect two Siddhi components to the join query as sources and another Siddhi component as the target, you can configure the join query. To configure the join query, click the settings icon on the join query component you added to the grid and update the following information. Query Meta Information : In this section, enter a unique name for the query and any annotations that you want to include in the query. The @dist annotation is supported by default to use the query in a fully distributed deployment if required (for more information, see Converting to a Distributed Streaming Application ). You can also add customized annotations. Input : Here, you can specify the input sources, the references, the join type, join condition, and stream handlers for the left source and right source of the join. For a detailed explanation of the join concept, see Siddhi Query Guide - Joins . Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events. Example A join query is configured as follows: The above configurations result in creating the following join query. from TempStream[temp 30.0 ]#window. time ( 1 min ) as T join RegulatorStream[isOn == false ]#window. length ( 1 ) as R on T.roomNo == R.roomNo select T.roomNo, R.deviceID, start as action insert into RegulatorActionStream; Source !!! info A join query must always be connected to two sources, and at least one of them must be a defined stream/trigger/window. Streams Tables Aggregations Windows Target !!! info A join query must always be connected to a single target. Streams Tables Windows Pattern Query Icon Description !!! tip Before you add a pattern query: You need to add and configure the following: The input stream with the events to be processed by the query. The output stream to which the events processed by the query are directed. A pattern query detects patterns in events that arrive overtime. For more information, see Siddhi Query Guide - Patterns . Form Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the pattern query, click the settings icon on the pattern query component you added to the grid and update the following information. Query Meta Information : In this section, enter a unique name for the query and any annotations that you want to include in the query. The @dist annotation is supported by default to use the query in a fully distributed deployment if required (for more information, see Converting to a Distributed Streaming Application ). You can also add customized annotations. Input : This section defines the conditions based on which patterns are identified. This involves specifying a unique ID and the input stream considered for each condition. Multiple conditions can be added. Each condition is configured in a separate tab within this section. For more information about the Pattern concept, see Siddhi Query Guide - Patterns . Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events. Example The above configuration results in creating the following query. from every (e1 = MaterialSupplyStream) - not MaterialConsumptionStream[name == e1.name and amount == e1.amount] for 15 sec select e1.name, e1.amount insert into ProductionDelayAlertStream; Source Streams Tables Triggers Windows Target Streams Tables Windows Sequence Query Icon Description !!! tip Before you add a sequence query: You need to add and configure the following: The input stream with the events to be processed by the query. The output stream to which the events processed by the query are directed. A sequence query detects sequences in event occurrences over time. For more information, see Siddhi Query Guide - Sequence . Form Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the sequence query, click the settings icon on the sequence query component you added to the grid and update the following information. Query Meta Information : In this section, enter a unique name for the query and any annotations that you want to include in the query. The @dist annotation is supported by default to use the query in a fully distributed deployment if required (for more information, see Converting to a Distributed Streaming Application ). You can also add customized annotations. Input : This section defines the conditions based on which sequences are identified. This involves specifying a unique ID and the input stream considered for each condition. Multiple conditions can be added. Each condition is configured in a separate tab within this section. For more information about the Sequence concept, see Siddhi Query Guide - Sequences . Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events. Example The above configuration results in creating the following query. from every e1 = SweetProductionStream, e2 = SweetProductionStream[e1.amount amount and ( timestamp - e1. timestamp ) 10 * 60000 ] * , e3 = SweetProductionStream[ timestamp - e1. timestamp 10 * 60000 and e1.amount amount] select e1.name, e1.amount as initialAmount, e2.amount as finalAmount, e2. timestamp insert into DecreasingTrendAlertStream; Source Streams Tables Triggers Windows Target Streams Tables Windows Partitions Icon Description !!! tip Before you add a partition: You need to add the stream to be partitioned. Partitions divide streams and queries into isolated groups in order to process them in parallel and in isolation. For more information, see Siddhi Query Guide - Partition . Form Once the stream to be partitioned is connected as a source to the partition, you can configure the partition. In order to do so, move the cursor over the partition and click the settings icon on the partition component. This opens the Partition Configuration form. In this form, you can enter expressions to convert the attributes of the stream that is selected to be partitioned. Example The above configuration creates the following partition query. partition with ( roomNo = 1030 as serverRoom or roomNo 1030 and roomNo = 330 as officeRoom or roomNo 330 as lobby of TempStream) begin from TempStream#window. time ( 10 min ) select roomNo, deviceID, avg (temp) as avgTemp insert into AreaTempStream end ; Source Streams Target N/A Connecting Siddhi components In order to define how the Siddhi components in a Siddhi application interact with each other to process events, you need to define connections between Siddhi components. A connection is defined by drawing an arrow from one component to another by dragging the cursor as demonstrated below. Saving, running and debugging Siddhi applications To save a Siddhi application that you created in the design view, you need to switch to the source view. You also need to switch to the source view to run or debug a Siddhi application. For more information, see the following sections: Stream Processor Studio Overview Debugging a Siddhi Application","title":"Working with the Design View"},{"location":"develop/working-with-the-Design-View/#working-with-the-design-view","text":"This section provides an overview of the design view of the Streaming Integrator Tooling.","title":"Working with the Design View"},{"location":"develop/working-with-the-Design-View/#accesing-the-design-view","text":"To open the design view of the Streaming Integrator Tooling: Start the Stream Processor Studio and log in with your credentials. For detailed instructions, see Streaming Integrator Tooling Overview - Starting Streaming Integrator Tooling . Click New and open a new Siddhi file, or click Open and open an existing Siddhi file. Click Design View to open the Design View. The design view opens as shown in the example below. It consists of a grid to which you can drag and drop the Siddhi components represented by icons displayed in the left panel to design a Siddhi application.","title":"Accesing the Design View"},{"location":"develop/working-with-the-Design-View/#adding-siddhi-components","text":"To add a Siddhi component to the Siddhi application that you are creating/editing in the design view, click on the relevant icon in the left pane, and then drag and drop it to the grid as demonstrated in the example below. Once you add a Siddhi component, you can configure it as required. To configure a Siddhi component, click the settings icon on the component. This opens a form with parameters related to the relevant component. The following is the complete list of Siddhi components that you can add to the grid of the design view when you create a Siddhi application.","title":"Adding Siddhi components"},{"location":"develop/working-with-the-Design-View/#stream","text":"Icon Description A stream represents a logical series of events ordered in time. For a detailed description of streams, see Siddhi Query Guide - Stream . Form To configure the stream, click the settings icon on the stream component you added to the grid. Then enter values as follows: Stream Name : A unique name for the stream. This should be specified in title caps, and without spaces (e.g., ProductionDataStream ). Attributes : Attributes of streams are specified as name and type pairs in the Attributes table. Example The details entered in the above form creates a stream configuration as follows: define stream SweetProductionStream (amount double , name string); Source Sources Projection queries Filter queries Window queries Join queries Target Sinks Projection queries Filter queries Window queries Join queries","title":"Stream"},{"location":"develop/working-with-the-Design-View/#source","text":"Icon Description A source receives events in the specified transport and in the specified format. For more information, see Siddhi Query Guide - Source . Form To configure the source, click the settings icon on the source component you added to the grid. This opens a form where you can enter the following information: !!! info To access the form in which you can configure a source, you must first connect the source as the source (input) object to a stream component. Source Type : This specifies the transport type via which the events are received. The value should be entered in lower case (e.g., t cp ). The other parameters displayed for the source depends on the source type selected. Map Type : This specifies the format in which you want to receive the events (e.g., xml ). The other parameters displayed for the map depends on the map type selected. If you want to add more configurations to the mapping, click Customized Options and set the required properties and key value pairs. Map Attribute as Key/Value Pairs : If this check box is selected, you can define custom mapping by entering key value pairs. You can add as many key value pairs as required under this check box. Example The details entered in the above form creates a source configuration as follows: @source (type = tcp , @map (type = json , @attributes (name = $.sweet , amount = $.batch.count ))) Source No connection can start from another Siddhi component and link to a source because a source is the point from which events selected into the event flow of the Siddhi application start. Target Streams","title":"Source"},{"location":"develop/working-with-the-Design-View/#sink","text":"Icon Description A sink publishes events via the specified transport and in the specified format. For more information, see Siddhi Query Guide - Sink . Form To configure the sink, click the settings icon on the sink component you added to the grid. !!! info To access the form in which you can configure a sink, you must first connect the sinke as the target object to a stream component. Sink Type : This specifies the transport via which the sink publishes processed events. The value should be entered in lower case (e.g., log ). Map Type : This specifies the format in which you want to publish the events (e.g., passThrough ). The other parameters displayed for the map depends on the map type selected. If you want to add more configurations to the mapping, click Customized Options and set the required properties and key value pairs. Map Attribute as Key/Value Pairs : If this check box is selected, you can define custom mapping by entering key value pairs. You can add as many key value pairs as required under this check box. Example The details entered in the above form creates a sink configuration as follows: @sink(type = log , prefix = Sweet Totals: ) Source Streams Target N/A A sink cannot be followed by another Siddhi component because it represents the last stage of the event flow where the results of the processing carried out by the Siddhi application are communicated via the required interface.","title":"Sink"},{"location":"develop/working-with-the-Design-View/#table","text":"Icon Description A table is a stored version of an stream or a table of events. For more information, see Siddhi Query Guide - Table . Form To configure the table, click the settings icon on the table component you added to the grid. Name : This field specifies unique name for the table. This should be specified in title caps, and without spaces (e.g., ProductionDataTable ). Attributes : Attributes of tables are specified as name and type pairs in the Attributes table. To add a new attribute, click +Attribute . Store Type : This specifies the specific database type in which you want to stopre data or whether the data is to be stored in-memory. Once the store type is selected, select an option to indicate whether the datastore needs to be defined inline, whether you want to use a datasource defined in the SP_HOME /conf/worker/deployment.yaml file, or connected to a JNDI resource. For more information, see Defining Tables for Physical Stores . The other parameters configured under Store Type depend on the store type you select. Annotations : This section allows you to specify the table attributes you want to use as the primary key and indexes via the @primarykey and @index annotations. For more information, see Defining Data Tables . If you want to add any other custom annotations to your table definition, click +Annotation to define them. Example The details entered in the above form creates a table definition as follows: @store(type = rdbms , datasource = SweetProductionDB ) define table ShipmentDetails (name string, supplier string, amount double ); Source Projection queries Window queries Filter queries Join queries Target Projection queries Window queries Filter queries Join queries","title":"Table"},{"location":"develop/working-with-the-Design-View/#window","text":"Icon Description This icon represents a window definition that can be shared across multiple queries. For more information, see Siddhi Query Guide - (Defined) Window . Form To configure the window, click the settings icon on the window component you added to the grid, and update the following information. Name : This field specifies a unique name for the window. PascalCase is used for window names as a convention. Attributes : Attributes of windows are specified as name and type pairs in the Attributes table. Window Type : This specifies the function of the window (i.e., the window type such as time , length , frequent etc.). The window types supported include time , timeBatch , timeLength , length , lengthBatch , sort , frequent , lossyFrequent , cron , externalTime , externalTimeBatch . Parameters : This section allows you to define one or more parameters for the window definition based on the window type you entered in the Window Type field. Annotations : If you want to add any other custom annotations to your window definition, click +Annotation to define them. Example The details entered in the above form creates a window definition as follows: define window FiveMinTempWindow (roomNo int , temp double ) time ( 5 min ) output all events ; Source Projection queries Window queries Filter queries Join queries Target Projection queries Window queries Filter queries Join queries","title":"Window"},{"location":"develop/working-with-the-Design-View/#trigger","text":"Icon Description A trigger allows you to generate events periodically. For more information, see Siddhi Query Guide - Trigger . Form To configure the trigger, click the settings icon on the trigger component you added to the grid, and update the following information. Name : A unique name for the trigger. Trigger Criteria : This specifies the criteria based on which the trigger is activated. Possible values are as follows: start : Select this to trigger events when the SP server has started. every : Select this to specify a time interval at which events should be triggered. cron-expression : Select this to enter a cron expression based on which the events can be triggered. For more information about cron expressions, see the quartz-scheduler . Example The details entered in the above orm creates a trigger definition as follows: define trigger FiveMinTriggerStream at every 5 min ; Source N/A Target Projection queries Window queries Filter queries Join queries","title":"Trigger"},{"location":"develop/working-with-the-Design-View/#aggregation","text":"Icon Description Incremental aggregation allows you to obtain aggregates in an incremental manner for a specified set of time periods. For more information, see Siddhi Query Guide - Incremental Aggregation . !!! tip Before you add an aggregation: Make sure that you have already added the stream with the events to which the aggregation is applied is already defined. Form To configure the aggregation, click the settings icon on the aggregation component you added to the grid, and update the following information. Aggregation Meta Information : In this section, define a unique name for the aggregation in the Name field, and specify the stream from which the input information is taken to perform the aggregations. You can also select the optional annotations you want to use in the aggregation definition by selecting the relevant check boxes. For more information about configuring the annotations once you select them, see Incremental Analysis . Projection : This section specifies the attributes to be included in the aggregation query. In the Select field, you can select All attributes to perform the aggregation for all the attributes of the stream specified under Input , or select User Defined Attributes to select specific attributes. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Aggregation Criteria : Here, you can specify the time values based on which the aggregates are calculated. Example The details entered in the above form creates an aggregation definition as follows: define aggregation TradeAggregation from TradeStream select symbol, avg (price) as avgPrice, sum (price) as total group by symbol aggregate by timestamp every seconds .. .years; Source N/A Target Join queries","title":"Aggregation"},{"location":"develop/working-with-the-Design-View/#function","text":"Icon Description The function icon represents Script in Siddhi Query Language . It allows you to write functions in other programming languages and execute them within Siddhi queries. A function component in a Siddhi application is not connected to ther Siddhi components in the design UI. However, the configuration of one or more Query components can include a reference to it. Form To configure the function, click the settings icon on the function component you added to the grid, and update the following information. Name : A unique name for the function. Script Type : The language in which the function is written. Return Value : The data format of the value that is generated as the output via the function. Script Body : This is a free text field to write the function in the specified script type. Example The details entered in the above form creates a function definition as follows: define function concatFN[JAVASCRIPT] return string { var str1 = data [ 0 ]; var str2 = data [ 1 ]; var str3 = data [ 2 ]; var responce = str1 + str2 + str3; return responce; };","title":"Function"},{"location":"develop/working-with-the-Design-View/#projection-query","text":"Icon Description !!! tip Before you add a projection query: You need to add and configure the following: The input stream with the events to be processed by the query. The output stream to which the events processed by the query are directed. This icon represents a query to project the events in an input stream to an output stream. This invoves selectng the attributes to be included in the output, renaming attributes, introducing constant values, and using mathematical and/or logical expressions. For more information, see Siddhi Query Guide - Query Projection . Form Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the projection query, click the settings icon on the projection query component you added to the grid, and update the following information. Query Meta Information : This section specifies the stream to be considered as the input stream with the events to which the query needs to be applied. The input stream connected to the query as the source is automatically displayed. Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events.| Example The details entered in the above form creates a query as follows: from TradeStream select symbol, avg (price) as averagePrice, sum (volume) as total insert all events into OutputStream; Source Streams Tables Triggers Windows Target Streams Tables Windows","title":"Projection Query"},{"location":"develop/working-with-the-Design-View/#filter-query","text":"Icon Description !!! tip Before you add a filter query: You need to add and configure the following: The input stream with the events to be processed by the query. The output stream to which the events processed by the query are directed. A filter query filters information in an input stream based on a given condition. For more information, see Siddhi Query Guide - Filters . Form Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the filter query, click the settings icon on the filter query component you added to the grid, and update the following information. By default, the Stream Handler check box is selected, and a stream handler of the filter type is available under it to indicate that the query is a filter. Expand this stream handler, and enter the condition based on which the information needs to be filtered. !!! info A Siddhi application can have multiple stream handlers. To add another stream handler, click the + Stream Handler . Multiple functions, filters and windows can be defined within the same form as stream handlers. Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events. Example The details entered in the above form creates a query with a filter as follows: from TradeStream[ sum (amount) 10000 ] select symbol, avg (price) as averagePrice, sum (amount) as total insert all events into OutputStream; Source Streams Tables Triggers Windows Target Streams Tables Windows","title":"Filter Query"},{"location":"develop/working-with-the-Design-View/#window-query","text":"Icon Description !!! tip Before you add a window query: You need to add and configure the following: The input stream with the events to be processed by the query. The output stream to which the events processed by the query are directed. Window queries include a window to select a subset of events to be processed based on a specific criterion. For more information, see Siddhi Query Guide - (Defined) Window . Form Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the window query, click the settings icon on the window query component you added to the grid, and update the following information. By default, the Stream Handler check box is selected, and a stream handler of the window type is available under it to indicate that the query is a filter. Expand this stream handler, and enter details to determine the window including the window type and the basis on which the subset of events considered by the window is determined (i.e., based on the window type selected). !!! info A Siddhi application can have multiple stream handlers. To add another stream handler, click the + Stream Handler . Multiple functions, filters and windows can be defined within the same form as stream handlers. Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events. Example The details entered in the above Query Configuration form creates a query with a window as follows: from TradeStream#window. time ( 1 month ) select symbol, avg (price) as averagePrice, sum (amount) as total insert all events into OutputStream; Source !!! info A window query can have only one source at a given time. Streams Tables Triggers Windows Target Streams Tables Windows","title":"Window Query"},{"location":"develop/working-with-the-Design-View/#join-query","text":"Icon Description A join query derives a combined result from two streams in real-time based on a specified condition. For more information, see Siddhi Query Guide - Join . Form Once you connect two Siddhi components to the join query as sources and another Siddhi component as the target, you can configure the join query. To configure the join query, click the settings icon on the join query component you added to the grid and update the following information. Query Meta Information : In this section, enter a unique name for the query and any annotations that you want to include in the query. The @dist annotation is supported by default to use the query in a fully distributed deployment if required (for more information, see Converting to a Distributed Streaming Application ). You can also add customized annotations. Input : Here, you can specify the input sources, the references, the join type, join condition, and stream handlers for the left source and right source of the join. For a detailed explanation of the join concept, see Siddhi Query Guide - Joins . Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events. Example A join query is configured as follows: The above configurations result in creating the following join query. from TempStream[temp 30.0 ]#window. time ( 1 min ) as T join RegulatorStream[isOn == false ]#window. length ( 1 ) as R on T.roomNo == R.roomNo select T.roomNo, R.deviceID, start as action insert into RegulatorActionStream; Source !!! info A join query must always be connected to two sources, and at least one of them must be a defined stream/trigger/window. Streams Tables Aggregations Windows Target !!! info A join query must always be connected to a single target. Streams Tables Windows","title":"Join Query"},{"location":"develop/working-with-the-Design-View/#pattern-query","text":"Icon Description !!! tip Before you add a pattern query: You need to add and configure the following: The input stream with the events to be processed by the query. The output stream to which the events processed by the query are directed. A pattern query detects patterns in events that arrive overtime. For more information, see Siddhi Query Guide - Patterns . Form Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the pattern query, click the settings icon on the pattern query component you added to the grid and update the following information. Query Meta Information : In this section, enter a unique name for the query and any annotations that you want to include in the query. The @dist annotation is supported by default to use the query in a fully distributed deployment if required (for more information, see Converting to a Distributed Streaming Application ). You can also add customized annotations. Input : This section defines the conditions based on which patterns are identified. This involves specifying a unique ID and the input stream considered for each condition. Multiple conditions can be added. Each condition is configured in a separate tab within this section. For more information about the Pattern concept, see Siddhi Query Guide - Patterns . Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events. Example The above configuration results in creating the following query. from every (e1 = MaterialSupplyStream) - not MaterialConsumptionStream[name == e1.name and amount == e1.amount] for 15 sec select e1.name, e1.amount insert into ProductionDelayAlertStream; Source Streams Tables Triggers Windows Target Streams Tables Windows","title":"Pattern Query"},{"location":"develop/working-with-the-Design-View/#sequence-query","text":"Icon Description !!! tip Before you add a sequence query: You need to add and configure the following: The input stream with the events to be processed by the query. The output stream to which the events processed by the query are directed. A sequence query detects sequences in event occurrences over time. For more information, see Siddhi Query Guide - Sequence . Form Once you connect the query to an input stream (source) and an output stream (target), you can configure it. To configure the sequence query, click the settings icon on the sequence query component you added to the grid and update the following information. Query Meta Information : In this section, enter a unique name for the query and any annotations that you want to include in the query. The @dist annotation is supported by default to use the query in a fully distributed deployment if required (for more information, see Converting to a Distributed Streaming Application ). You can also add customized annotations. Input : This section defines the conditions based on which sequences are identified. This involves specifying a unique ID and the input stream considered for each condition. Multiple conditions can be added. Each condition is configured in a separate tab within this section. For more information about the Sequence concept, see Siddhi Query Guide - Sequences . Projection : This section specifies the attributes to be included in the output. In the Select field, you can select All Attributes to select all the attributes of the events, or select User Defined Attributes to select specific attributes from the input stream. If you select User Defined Attributes , you can add attributes to be selected to be inserted into the output stream. Here, you can enter the names of specific attributes in the input stream, or enter expressions to convert input stream attribute values as required to generate output events. You can also specify the attribute(s) by which you want to group the output. Output : This section specifies the action to be performed on the output event. The fields to be configured in this section are as follows: Operation : This field specifies the operation to be performed on the generated output event (e.g., Insert to insert events to a selected stream/table/window). Into : This field specifies the stream/table/window in which the operation specified need to be performed. Event Type This field specifies whether the operation needs to be performed for all output events, only current events or for only expired events. Example The above configuration results in creating the following query. from every e1 = SweetProductionStream, e2 = SweetProductionStream[e1.amount amount and ( timestamp - e1. timestamp ) 10 * 60000 ] * , e3 = SweetProductionStream[ timestamp - e1. timestamp 10 * 60000 and e1.amount amount] select e1.name, e1.amount as initialAmount, e2.amount as finalAmount, e2. timestamp insert into DecreasingTrendAlertStream; Source Streams Tables Triggers Windows Target Streams Tables Windows","title":"Sequence Query"},{"location":"develop/working-with-the-Design-View/#partitions","text":"Icon Description !!! tip Before you add a partition: You need to add the stream to be partitioned. Partitions divide streams and queries into isolated groups in order to process them in parallel and in isolation. For more information, see Siddhi Query Guide - Partition . Form Once the stream to be partitioned is connected as a source to the partition, you can configure the partition. In order to do so, move the cursor over the partition and click the settings icon on the partition component. This opens the Partition Configuration form. In this form, you can enter expressions to convert the attributes of the stream that is selected to be partitioned. Example The above configuration creates the following partition query. partition with ( roomNo = 1030 as serverRoom or roomNo 1030 and roomNo = 330 as officeRoom or roomNo 330 as lobby of TempStream) begin from TempStream#window. time ( 10 min ) select roomNo, deviceID, avg (temp) as avgTemp insert into AreaTempStream end ; Source Streams Target N/A","title":"Partitions"},{"location":"develop/working-with-the-Design-View/#connecting-siddhi-components","text":"In order to define how the Siddhi components in a Siddhi application interact with each other to process events, you need to define connections between Siddhi components. A connection is defined by drawing an arrow from one component to another by dragging the cursor as demonstrated below.","title":"Connecting Siddhi components"},{"location":"develop/working-with-the-Design-View/#saving-running-and-debugging-siddhi-applications","text":"To save a Siddhi application that you created in the design view, you need to switch to the source view. You also need to switch to the source view to run or debug a Siddhi application. For more information, see the following sections: Stream Processor Studio Overview Debugging a Siddhi Application","title":"Saving, running and debugging Siddhi applications"},{"location":"examples/analyzing-data-and-deriving-insights/","text":"","title":"Analyzing Data and Deriving Insights"},{"location":"examples/exposing-processed-data-as-api/","text":"","title":"Exposing Processed Data as API"},{"location":"examples/integrating-databases-to-data-flows/","text":"","title":"Integrating Databases to Data Flows"},{"location":"examples/performing-real-time-etl-with-files/","text":"","title":"Performing Real-time ETL with Files"},{"location":"examples/performing-real-time-etl-with-mysql/","text":"","title":"Performing Real-time ETL with MySQL"},{"location":"examples/reliable-data-integration/","text":"","title":"Performing Reliable Data Integration (Error Streams)"},{"location":"examples/summarizing-and-aggregating-data/","text":"","title":"Summarizing and Aggregating Data in Real Time"},{"location":"examples/transforming-json-messages/","text":"","title":"Transforming JSON Messages"},{"location":"examples/transforming-xml-messages/","text":"","title":"Transforming XML Messages"},{"location":"examples/triggering-integrations-via-micro-integrator/","text":"","title":"Triggering Integrations via Micro Integrator"},{"location":"examples/working-with-kafka/","text":"","title":"Working with Kafka"},{"location":"guides/accessing-and-Manipulating-Data-in-Multiple-Tables/","text":"Accessing and Manipulating Data in Multiple Tables WSO2 SP allows you to perform CUD operations (i.e., inserting updating and deleting data) and retrieval queries for multiple normalized tables within a single data store. This is supported via the siddhi-store-rdbms extension . Performing CUD operations for multiple tables In order to perform CUD operations, the system parameter named perform.CUD.operations needs to be set to true in deployment.yaml file . The syntax for a Siddhi query to perform a CUD operation in multiple tables is as follows: from TriggerStream#rdbms:cud( STRING datasource.name, STRING query) select numRecords insert into OutputStream; e.g., If you need to change the details of a customer in customer details table connected to a datasource named, SAMPLE_DB a Siddhi query can be written as follows: from Trigger Stream#rdbms:cud('SAMPLE_DB', 'UPDATE Customers ON CUSTOMERS SET ContactName='Alfred Schmidt', City='Frankfurt' WHERE CustomerID=1;') select numRecords insert into OutputStream Retrieving data from multiple tables In order to retrieve information from multiple tables in a data store, the syntax is as follows: from TriggerStream#rdbms:query( STRING dataource.name, STRING query, STRING stream definition) select attributes insert into OutputStream e.g., If you need to find matching records in both customer and orders table based on orderId and customerId in the SAMPLE_DB database, a Siddhi query can be written as follows: from TriggerStream#rdbms:query('SAMPLE_DB', 'SELECT Orders.OrderID, Customers.CustomerName, Orders.OrderDate FROM Orders INNER JOIN Customers ON Orders.CustomerID=Customers.CustomerID', 'orderId string, customerName string, orderDate string') select orderId, customerName, orderDate insert into OutputStream;","title":"Accessing and Manipulating Data in Multiple Tables"},{"location":"guides/accessing-and-Manipulating-Data-in-Multiple-Tables/#accessing-and-manipulating-data-in-multiple-tables","text":"WSO2 SP allows you to perform CUD operations (i.e., inserting updating and deleting data) and retrieval queries for multiple normalized tables within a single data store. This is supported via the siddhi-store-rdbms extension .","title":"Accessing and Manipulating Data in Multiple Tables"},{"location":"guides/accessing-and-Manipulating-Data-in-Multiple-Tables/#performing-cud-operations-for-multiple-tables","text":"In order to perform CUD operations, the system parameter named perform.CUD.operations needs to be set to true in deployment.yaml file . The syntax for a Siddhi query to perform a CUD operation in multiple tables is as follows: from TriggerStream#rdbms:cud( STRING datasource.name, STRING query) select numRecords insert into OutputStream; e.g., If you need to change the details of a customer in customer details table connected to a datasource named, SAMPLE_DB a Siddhi query can be written as follows: from Trigger Stream#rdbms:cud('SAMPLE_DB', 'UPDATE Customers ON CUSTOMERS SET ContactName='Alfred Schmidt', City='Frankfurt' WHERE CustomerID=1;') select numRecords insert into OutputStream","title":"Performing CUD operations for multiple tables"},{"location":"guides/accessing-and-Manipulating-Data-in-Multiple-Tables/#retrieving-data-from-multiple-tables","text":"In order to retrieve information from multiple tables in a data store, the syntax is as follows: from TriggerStream#rdbms:query( STRING dataource.name, STRING query, STRING stream definition) select attributes insert into OutputStream e.g., If you need to find matching records in both customer and orders table based on orderId and customerId in the SAMPLE_DB database, a Siddhi query can be written as follows: from TriggerStream#rdbms:query('SAMPLE_DB', 'SELECT Orders.OrderID, Customers.CustomerName, Orders.OrderDate FROM Orders INNER JOIN Customers ON Orders.CustomerID=Customers.CustomerID', 'orderId string, customerName string, orderDate string') select orderId, customerName, orderDate insert into OutputStream;","title":"Retrieving data from multiple tables"},{"location":"guides/cleansing-data/","text":"","title":"Cleansing Data"},{"location":"guides/collecting-Events/","text":"Collecting Events The first step in stream processing is to collect the data that needs to be analyzed. Collection of data is done through Siddhi source which can be defined via @source() annotation on a steam. To collect data to be processed a stream should have been defined in the Siddhi Application along with the @source() annotation and deployed in WSO2 SP. Here a single SiddhiApp can contain multiple streams and each of those steams can also contain multiple @source() annotations. Example Input Source definition as bellow. @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='json', @attributes( name='$.name', amount='$.quantity'))) define stream SweetProductionStream (name string, amount double); The source defines the following : Input source type: via type = 'http' Source type configurations (This is optional. I n this example h ttp source type configurations are defined via receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false') Input message format: with @map() sub-annotation. Custom attribute mapping of the input message format (optional): with optional @attributes( name='$.name', amount='$.quantity') sub-annotation of @map. Info A source could also be defined externally, and referred to from several siddhi applications as described below,Multiple sources can be defined in the SP HOME /conf/ PROFILE /deployment.yaml file. A \\ PROFILE> could refer to dashboard, editor, manager or worker. The following is a sample configuration of a source. siddhi: refs: - ref: name: 'source1' type: ' store.type ' properties: property1 : value1 property2 : value2 You can refer to a source configured in the ` lt;SP HOME gt;/conf/ lt;PROFILE gt;/deployment.yaml ` file from a Siddhi application as shown in the example below. @Source(ref='source1', basic.auth.enabled='false', @map(type='json', @attributes( name='$.name', amount='$.quantity'))) define stream SweetProductionStream (name string, amount double); For detailed instructions to configure a source, see [Siddhi Guide - Source](https://siddhi-io.github.io/siddhi/documentation/siddhi-4.x/query-guide-4.x/#source) . Source types WSO2 SP supports following source types out of the box, to receive events via corresponding transports. Click on the required source type for instructions to configure a source to receive events via them. HTTP Kafka TCP In-memory WSO2 Event Email JMS File RabbitMQ MQTT Event format WSO2 Siddhi allows events to be received in multiple formats. The following formats are currently supported. Once an event is received in a specific format, it is internally converted to a Siddhi event so that it can be processed by applying the WSO2 Siddhi logic. Click on the required format for detailed information on how a source can be configured to receive events in that format. WSO2Event XML Text JSON Binary Key Value","title":"Collecting Events"},{"location":"guides/collecting-Events/#collecting-events","text":"The first step in stream processing is to collect the data that needs to be analyzed. Collection of data is done through Siddhi source which can be defined via @source() annotation on a steam. To collect data to be processed a stream should have been defined in the Siddhi Application along with the @source() annotation and deployed in WSO2 SP. Here a single SiddhiApp can contain multiple streams and each of those steams can also contain multiple @source() annotations. Example Input Source definition as bellow. @Source(type = 'http', receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false', @map(type='json', @attributes( name='$.name', amount='$.quantity'))) define stream SweetProductionStream (name string, amount double); The source defines the following : Input source type: via type = 'http' Source type configurations (This is optional. I n this example h ttp source type configurations are defined via receiver.url='http://localhost:8006/productionStream', basic.auth.enabled='false') Input message format: with @map() sub-annotation. Custom attribute mapping of the input message format (optional): with optional @attributes( name='$.name', amount='$.quantity') sub-annotation of @map. Info A source could also be defined externally, and referred to from several siddhi applications as described below,Multiple sources can be defined in the SP HOME /conf/ PROFILE /deployment.yaml file. A \\ PROFILE> could refer to dashboard, editor, manager or worker. The following is a sample configuration of a source. siddhi: refs: - ref: name: 'source1' type: ' store.type ' properties: property1 : value1 property2 : value2 You can refer to a source configured in the ` lt;SP HOME gt;/conf/ lt;PROFILE gt;/deployment.yaml ` file from a Siddhi application as shown in the example below. @Source(ref='source1', basic.auth.enabled='false', @map(type='json', @attributes( name='$.name', amount='$.quantity'))) define stream SweetProductionStream (name string, amount double); For detailed instructions to configure a source, see [Siddhi Guide - Source](https://siddhi-io.github.io/siddhi/documentation/siddhi-4.x/query-guide-4.x/#source) .","title":"Collecting Events"},{"location":"guides/collecting-Events/#source-types","text":"WSO2 SP supports following source types out of the box, to receive events via corresponding transports. Click on the required source type for instructions to configure a source to receive events via them. HTTP Kafka TCP In-memory WSO2 Event Email JMS File RabbitMQ MQTT","title":"Source types"},{"location":"guides/collecting-Events/#event-format","text":"WSO2 Siddhi allows events to be received in multiple formats. The following formats are currently supported. Once an event is received in a specific format, it is internally converted to a Siddhi event so that it can be processed by applying the WSO2 Siddhi logic. Click on the required format for detailed information on how a source can be configured to receive events in that format. WSO2Event XML Text JSON Binary Key Value","title":"Event format"},{"location":"guides/complex-Event-Processing/","text":"Complex Event Processing Gartner\u2019s IT Glossary defines CEP as follows: Info \"CEP is a kind of computing in which incoming data about events is distilled into more useful, higher level \u201ccomplex\u201d event data that provides insight into what is happening.\" \" CEP is event-driven because the computation is triggered by the receipt of event data. CEP is used for highly demanding, continuous-intelligence applications that enhance situation awareness and support real-time decisions.\" WSO2 SP allows you to detect patterns and trends for decision making via Patterns and Sequences supported for Siddhi. Patterns Siddhi patterns allow you to detect patterns in the events that arrive over time. This can correlate events within a single stream or between multiple streams. A pattern can be a counting pattern that allows you to match multiple events received for the same matching condition, or a logical pattern that match events that arrive in temporal order and correlate them with logical relationships. For more information, see Siddhi Query Guide - Patterns . Sequences This allows you to carry out analysis based on the sequential order in which events matching certain conditions arrive. The difference between sequences and patterns is, in Siddhi sequence, events arriving consecutively are compared based on given conditions. For patterns, events do not have to arrive in consecutive manner. For more information, see Siddhi Query Guide - Sequence .","title":"Complex Event Processing"},{"location":"guides/complex-Event-Processing/#complex-event-processing","text":"Gartner\u2019s IT Glossary defines CEP as follows: Info \"CEP is a kind of computing in which incoming data about events is distilled into more useful, higher level \u201ccomplex\u201d event data that provides insight into what is happening.\" \" CEP is event-driven because the computation is triggered by the receipt of event data. CEP is used for highly demanding, continuous-intelligence applications that enhance situation awareness and support real-time decisions.\" WSO2 SP allows you to detect patterns and trends for decision making via Patterns and Sequences supported for Siddhi.","title":"Complex Event Processing"},{"location":"guides/complex-Event-Processing/#patterns","text":"Siddhi patterns allow you to detect patterns in the events that arrive over time. This can correlate events within a single stream or between multiple streams. A pattern can be a counting pattern that allows you to match multiple events received for the same matching condition, or a logical pattern that match events that arrive in temporal order and correlate them with logical relationships. For more information, see Siddhi Query Guide - Patterns .","title":"Patterns"},{"location":"guides/complex-Event-Processing/#sequences","text":"This allows you to carry out analysis based on the sequential order in which events matching certain conditions arrive. The difference between sequences and patterns is, in Siddhi sequence, events arriving consecutively are compared based on given conditions. For patterns, events do not have to arrive in consecutive manner. For more information, see Siddhi Query Guide - Sequence .","title":"Sequences"},{"location":"guides/configuring-Database-Queries/","text":"Configuring Database Queries Database queries used in the following features and can configured for new database or override the default queries for the new database version. The relevant configurations should be added to the SP_Home /conf/ Runtime /deployment.yaml file. Business Rules Status Dashboard Dashboard RDBMS Data Provider The following are sample database query configurations that override the MySQL default version queries in the RDBMS Data Provider Feature by MySQL new version 5.7.0 and adding new queries for new database type MariaDB with version 10.0.20 respectiveley: wso2.rdbms.data.provider: queries: - mappings: record_delete: \"DELETE FROM {{TABLE_NAME}} ORDER BY {{INCREMENTAL_COLUMN}} ASC LIMIT {{LIMIT_VALUE}}\" total_record_count: \"SELECT COUNT(*) FROM {{TABLE_NAME}}\" record_limit: \"{{CUSTOM_QUERY}} ORDER BY {{INCREMENTAL_COLUMN}} DESC LIMIT {{LIMIT_VALUE}}\" record_greater_than: \"SELECT * FROM ({{CUSTOM_QUERY}} ORDER BY {{INCREMENTAL_COLUMN}} DESC ) AS A_TABLE WHERE {{INCREMENTAL_COLUMN}} {{LAST_RECORD_VALUE}}\" type: MySQL version: 5.7.0 - mappings: record_delete: \"DELETE FROM {{TABLE_NAME}} ORDER BY {{INCREMENTAL_COLUMN}} ASC LIMIT {{LIMIT_VALUE}}\" total_record_count: \"SELECT COUNT(*) FROM {{TABLE_NAME}}\" record_limit: \"{{CUSTOM_QUERY}} ORDER BY {{INCREMENTAL_COLUMN}} DESC LIMIT {{LIMIT_VALUE}}\" record_greater_than: \"SELECT * FROM ({{CUSTOM_QUERY}} ORDER BY {{INCREMENTAL_COLUMN}} DESC ) AS A_TABLE WHERE {{INCREMENTAL_COLUMN}} {{LAST_RECORD_VALUE}}\" type: MariaDB version: 10.0.20 Business Rules To add or override database queries for Business Rules you need to configure the \\ SP_Home>/conf/dashboard/deployment.yaml file as indicated in the above example: You can find the avaialble default queries in : https://github.com/wso2/carbon-analytics/blob/v2.0.250/components/org.wso2.carbon.business.rules.core/src/main/resources/queries.yaml Copy the queries with the above structure under the ' wso2.business.rules.manager ' namespace. Status Dashboard To add or override database queries for Status Dashbord you need to configure the \\ SP_Home>/conf/dashboard/deployment.yaml file as indicated in the above example: You can find the avaialble default queries in : https://github.com/wso2/carbon-analytics/blob/v2.0.250/components/org.wso2.carbon.status.dashboard.core/src/main/resources/queries.yaml Copy the queries with the above structure under the ' wso2.status.dashboard ' namespace. Dashboard To add or override database queries for Dashbord you need to configure the \\ SP_Home>/conf/dashboard/deployment.yaml file as indicated in the above example: You can find the avaialble default queries in : https://github.com/wso2/carbon-dashboards/blob/v4.0.8/components/dashboards/org.wso2.carbon.dashboards.core/src/main/resources/sql-queries.yaml Copy the queries with the above structure under the ' wso2.dashboard ' namespace. RDBMS Data Provider To add or override database queries for RDBMS Data Provider you need to configure the \\ SP_Home>/conf/dashboard/deployment.yaml file as indicated in the above example: You can find the avaialble default queries in : https://github.com/wso2/carbon-analytics/blob/v2.0.250/components/org.wso2.carbon.data.provider/src/main/resources/queries.yaml Copy the queries with the above structure under the ' wso2.rdbms.data.provider' namespace. Info Note : Please note that in order to apply the above changes, you need to restart the server runtime.","title":"Configuring Database Queries"},{"location":"guides/configuring-Database-Queries/#configuring-database-queries","text":"Database queries used in the following features and can configured for new database or override the default queries for the new database version. The relevant configurations should be added to the SP_Home /conf/ Runtime /deployment.yaml file. Business Rules Status Dashboard Dashboard RDBMS Data Provider The following are sample database query configurations that override the MySQL default version queries in the RDBMS Data Provider Feature by MySQL new version 5.7.0 and adding new queries for new database type MariaDB with version 10.0.20 respectiveley: wso2.rdbms.data.provider: queries: - mappings: record_delete: \"DELETE FROM {{TABLE_NAME}} ORDER BY {{INCREMENTAL_COLUMN}} ASC LIMIT {{LIMIT_VALUE}}\" total_record_count: \"SELECT COUNT(*) FROM {{TABLE_NAME}}\" record_limit: \"{{CUSTOM_QUERY}} ORDER BY {{INCREMENTAL_COLUMN}} DESC LIMIT {{LIMIT_VALUE}}\" record_greater_than: \"SELECT * FROM ({{CUSTOM_QUERY}} ORDER BY {{INCREMENTAL_COLUMN}} DESC ) AS A_TABLE WHERE {{INCREMENTAL_COLUMN}} {{LAST_RECORD_VALUE}}\" type: MySQL version: 5.7.0 - mappings: record_delete: \"DELETE FROM {{TABLE_NAME}} ORDER BY {{INCREMENTAL_COLUMN}} ASC LIMIT {{LIMIT_VALUE}}\" total_record_count: \"SELECT COUNT(*) FROM {{TABLE_NAME}}\" record_limit: \"{{CUSTOM_QUERY}} ORDER BY {{INCREMENTAL_COLUMN}} DESC LIMIT {{LIMIT_VALUE}}\" record_greater_than: \"SELECT * FROM ({{CUSTOM_QUERY}} ORDER BY {{INCREMENTAL_COLUMN}} DESC ) AS A_TABLE WHERE {{INCREMENTAL_COLUMN}} {{LAST_RECORD_VALUE}}\" type: MariaDB version: 10.0.20","title":"Configuring Database Queries"},{"location":"guides/configuring-Database-Queries/#business-rules","text":"To add or override database queries for Business Rules you need to configure the \\ SP_Home>/conf/dashboard/deployment.yaml file as indicated in the above example: You can find the avaialble default queries in : https://github.com/wso2/carbon-analytics/blob/v2.0.250/components/org.wso2.carbon.business.rules.core/src/main/resources/queries.yaml Copy the queries with the above structure under the ' wso2.business.rules.manager ' namespace.","title":"Business Rules"},{"location":"guides/configuring-Database-Queries/#status-dashboard","text":"To add or override database queries for Status Dashbord you need to configure the \\ SP_Home>/conf/dashboard/deployment.yaml file as indicated in the above example: You can find the avaialble default queries in : https://github.com/wso2/carbon-analytics/blob/v2.0.250/components/org.wso2.carbon.status.dashboard.core/src/main/resources/queries.yaml Copy the queries with the above structure under the ' wso2.status.dashboard ' namespace.","title":"Status Dashboard"},{"location":"guides/configuring-Database-Queries/#dashboard","text":"To add or override database queries for Dashbord you need to configure the \\ SP_Home>/conf/dashboard/deployment.yaml file as indicated in the above example: You can find the avaialble default queries in : https://github.com/wso2/carbon-dashboards/blob/v4.0.8/components/dashboards/org.wso2.carbon.dashboards.core/src/main/resources/sql-queries.yaml Copy the queries with the above structure under the ' wso2.dashboard ' namespace.","title":"Dashboard"},{"location":"guides/configuring-Database-Queries/#rdbms-data-provider","text":"To add or override database queries for RDBMS Data Provider you need to configure the \\ SP_Home>/conf/dashboard/deployment.yaml file as indicated in the above example: You can find the avaialble default queries in : https://github.com/wso2/carbon-analytics/blob/v2.0.250/components/org.wso2.carbon.data.provider/src/main/resources/queries.yaml Copy the queries with the above structure under the ' wso2.rdbms.data.provider' namespace. Info Note : Please note that in order to apply the above changes, you need to restart the server runtime.","title":"RDBMS Data Provider"},{"location":"guides/consuming_messages/","text":"Consuming Messages","title":"Consuming messages"},{"location":"guides/consuming_messages/#consuming-messages","text":"","title":"Consuming Messages"},{"location":"guides/enriching-data/","text":"","title":"Enriching Data"},{"location":"guides/fault-Handling/","text":"Fault Handling Siddhi allows you to manage any faults that may occur when handling streaming data in a graceful manner. This section explains the different ways in which the faults can be handled gracefully. Handling runtime errors To specify how errors that occur at runtime, you need to add an @OnError annotation to a stream definition as shown below. @OnError(action='on_error_action') define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The on_error_action parameter specifies the action to be executed during failure scenarios. The possible action types are as follows: LOG : This logs the event with an error, and then drops the event. If you do not specify the fault handling actionvia the @OnError annotation, LOG is considered the default action. STREAM : This automatically creates a fault stream for the base stream. The definition of the fault stream includes all the attributes of the base stream as well as an additional attribute named _error . The events are inserted into the fault stream during a failure. The error identified is captured as the value for the _error attribute. e.g., The following is a Siddhi application that includes the @OnError annotation to handle failures during runtime. @OnError(name='STREAM') define stream StreamA (symbol string, volume long); from StreamA[custom:fault() volume] insert into StreamB; from !StreamA#log(\"Error Occured\") select symbol, volume long, _error insert into tempStream; Here, if an error occurs for the base stream named StreamA , a stream named !StreamA is automatically created. The base stream has two attributes named symbol and volume. Therefore, !StreamA has the same two attributes, and in addition, another attribute named _error. The Siddhi query uses the custom:fault() extension generates the error detected bsed on the specified condition (i.e., if the volume is less than a specified amount). If no error is detected, the output is inserted into StreamB stream. However, if an error is detected, it is logged with the Error Occured text. The output is inserted into a stream named tempStream , and the error details are presented via the _error stream attribute (which is automatically included in the !StreamA i fault stream and then inserted into the TempStream which is the inferred output stream).. Handling errors that occur when publishing the output To specify the error handling methods for errors that occur at the time the output is published, you can include the on.error parameter in the sink configuration as shown below. @sink(type='sink_type', on.error='on.error.action' ) define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The action types that can be specified via the on.error parameter when configuring a sink are as follows. If this parameter is not included in the sink configuration, LOG is the action type by default. LOG : Logs the event with the error, and then drops the event. WAIT : The thread waits in the back-off and re-trying state, and reconnects once the connection is re-established. STREAM : The corresponding fault stream is populated with the failed event and the error that occured while publishing.","title":"Fault Handling"},{"location":"guides/fault-Handling/#fault-handling","text":"Siddhi allows you to manage any faults that may occur when handling streaming data in a graceful manner. This section explains the different ways in which the faults can be handled gracefully.","title":"Fault Handling"},{"location":"guides/fault-Handling/#handling-runtime-errors","text":"To specify how errors that occur at runtime, you need to add an @OnError annotation to a stream definition as shown below. @OnError(action='on_error_action') define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The on_error_action parameter specifies the action to be executed during failure scenarios. The possible action types are as follows: LOG : This logs the event with an error, and then drops the event. If you do not specify the fault handling actionvia the @OnError annotation, LOG is considered the default action. STREAM : This automatically creates a fault stream for the base stream. The definition of the fault stream includes all the attributes of the base stream as well as an additional attribute named _error . The events are inserted into the fault stream during a failure. The error identified is captured as the value for the _error attribute. e.g., The following is a Siddhi application that includes the @OnError annotation to handle failures during runtime. @OnError(name='STREAM') define stream StreamA (symbol string, volume long); from StreamA[custom:fault() volume] insert into StreamB; from !StreamA#log(\"Error Occured\") select symbol, volume long, _error insert into tempStream; Here, if an error occurs for the base stream named StreamA , a stream named !StreamA is automatically created. The base stream has two attributes named symbol and volume. Therefore, !StreamA has the same two attributes, and in addition, another attribute named _error. The Siddhi query uses the custom:fault() extension generates the error detected bsed on the specified condition (i.e., if the volume is less than a specified amount). If no error is detected, the output is inserted into StreamB stream. However, if an error is detected, it is logged with the Error Occured text. The output is inserted into a stream named tempStream , and the error details are presented via the _error stream attribute (which is automatically included in the !StreamA i fault stream and then inserted into the TempStream which is the inferred output stream)..","title":"Handling runtime errors"},{"location":"guides/fault-Handling/#handling-errors-that-occur-when-publishing-the-output","text":"To specify the error handling methods for errors that occur at the time the output is published, you can include the on.error parameter in the sink configuration as shown below. @sink(type='sink_type', on.error='on.error.action' ) define stream stream name ( attribute name attribute type , attribute name attribute type , ... ); The action types that can be specified via the on.error parameter when configuring a sink are as follows. If this parameter is not included in the sink configuration, LOG is the action type by default. LOG : Logs the event with the error, and then drops the event. WAIT : The thread waits in the back-off and re-trying state, and reconnects once the connection is re-established. STREAM : The corresponding fault stream is populated with the failed event and the error that occured while publishing.","title":"Handling errors that occur when publishing the output"},{"location":"guides/incremental-Analysis/","text":"Incremental Analysis Incremental aggregation allows you to obtain aggregates in an incremental manner for a specified set of time periods. This not only allows you to calculate aggregations with varied time granularity, but also allows you to access them in an interactive manner for reports, dashboards, and for further processing. Its schema is defined via the aggregation definition. Configuring aggregation queries Incremental aggregation in single-node deployments and HA deployments Scaling through distributed aggregations Configuring aggregation queries This section explains how to calculate aggregates in an incremental manner for different sets of time periods, store the calculated values in a data store and then retrieve that information. To demonstrate this, consider a scenario where a business that sells multiple brands stores its sales data in a physicaldatabase for the purpose of retrieving them later to perform sales analysis. Each sales transaction is received with the following details: symbol : The symbol that represents the brand of the items sold. price : the price at which each item was sold. amount : The number of items sold. The Sales Analyst needs to retrieve the total number of items sold of each brand per month, per week, per day etc., and then retrieve these totals for specific time durations to prepare sales analysis reports. To address the above use case, Siddhi queries need to be designed in two steps as follows: Step 1: Calculate and persist time-based aggregate values Step 2: Retrieve calculated and persisted aggregate values Step 1: Calculate and persist time-based aggregate values Tip Before you begin: Before creating queries to calculate and persist aggregate values, the following prerequisites need to be completed: When you are defining a physical store to store aggregate values, you need to first download, install and set up the required database type. For more information about setting up the database, see Defining Tables for Physical Stores . If the aggregation query included in the Siddhi application is only for read-only purposes, disable data purging. To write the required Siddhi queries to calculate and persist aggregate values required for the scenario outlined above, follow the steps below: To capture the input events based on which the aggregations are calculated, define an input stream as follows. The stream definition includes attributes named symbol, price and amount to capture the details described above. define stream TradeStream (symbol string, price double, quantity long, timestamp long); The stream definition includes attributes named symbol , price and amount to capture the details described above. In addition, it has an attribute named timestamp to capture the time at which the sales transaction occurs. The aggregations are executed based on this time. !!! info This attribute's value could either be a long value (reflecting the Unix timestamp in milliseconds), or a string value adhering to one of the following formats. - **` lt;yyyy gt;- lt;MM gt;- lt;dd gt; lt;HH gt;: lt;mm gt;: lt;ss gt; lt;Z gt; `** : This format can be used if the timezone needs to be specified explicitly. Here the ISO 8601 UTC offset must be provided for \\ lt;Z\\ gt; . e.g., +05:30 reflects the India Time Zone. If time is not in GMT, this value must be provided.) - **` lt;yyyy gt;- lt;MM gt;- lt;dd gt; lt;HH gt;: lt;mm gt;: lt;ss gt; `** : This format can be used if the timezone is in GMT. To persist the aggregates that are calculated via your Siddhi application, include a store definition as follows, if not the data is stored in-memory and lost when siddhi app is stopped. define stream TradeStream (symbol string, price double, quantity long, timestamp long); @store(type='rdbms', jdbc.url=\"jdbc: mysql://localhost:3306/TestDB \", username=\"root\", password=\"root\" , jdbc.driver.name =\"com.mysql.jdbc.Driver\") Define an aggregation as follows. You can name it TradeAggregation . !!! info When you save aggregate values in a store, the system uses the aggregation name you define here as part of the database table name. Table name is ` lt;Aggregation_Name gt;_ lt;Granularity gt; ` . Some database types have length limitations for table names (e.g., for Oracle, it is 30 characters). Therefore, you need to check whether the database type you have used for the store has such table name length limitation, and make sure that the aggregation name does not exceed that maximum length. define aggregation TradeAggregation To calculate aggregations, include a query as follows: To get input events from the TradeStream stream that you previously defined, add a from clause as follows. define aggregation TradeAggregation from TradeStream To select attributes to be included in the output event, add a select clause as follows. define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(quantity) as total Based on this, each output event has the following attributes. Attribute Description symbol The symbol representing the product sold is included in each output event. avgPrice The average price at which the brand is sold. This is derived by applying the avg() function to the price attribute with each input event. total The total number of items sold for each product. This is derived by applying the sum() function to the quantity attribute with each input event. The average price and the total number of items are the aggregates calculated in this scenario. To group the output by the symbol, add a group by clause as follows. define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(quantity) as total group by symbol The group by clause is optional in incremental aggregations. If it is not provided, all the events are aggregated together for each duration. The timestamp included in each input event allows you to calculate aggregates for the range of time granularities seconds-years. Therefore, to calculate aggregates for each time granularity within this range, add the aggregate by clause to this aggregate query as follows. define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(quantity) as total group by symbol aggregate by timestamp every sec ... year; This results in the average price and the total number of items sold being calculated per second, minute, hour, day, month and year. These time-based calculations are saved in the MySQL store you defined. !!! warning If you exclude one or more time granularities from this range, you cannot later retrieve aggregates for them. e.g., You cannot aggregate by timestamp every ` sec ... day ` in the aggregate definition, and then attempt to retrieve per ` months ` . This is because monthly values are not calculated and persisted. !!! info You can also provide comma-separated values such as ` every day, month, ` ... The complete Siddhi application with all the definitions and queries added looks as follows. @App:name('TradeApp') define stream TradeStream (symbol string, price double, quantity long, timestamp long); @store(type='rdbms', jdbc.url=\"jdbc:mysql://localhost:3306/TestDB\", username=\"root\", password=\"root\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(quantity) as total group by symbol aggregate by timestamp every sec ... year; The following is a list of optional annotations supported for incremental aggregation. @store(type= store type , ...) The aggregated results are stored in the database defined via this annotation in tables corresponding to each aggregate duration. For more information on how to define a store, see Defining Tables for Physical Stores . !!! info When these database entries are created, the primary key is defined internally. Therefore, you must not include an ` @PrimaryKey ` annotation within this ` @store ` annotation in the aggregation definition. If a store is not defined via this annotation, the aggregated results are stored in in-memory tables by default. Adding a specific store definition is useful in a production environment. This is because the aggregations stored in in-memory tables can be lost if a system failure occurs. The names of the tables are created in the AGGREGATION_NAME _ DURATION format. In this scenario, the following tables are created: TradeAggregation_SECONDS TradeAggregation_MINUTES TradeAggregation_HOURS TradeAggregation_DAYS TradeAggregation_MONTHS TradeAggregation_YEARS The primary keys of these tables are determined as follows: {width=\"500\"} AGG_TIMESTAMP and AGG_EVENT_TIMESTAMP are internally calculated values that reflect the time bucket of an aggregation for a particular duration. e.g. for the day aggregations, AGG_TIMESTAMP = 1515110400000 reflects that it is the aggregation for the 5 th of January 2018 ( 1515110400000 is the Unix time for 2018-01-05 00:00:00 ). All aggregations are based on GMT timezone. The other values stored in the table would be aggregations and other function calculations done in the aggregate definition. If any such function calculation is not an aggregation, the output value corresponds with the function calculation for the latest event of that time bucket. e.g., If a multiplication is carried out, the multiplication value corresponds with the latest event's multiplication as per the duration. Certain aggregations are internally stored as a collection of other aggregations. e.g., the average aggregation is internally calculated as a function of sum and count. Hence, the table only reflects a sum and a count. The actual average is returned when the user retrieves the aggregate values as described in Step 2: Retrieve calculated and persisted aggregate values . The other values stored in the database table are aggregations and other function calculations carried out via the aggregation definition. In the scenario described in this section, a timestamp is assigned to each input event via the timestamp attribute, and symbol is the group by key. Therefore, the TestDB store defined via the @store annotation uses the AGG_EVENT_TIMESTAMP and symbol attributes as primary keys. Each record . in this store must have a unique combination of values for these two attributes. @purge This specifies whether automatic data purging is enabled or not. If automatic data purging is enabled, you need to specify the time interval at which the data purging should be carried out. In addition, you need to specify the time period for which the data should be retained based on the granularity by including the @retentionPeriod annotation (described below). If this annotation is not included in an incremental aggregation query, the data purging is carried out every 15 minutes based on the default retention periods mentioned in the description of the @retentionPeriod annotation. !!! tip If you want to disable automatic data purging, you can use this annotation as follows: ` @purge(enable= false ) ` You should disable data purging if the aggregation query is included in the Siddhi application for read-only purposes. @retentionPeriod This specifies the time period for which data needs to be retained before it is purged, based on the granularity. The retention period defined for each granularity should be greater than, or equal to the minimum retention period as given below: second : 120 seconds minute : 120 minutes hour : 25 hours day : 32 days month : 13 months year : none If the retention period is not specified, the default retention period for each granularity is applied as given below: second : 120 seconds minute : 24 hours hour : 30 days day : 1 year month : All year : All Step 2: Retrieve calculated and persisted aggregate values This step involves retrieving the aggregate values that you calculated and persisted in Step 1. To do this, let's add the Siddhi definitions and queries required for retrieval to the TradeApp Siddhi application that you have already started creating for this scenario. Info Retrieval logic for the same aggregation can be defined in different Siddhi app. However, only one aggregation should carry out the processing (i.e. the aggregation input stream should only feed events to one aggregation definition). For this scenario, let's assume that the Sales Analyst needs to retrieve the sales totals for the time duration between 15 th February 2014 and 16 th March 2014. To retrieve aggregations, you need to make retrieval requests. To capture these requests as events, let's define a stream as follows. @App:name('TradeApp') define stream TradeStream (symbol string, price double, quantity long, timestamp long); define stream TradeSummaryRetrievalStream (symbol string); @store(type='rdbms', jdbc.url=\"jdbc:mysql://localhost:3306/TestDB\", username=\"root\", password=\"root\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(quantity) as total group by symbol aggregate by timestamp every sec ... year; To process the events captured via the TradeSummaryRetrievalStream stream you defined, add a new query as follows. from TradeSummaryRetrievalStream as b join TradeAggregation as a on a.symbol == b.symbol within 2014-02-15 00:00:00 +05:30 , 2014-03-16 00:00:00 +05:30 per days select a.symbol, a.total, a.avgPrice insert into TradeSummaryStream; This query matches events in the TradeSummaryRetrievalStream stream and the TradeAggregation aggregation that was defined in step 1 based on the value for the symbol attribute, and performs a join for each matching pair. Based on that join, it produces an output event(s) with the symbo l, total and avgPrice attributes for the day granularity within the time range 2014-02-15 00:00:00 to 2014-03-16 00:00:00 . The time zone is represented by +05:30 . Note the following about the query given above: The on condition is optional for retrieval. You can provide the within duration in two ways as follows: within start_time , end_time : The start_time and end_time can be STRING or LONG values. If it is a string value, the format needs to be yyyy - MM - dd HH : mm : ss Z ( Z represents the timezone. It can be omitted if the time is in GMT). You can provide long values if you need to specify the times as Unix timestamps (in milliseconds). In the above query, you are specifying the time duration via this method in the STRING format. within within_duration : This method allows you to enter the time duration only as a STRING value. The format can be one of the following: yyyy -**-** **:**:** Z yyyy - MM -** **:**:** Z yyyy - MM - dd **:**:** Z yyyy - MM - dd HH :**:** Z yyyy - MM - dd HH : mm :** Z yyyy - MM - dd HH : mm : ss Z e.g., If the duration is specified as 2018-01-** **:**:** , it means within the first month of 2018. This is equal to the 2018-01-01 00:00:00\", \"2018-02-01 00:00:00 clause provided as per the previous method. You do not need to specify the timezone via Z if the time is in GMT. - The per clause specifies the time granularity for which the aggregations need to be retrieved. The value for this clause can be seconds , minutes , hours , days , months , or years . The time granularity for which you want to retrieve values must have been included in the time range you specified when calculating and persisting aggregates. For more information, see Calculating and persisting time-based aggregate values - Step 4, substep d . - The output contains the total and avgPrice per symbol for all the days falling within the given time range. Tip The following are other ways in which you can construct the query to retrieve aggregate values: Instead of providing the within and per values as constants in the query itself, you can retrieve them via attributes in the input stream definition as shown below. Define the input stream as follows. define stream TradeSummaryRetrievalStream (symbol string, startTime long, endTime long, perDuration string); The above schema allows you to enter the start time and the end time of the time duration for which you want to retrieve aggregates in the aggregate retrieval request by including values for the startTime and endTime attributes. The time granularity for which the aggregations need to be retrieved can be specified as the value for the perDuration attribute. Then add the query as follows. from TradeSummaryRetrievalStream as b join TradeAggregation as a on a.symbol == b.symbol within b.startTime, b.endTime per b.perDuration select a.symbol, a.total, a.avgPrice insert into TradeSummaryStream; Here, the within and per clauses refer to the attributes in the input stream that specify the time duration and the per duration. If you want the retrieved aggregates to be sorted in the ascending order based on time, include the AGG_TIMESTAMP in the select clause as shown below. Once it is included in the select clause, you can add the order by AGG_TIMESTAMP clause. from TradeSummaryRetrievalStream as b join TradeAggregation as a on a.symbol == b.symbol within \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" per \"days\" select AGG_TIMESTAMP , a.symbol, a.total, a.avgPrice order by AGG_TIMESTAMP insert into TradeSummaryStream; Incremental aggregation in single-node deployments and HA deployments In order to understand how incremental aggregation is carried out in different deployment, consider the following example that explains how the aggregation is executed internally. Assume that six events arrive in the following sequence, with the given timestamps. event 0 \u2192 2018-01-01 05:59:58 event 1 \u2192 2018-01-01 05:59:58 event 2 \u2192 2018-01-01 05:59:59 event 3 \u2192 2018-01-01 06:00:00 event 4 \u2192 2018-01-01 06:00:01 event 5 \u2192 2018-01-01 06:00:02 In this scenario, the aggregation is done for second, minute and hour durations. Therefore, based on the above timestamps, the second, minute, and hour during which each event occured is as follows. {width=\"500\"} As mentioned before, when storing, time based aggregatesa table is created for each time granularity in the AGGREGATE_NAME _ TIME_GRANUARITY format. Assuming that the name of the aggregation in TradeAggregation , three tables are created as follows. TradeAggregation_SECONDS TradeAggregation_MINUTES TradeAggregation_HOURS The incremental analysis related execution that is carried out by the system with the arrival of each event is described in the table below. Arriving Event Execution 0 The system initially processes event 0 at the SECOND executor level. The aggregation is retained in memory until the 58th second elapses. 1 This event also occurs during the 58th second (same as event 0). Therefore, the system aggregates events 0 and 1 together. 2 Event 2 arrives during the 59th second. The 58th second has elapsed at the time of this event arrival. Therefore, the system does the following: Writes the aggregation for the 58th second (i.e., the total aggregation for events 0 and 1) to the TradeAggregation_SECONDS table and then forwards the aggregation to be processed at the MINUTE executor level. Processes event 2 in the in-memory table at t he SECOND executor level for the 59th second. 3 Event 3 arrives during the second 0 of 06.00 minute. With this arrival, the aggregations for the 59th second and 59th minute expire. Therefore, the system does the following: Writes the aggregation for the 59th second (i.e., aggregation of event 2) to the TradeAggregation_SECONDS table, and forwards the aggregation to be processed at the MINUTE executor level . Writes the aggregation for the 59th minute to the TradeAggregation_MINUTES table, and forwards the aggregation to be processed at the HOUR execution level. The HOUR executor processes this for the 5th hour. Processes event 3 in the in-memory table at the SECOND executor level. 4 At the time event 4 arrives during the 1st second of the 06.00 minute, second 0 of the same minute has elapsed. Therefore, the system does the following: Writes the aggregation for second 0 to the TradeAggregation_SECONDS table, and forwards the aggregation to be processed at the MINUTE executor level. Processes event 4 in the in-memory table at the SECOND executor level. 5 Event 5 arrives during the 2nd second. At this time, the 1st second has elapsed. Therefore, the system does the following: Writes the aggregation for the 1st second (i.e., aggregation for event 4) to the TradeAggregation_SECONDS table, and forwards the aggregation to be processed at the MINUTE executor level. Once the aggregation for event 4 is forwarded to the MINUTE executor level, events 3 and 4 are aggregated together because they occured during the same minute. Processes event 5 in the in-memory table at the SECOND executor level. The following is the aggregation status after all six events have arrrived. In-memory Table Available Aggregation Records Processing In-Memory TradeAggregation_SECONDS Aggregation for 2018-01-01 05:59:58 (Aggregation of event 0 and 1) Aggregation for 2018-01-01 05:59:59 (event 2) Aggregation for 2018-01-01 06:00:00 (event 3) Aggregation for 2018-01-01 06:00:01 (event 4) Aggregation for 2018-01-01 06:00:02 (event 5) TradeAggregation_MINUTES Aggregation for 2018-01-01 05:59:00 (Aggregation of event 0, 1 and 2) Aggregation for 2018-01-01 06:00:00 (i.e., minute 0 of the 6th hour. Here, event 3 and 4 are aggregated together.) TradeAggregation_HOURS None Aggregation for 2018-01-01 05:00:00 (i.e., the 5th hour. Here, events 0,1, and 2 are aggregated together.) Now let's consider how the scenario described above works in a single node deployment and an HA deployment. Single node deployment If the @store configurtation is not provided, the system stores older aggregations (i.e., aggregations that are already completed for a particular second, minute etc., and therefore not running in-memory) in in-memory tables. In such a situation, a server failure results in all these aggregations done up to the time of the failure being lost. If a database configuration is provided via the @store annotation, the older aggregations are stored in the external database. Therefore, if the node fails, the system recreates the in-memory running aggregations from this stored data once you restart the server. In the given scenario, in-memory aggregations for the MINUTE execution level can be recreated with data in the TradeAggregation_SECONDS table (i.e., points 3 and 4 in the above aggregation status summary table). Similarly, in-memory aggregations for the HOUR execution level can be recreated from the TradeAggregation_MINUTES table. However, the recreation described above cannot be done for the most granular duration for which aggregation done. e.g., In the given scenario, the system cannot recreate the in-memory aggregations for the SECOND execution level because there is no database table for a prior duration. Assume that in the given scenario, a server failure occurs after all five events have arrived. Once you restart the server, only event 5 is lost because that was the only aggregation being executed for the SECOND execution level in the in-memory tables at that time. - HA deployment !!! info To understand how WSO2 SP runs as a minimum HA cluster, see [Minimum High Availability (HA) Deployment](https://docs.wso2.com/display/SP400/Minimum+High+Availability+%2528HA%2529+Deployment) . In a minimum HA setup, one runs as an active node and the other is passive. No events are lost even if the @store configuration is not provided due to snapshoting the current state of SP. When a snapshot iof the SP state is created, the system does not recreate aggregations from tables because it is not required. The newly active node (which was previously passive) continues to process the new events that arrive after the failure of the other node as in a situation where no server failura has occured. The following is a summary of the retrievability of aggregates based on how they are stored and how WSO2 SP is deployed. Scaling through distributed aggregations Distributed aggregations partially process aggregations in different nodes. This allows you to assign one node to process only a part of an aggregation (regional scaling, etc.). In order to do this all the aggregations must have a physical database and must be linked to the same database. Syntax The @PartitionById annotation must be added before the aggregation definition as shown below. @store(type=\" store type \", ...) @PartitionById(enable='true') define aggregation aggregator name from input stream select attribute name , aggregate function ( attribute name ) as attribute name , ... group by attribute name aggregate by timestamp attribute every time periods ; You can enable all Siddhi applications to partition aggregations in this manner by adding the following system parameters in the SP_HOME /conf/ PROFILE/deployment.yaml file under siddhi . siddhi: properties partitionById: true shardId: wso2-sp A unique ID must be provided for each node via a system parameter named shardId . This parameter is required when partitioning aggregations is enabled for a single Siddhi application as well as when it is enabled for all Siddhi applications. Tip To maintain data consistency, do not change the shard IDs after the first configuration. When you enable the aggregation partitioning feature, a new column ID named SHARD_ID is introduced to the aggregation tables. Therefore, you need to do one of the following options after enabling this feature to avoid errors occuring due to the differences in the table schema. Delete all the aggregation tables for SECONDS , MINUTES , HOURS , DAYS , MONTHS , YEARS . Edit the aggregation tables by adding a new column named SHARD_ID , and specify it as a primary key. Example The following query can be included in two Siddhi applications in two different nodes that are connected to the same database. Separate input events are generated for both nodes. Each node performs the aggregations and stores the results in the database. When the aggregations are retrieved, the collective result of both nodes are considered. define stream TradeStream (symbol string, price double, quantity long, timestamp long); @store(type='rdbms', jdbc.url=\"jdbc:mysql://localhost:3306/TestDB\", username=\"root\", password=\"root\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") @PartitionById(enable='true') define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(quantity) as total group by symbol aggregate by timestamp every sec ... year; Let's assume that the following input events were generated for the two nodes during a specific hour. Node 1 Event symbol price quantity 1 wso2 100 10 2 wso2 100 20 Node 2 Event symbol price quantity 1 wso2 100 10 2 wso2 100 30 Here, node 1 calculates an hourly total of 30, and node 2 calculates an hourly total of 40. When you retrieve the total for this hour via a retrieval query, the result is 70. Info For more information about incremental aggregation, see Siddhi Query Guide - Incremental Aggregation .","title":"Summarizing Data"},{"location":"guides/incremental-Analysis/#incremental-analysis","text":"Incremental aggregation allows you to obtain aggregates in an incremental manner for a specified set of time periods. This not only allows you to calculate aggregations with varied time granularity, but also allows you to access them in an interactive manner for reports, dashboards, and for further processing. Its schema is defined via the aggregation definition. Configuring aggregation queries Incremental aggregation in single-node deployments and HA deployments Scaling through distributed aggregations","title":"Incremental Analysis"},{"location":"guides/incremental-Analysis/#configuring-aggregation-queries","text":"This section explains how to calculate aggregates in an incremental manner for different sets of time periods, store the calculated values in a data store and then retrieve that information. To demonstrate this, consider a scenario where a business that sells multiple brands stores its sales data in a physicaldatabase for the purpose of retrieving them later to perform sales analysis. Each sales transaction is received with the following details: symbol : The symbol that represents the brand of the items sold. price : the price at which each item was sold. amount : The number of items sold. The Sales Analyst needs to retrieve the total number of items sold of each brand per month, per week, per day etc., and then retrieve these totals for specific time durations to prepare sales analysis reports. To address the above use case, Siddhi queries need to be designed in two steps as follows: Step 1: Calculate and persist time-based aggregate values Step 2: Retrieve calculated and persisted aggregate values","title":"Configuring aggregation queries"},{"location":"guides/incremental-Analysis/#step-1-calculate-and-persist-time-based-aggregate-values","text":"Tip Before you begin: Before creating queries to calculate and persist aggregate values, the following prerequisites need to be completed: When you are defining a physical store to store aggregate values, you need to first download, install and set up the required database type. For more information about setting up the database, see Defining Tables for Physical Stores . If the aggregation query included in the Siddhi application is only for read-only purposes, disable data purging. To write the required Siddhi queries to calculate and persist aggregate values required for the scenario outlined above, follow the steps below: To capture the input events based on which the aggregations are calculated, define an input stream as follows. The stream definition includes attributes named symbol, price and amount to capture the details described above. define stream TradeStream (symbol string, price double, quantity long, timestamp long); The stream definition includes attributes named symbol , price and amount to capture the details described above. In addition, it has an attribute named timestamp to capture the time at which the sales transaction occurs. The aggregations are executed based on this time. !!! info This attribute's value could either be a long value (reflecting the Unix timestamp in milliseconds), or a string value adhering to one of the following formats. - **` lt;yyyy gt;- lt;MM gt;- lt;dd gt; lt;HH gt;: lt;mm gt;: lt;ss gt; lt;Z gt; `** : This format can be used if the timezone needs to be specified explicitly. Here the ISO 8601 UTC offset must be provided for \\ lt;Z\\ gt; . e.g., +05:30 reflects the India Time Zone. If time is not in GMT, this value must be provided.) - **` lt;yyyy gt;- lt;MM gt;- lt;dd gt; lt;HH gt;: lt;mm gt;: lt;ss gt; `** : This format can be used if the timezone is in GMT. To persist the aggregates that are calculated via your Siddhi application, include a store definition as follows, if not the data is stored in-memory and lost when siddhi app is stopped. define stream TradeStream (symbol string, price double, quantity long, timestamp long); @store(type='rdbms', jdbc.url=\"jdbc: mysql://localhost:3306/TestDB \", username=\"root\", password=\"root\" , jdbc.driver.name =\"com.mysql.jdbc.Driver\") Define an aggregation as follows. You can name it TradeAggregation . !!! info When you save aggregate values in a store, the system uses the aggregation name you define here as part of the database table name. Table name is ` lt;Aggregation_Name gt;_ lt;Granularity gt; ` . Some database types have length limitations for table names (e.g., for Oracle, it is 30 characters). Therefore, you need to check whether the database type you have used for the store has such table name length limitation, and make sure that the aggregation name does not exceed that maximum length. define aggregation TradeAggregation To calculate aggregations, include a query as follows: To get input events from the TradeStream stream that you previously defined, add a from clause as follows. define aggregation TradeAggregation from TradeStream To select attributes to be included in the output event, add a select clause as follows. define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(quantity) as total Based on this, each output event has the following attributes. Attribute Description symbol The symbol representing the product sold is included in each output event. avgPrice The average price at which the brand is sold. This is derived by applying the avg() function to the price attribute with each input event. total The total number of items sold for each product. This is derived by applying the sum() function to the quantity attribute with each input event. The average price and the total number of items are the aggregates calculated in this scenario. To group the output by the symbol, add a group by clause as follows. define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(quantity) as total group by symbol The group by clause is optional in incremental aggregations. If it is not provided, all the events are aggregated together for each duration. The timestamp included in each input event allows you to calculate aggregates for the range of time granularities seconds-years. Therefore, to calculate aggregates for each time granularity within this range, add the aggregate by clause to this aggregate query as follows. define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(quantity) as total group by symbol aggregate by timestamp every sec ... year; This results in the average price and the total number of items sold being calculated per second, minute, hour, day, month and year. These time-based calculations are saved in the MySQL store you defined. !!! warning If you exclude one or more time granularities from this range, you cannot later retrieve aggregates for them. e.g., You cannot aggregate by timestamp every ` sec ... day ` in the aggregate definition, and then attempt to retrieve per ` months ` . This is because monthly values are not calculated and persisted. !!! info You can also provide comma-separated values such as ` every day, month, ` ... The complete Siddhi application with all the definitions and queries added looks as follows. @App:name('TradeApp') define stream TradeStream (symbol string, price double, quantity long, timestamp long); @store(type='rdbms', jdbc.url=\"jdbc:mysql://localhost:3306/TestDB\", username=\"root\", password=\"root\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(quantity) as total group by symbol aggregate by timestamp every sec ... year; The following is a list of optional annotations supported for incremental aggregation. @store(type= store type , ...) The aggregated results are stored in the database defined via this annotation in tables corresponding to each aggregate duration. For more information on how to define a store, see Defining Tables for Physical Stores . !!! info When these database entries are created, the primary key is defined internally. Therefore, you must not include an ` @PrimaryKey ` annotation within this ` @store ` annotation in the aggregation definition. If a store is not defined via this annotation, the aggregated results are stored in in-memory tables by default. Adding a specific store definition is useful in a production environment. This is because the aggregations stored in in-memory tables can be lost if a system failure occurs. The names of the tables are created in the AGGREGATION_NAME _ DURATION format. In this scenario, the following tables are created: TradeAggregation_SECONDS TradeAggregation_MINUTES TradeAggregation_HOURS TradeAggregation_DAYS TradeAggregation_MONTHS TradeAggregation_YEARS The primary keys of these tables are determined as follows: {width=\"500\"} AGG_TIMESTAMP and AGG_EVENT_TIMESTAMP are internally calculated values that reflect the time bucket of an aggregation for a particular duration. e.g. for the day aggregations, AGG_TIMESTAMP = 1515110400000 reflects that it is the aggregation for the 5 th of January 2018 ( 1515110400000 is the Unix time for 2018-01-05 00:00:00 ). All aggregations are based on GMT timezone. The other values stored in the table would be aggregations and other function calculations done in the aggregate definition. If any such function calculation is not an aggregation, the output value corresponds with the function calculation for the latest event of that time bucket. e.g., If a multiplication is carried out, the multiplication value corresponds with the latest event's multiplication as per the duration. Certain aggregations are internally stored as a collection of other aggregations. e.g., the average aggregation is internally calculated as a function of sum and count. Hence, the table only reflects a sum and a count. The actual average is returned when the user retrieves the aggregate values as described in Step 2: Retrieve calculated and persisted aggregate values . The other values stored in the database table are aggregations and other function calculations carried out via the aggregation definition. In the scenario described in this section, a timestamp is assigned to each input event via the timestamp attribute, and symbol is the group by key. Therefore, the TestDB store defined via the @store annotation uses the AGG_EVENT_TIMESTAMP and symbol attributes as primary keys. Each record . in this store must have a unique combination of values for these two attributes. @purge This specifies whether automatic data purging is enabled or not. If automatic data purging is enabled, you need to specify the time interval at which the data purging should be carried out. In addition, you need to specify the time period for which the data should be retained based on the granularity by including the @retentionPeriod annotation (described below). If this annotation is not included in an incremental aggregation query, the data purging is carried out every 15 minutes based on the default retention periods mentioned in the description of the @retentionPeriod annotation. !!! tip If you want to disable automatic data purging, you can use this annotation as follows: ` @purge(enable= false ) ` You should disable data purging if the aggregation query is included in the Siddhi application for read-only purposes. @retentionPeriod This specifies the time period for which data needs to be retained before it is purged, based on the granularity. The retention period defined for each granularity should be greater than, or equal to the minimum retention period as given below: second : 120 seconds minute : 120 minutes hour : 25 hours day : 32 days month : 13 months year : none If the retention period is not specified, the default retention period for each granularity is applied as given below: second : 120 seconds minute : 24 hours hour : 30 days day : 1 year month : All year : All","title":"Step 1: Calculate and persist time-based aggregate values"},{"location":"guides/incremental-Analysis/#step-2-retrieve-calculated-and-persisted-aggregate-values","text":"This step involves retrieving the aggregate values that you calculated and persisted in Step 1. To do this, let's add the Siddhi definitions and queries required for retrieval to the TradeApp Siddhi application that you have already started creating for this scenario. Info Retrieval logic for the same aggregation can be defined in different Siddhi app. However, only one aggregation should carry out the processing (i.e. the aggregation input stream should only feed events to one aggregation definition). For this scenario, let's assume that the Sales Analyst needs to retrieve the sales totals for the time duration between 15 th February 2014 and 16 th March 2014. To retrieve aggregations, you need to make retrieval requests. To capture these requests as events, let's define a stream as follows. @App:name('TradeApp') define stream TradeStream (symbol string, price double, quantity long, timestamp long); define stream TradeSummaryRetrievalStream (symbol string); @store(type='rdbms', jdbc.url=\"jdbc:mysql://localhost:3306/TestDB\", username=\"root\", password=\"root\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(quantity) as total group by symbol aggregate by timestamp every sec ... year; To process the events captured via the TradeSummaryRetrievalStream stream you defined, add a new query as follows. from TradeSummaryRetrievalStream as b join TradeAggregation as a on a.symbol == b.symbol within 2014-02-15 00:00:00 +05:30 , 2014-03-16 00:00:00 +05:30 per days select a.symbol, a.total, a.avgPrice insert into TradeSummaryStream; This query matches events in the TradeSummaryRetrievalStream stream and the TradeAggregation aggregation that was defined in step 1 based on the value for the symbol attribute, and performs a join for each matching pair. Based on that join, it produces an output event(s) with the symbo l, total and avgPrice attributes for the day granularity within the time range 2014-02-15 00:00:00 to 2014-03-16 00:00:00 . The time zone is represented by +05:30 . Note the following about the query given above: The on condition is optional for retrieval. You can provide the within duration in two ways as follows: within start_time , end_time : The start_time and end_time can be STRING or LONG values. If it is a string value, the format needs to be yyyy - MM - dd HH : mm : ss Z ( Z represents the timezone. It can be omitted if the time is in GMT). You can provide long values if you need to specify the times as Unix timestamps (in milliseconds). In the above query, you are specifying the time duration via this method in the STRING format. within within_duration : This method allows you to enter the time duration only as a STRING value. The format can be one of the following: yyyy -**-** **:**:** Z yyyy - MM -** **:**:** Z yyyy - MM - dd **:**:** Z yyyy - MM - dd HH :**:** Z yyyy - MM - dd HH : mm :** Z yyyy - MM - dd HH : mm : ss Z e.g., If the duration is specified as 2018-01-** **:**:** , it means within the first month of 2018. This is equal to the 2018-01-01 00:00:00\", \"2018-02-01 00:00:00 clause provided as per the previous method. You do not need to specify the timezone via Z if the time is in GMT. - The per clause specifies the time granularity for which the aggregations need to be retrieved. The value for this clause can be seconds , minutes , hours , days , months , or years . The time granularity for which you want to retrieve values must have been included in the time range you specified when calculating and persisting aggregates. For more information, see Calculating and persisting time-based aggregate values - Step 4, substep d . - The output contains the total and avgPrice per symbol for all the days falling within the given time range. Tip The following are other ways in which you can construct the query to retrieve aggregate values: Instead of providing the within and per values as constants in the query itself, you can retrieve them via attributes in the input stream definition as shown below. Define the input stream as follows. define stream TradeSummaryRetrievalStream (symbol string, startTime long, endTime long, perDuration string); The above schema allows you to enter the start time and the end time of the time duration for which you want to retrieve aggregates in the aggregate retrieval request by including values for the startTime and endTime attributes. The time granularity for which the aggregations need to be retrieved can be specified as the value for the perDuration attribute. Then add the query as follows. from TradeSummaryRetrievalStream as b join TradeAggregation as a on a.symbol == b.symbol within b.startTime, b.endTime per b.perDuration select a.symbol, a.total, a.avgPrice insert into TradeSummaryStream; Here, the within and per clauses refer to the attributes in the input stream that specify the time duration and the per duration. If you want the retrieved aggregates to be sorted in the ascending order based on time, include the AGG_TIMESTAMP in the select clause as shown below. Once it is included in the select clause, you can add the order by AGG_TIMESTAMP clause. from TradeSummaryRetrievalStream as b join TradeAggregation as a on a.symbol == b.symbol within \"2014-02-15 00:00:00 +05:30\", \"2014-03-16 00:00:00 +05:30\" per \"days\" select AGG_TIMESTAMP , a.symbol, a.total, a.avgPrice order by AGG_TIMESTAMP insert into TradeSummaryStream;","title":"Step 2: Retrieve calculated and persisted aggregate values"},{"location":"guides/incremental-Analysis/#incremental-aggregation-in-single-node-deployments-and-ha-deployments","text":"In order to understand how incremental aggregation is carried out in different deployment, consider the following example that explains how the aggregation is executed internally. Assume that six events arrive in the following sequence, with the given timestamps. event 0 \u2192 2018-01-01 05:59:58 event 1 \u2192 2018-01-01 05:59:58 event 2 \u2192 2018-01-01 05:59:59 event 3 \u2192 2018-01-01 06:00:00 event 4 \u2192 2018-01-01 06:00:01 event 5 \u2192 2018-01-01 06:00:02 In this scenario, the aggregation is done for second, minute and hour durations. Therefore, based on the above timestamps, the second, minute, and hour during which each event occured is as follows. {width=\"500\"} As mentioned before, when storing, time based aggregatesa table is created for each time granularity in the AGGREGATE_NAME _ TIME_GRANUARITY format. Assuming that the name of the aggregation in TradeAggregation , three tables are created as follows. TradeAggregation_SECONDS TradeAggregation_MINUTES TradeAggregation_HOURS The incremental analysis related execution that is carried out by the system with the arrival of each event is described in the table below. Arriving Event Execution 0 The system initially processes event 0 at the SECOND executor level. The aggregation is retained in memory until the 58th second elapses. 1 This event also occurs during the 58th second (same as event 0). Therefore, the system aggregates events 0 and 1 together. 2 Event 2 arrives during the 59th second. The 58th second has elapsed at the time of this event arrival. Therefore, the system does the following: Writes the aggregation for the 58th second (i.e., the total aggregation for events 0 and 1) to the TradeAggregation_SECONDS table and then forwards the aggregation to be processed at the MINUTE executor level. Processes event 2 in the in-memory table at t he SECOND executor level for the 59th second. 3 Event 3 arrives during the second 0 of 06.00 minute. With this arrival, the aggregations for the 59th second and 59th minute expire. Therefore, the system does the following: Writes the aggregation for the 59th second (i.e., aggregation of event 2) to the TradeAggregation_SECONDS table, and forwards the aggregation to be processed at the MINUTE executor level . Writes the aggregation for the 59th minute to the TradeAggregation_MINUTES table, and forwards the aggregation to be processed at the HOUR execution level. The HOUR executor processes this for the 5th hour. Processes event 3 in the in-memory table at the SECOND executor level. 4 At the time event 4 arrives during the 1st second of the 06.00 minute, second 0 of the same minute has elapsed. Therefore, the system does the following: Writes the aggregation for second 0 to the TradeAggregation_SECONDS table, and forwards the aggregation to be processed at the MINUTE executor level. Processes event 4 in the in-memory table at the SECOND executor level. 5 Event 5 arrives during the 2nd second. At this time, the 1st second has elapsed. Therefore, the system does the following: Writes the aggregation for the 1st second (i.e., aggregation for event 4) to the TradeAggregation_SECONDS table, and forwards the aggregation to be processed at the MINUTE executor level. Once the aggregation for event 4 is forwarded to the MINUTE executor level, events 3 and 4 are aggregated together because they occured during the same minute. Processes event 5 in the in-memory table at the SECOND executor level. The following is the aggregation status after all six events have arrrived. In-memory Table Available Aggregation Records Processing In-Memory TradeAggregation_SECONDS Aggregation for 2018-01-01 05:59:58 (Aggregation of event 0 and 1) Aggregation for 2018-01-01 05:59:59 (event 2) Aggregation for 2018-01-01 06:00:00 (event 3) Aggregation for 2018-01-01 06:00:01 (event 4) Aggregation for 2018-01-01 06:00:02 (event 5) TradeAggregation_MINUTES Aggregation for 2018-01-01 05:59:00 (Aggregation of event 0, 1 and 2) Aggregation for 2018-01-01 06:00:00 (i.e., minute 0 of the 6th hour. Here, event 3 and 4 are aggregated together.) TradeAggregation_HOURS None Aggregation for 2018-01-01 05:00:00 (i.e., the 5th hour. Here, events 0,1, and 2 are aggregated together.) Now let's consider how the scenario described above works in a single node deployment and an HA deployment. Single node deployment If the @store configurtation is not provided, the system stores older aggregations (i.e., aggregations that are already completed for a particular second, minute etc., and therefore not running in-memory) in in-memory tables. In such a situation, a server failure results in all these aggregations done up to the time of the failure being lost. If a database configuration is provided via the @store annotation, the older aggregations are stored in the external database. Therefore, if the node fails, the system recreates the in-memory running aggregations from this stored data once you restart the server. In the given scenario, in-memory aggregations for the MINUTE execution level can be recreated with data in the TradeAggregation_SECONDS table (i.e., points 3 and 4 in the above aggregation status summary table). Similarly, in-memory aggregations for the HOUR execution level can be recreated from the TradeAggregation_MINUTES table. However, the recreation described above cannot be done for the most granular duration for which aggregation done. e.g., In the given scenario, the system cannot recreate the in-memory aggregations for the SECOND execution level because there is no database table for a prior duration. Assume that in the given scenario, a server failure occurs after all five events have arrived. Once you restart the server, only event 5 is lost because that was the only aggregation being executed for the SECOND execution level in the in-memory tables at that time. - HA deployment !!! info To understand how WSO2 SP runs as a minimum HA cluster, see [Minimum High Availability (HA) Deployment](https://docs.wso2.com/display/SP400/Minimum+High+Availability+%2528HA%2529+Deployment) . In a minimum HA setup, one runs as an active node and the other is passive. No events are lost even if the @store configuration is not provided due to snapshoting the current state of SP. When a snapshot iof the SP state is created, the system does not recreate aggregations from tables because it is not required. The newly active node (which was previously passive) continues to process the new events that arrive after the failure of the other node as in a situation where no server failura has occured. The following is a summary of the retrievability of aggregates based on how they are stored and how WSO2 SP is deployed.","title":"Incremental aggregation in single-node deployments and HA deployments"},{"location":"guides/incremental-Analysis/#scaling-through-distributed-aggregations","text":"Distributed aggregations partially process aggregations in different nodes. This allows you to assign one node to process only a part of an aggregation (regional scaling, etc.). In order to do this all the aggregations must have a physical database and must be linked to the same database.","title":"Scaling through distributed aggregations"},{"location":"guides/incremental-Analysis/#syntax","text":"The @PartitionById annotation must be added before the aggregation definition as shown below. @store(type=\" store type \", ...) @PartitionById(enable='true') define aggregation aggregator name from input stream select attribute name , aggregate function ( attribute name ) as attribute name , ... group by attribute name aggregate by timestamp attribute every time periods ; You can enable all Siddhi applications to partition aggregations in this manner by adding the following system parameters in the SP_HOME /conf/ PROFILE/deployment.yaml file under siddhi . siddhi: properties partitionById: true shardId: wso2-sp A unique ID must be provided for each node via a system parameter named shardId . This parameter is required when partitioning aggregations is enabled for a single Siddhi application as well as when it is enabled for all Siddhi applications. Tip To maintain data consistency, do not change the shard IDs after the first configuration. When you enable the aggregation partitioning feature, a new column ID named SHARD_ID is introduced to the aggregation tables. Therefore, you need to do one of the following options after enabling this feature to avoid errors occuring due to the differences in the table schema. Delete all the aggregation tables for SECONDS , MINUTES , HOURS , DAYS , MONTHS , YEARS . Edit the aggregation tables by adding a new column named SHARD_ID , and specify it as a primary key.","title":"Syntax"},{"location":"guides/incremental-Analysis/#example","text":"The following query can be included in two Siddhi applications in two different nodes that are connected to the same database. Separate input events are generated for both nodes. Each node performs the aggregations and stores the results in the database. When the aggregations are retrieved, the collective result of both nodes are considered. define stream TradeStream (symbol string, price double, quantity long, timestamp long); @store(type='rdbms', jdbc.url=\"jdbc:mysql://localhost:3306/TestDB\", username=\"root\", password=\"root\" , jdbc.driver.name=\"com.mysql.jdbc.Driver\") @PartitionById(enable='true') define aggregation TradeAggregation from TradeStream select symbol, avg(price) as avgPrice, sum(quantity) as total group by symbol aggregate by timestamp every sec ... year; Let's assume that the following input events were generated for the two nodes during a specific hour. Node 1 Event symbol price quantity 1 wso2 100 10 2 wso2 100 20 Node 2 Event symbol price quantity 1 wso2 100 10 2 wso2 100 30 Here, node 1 calculates an hourly total of 30, and node 2 calculates an hourly total of 40. When you retrieve the total for this hour via a retrieval query, the result is 70. Info For more information about incremental aggregation, see Siddhi Query Guide - Incremental Aggregation .","title":"Example"},{"location":"guides/managing-Stored-Data-via-REST-APIs/","text":"Managing Stored Data via REST APIs The actions such as inserting, searching, updating, retrieving, and deleting records can be carried out by invoking the POST/stores/query REST API. These actions can be performed for tables as well as windows and aggregations. The following sections provide sample commands for each action. Retrieving records Updating records Deleting records Inserting/updating records Info Configuring Store Query Endpoint The Siddhi store query endpoint can be configured as follows in worker/editor profiles: In the siddhi.stores.query.api: section of the SP_HOME /conf/worker/deployment.yaml file, configure the following properties. The following is a sample configuration with default values. siddhi.stores.query.api: transportProperties: - name: \"server.bootstrap.socket.timeout\" value: 60 - name: \"client.bootstrap.socket.timeout\" value: 60 - name: \"latency.metrics.enabled\" value: true listenerConfigurations: - id: \"default\" host: \"0.0.0.0\" port: 7070 - id: \"msf4j-https\" host: \"0.0.0.0\" port: 7443 scheme: https keyStoreFile: \"${carbon.home}/resources/security/wso2carbon.jks\" keyStorePassword: wso2carbon certPass: wso2carbon - **` transportProperties `** - ` server.bootstrap.socket.timeout : ` The number of seconds after which the connection socket of the bootstrap server times out. - **` client.bootstrap.socket.timeout `** : The number of seconds after which the connection socket of the bootstrap server times out. - **` latency.metrics.enabled `** : If this is set to ` true ` , the latency metrics are enabled and logged for the HTTP transport. - **` listenerConfigurations `** : Multiple listeners can be configured as shown in the above sample. - **` id `** : A unique ID for the listener. - **` host `** : The host of the listener. - **` port `** : The port of the listener. - **` scheme `** : This specifies whether the transport scheme is HTTP or HTTPS. - **` keyStoreFile `** : If the transport scheme is HTTPS, this parameter specifies the path to the key store file. - **` keyStorePassword `** : If the transport scheme is HTTPS, this parameter specifies the key store password. In the siddhi.stores.query.api: section of the SP_HOME /conf/editor/deployment.yaml file, the following properties are configured by default: siddhi.stores.query.api: transportProperties: - name: \"server.bootstrap.socket.timeout\" value: 60 - name: \"client.bootstrap.socket.timeout\" value: 60 - name: \"latency.metrics.enabled\" value: true listenerConfigurations: - id: \"default\" host: \"0.0.0.0\" port: 7370 The same parameter descriptions provided for the ` lt;SP_HOME gt;/conf/worker/deployment.yaml ` file apply to this configuration. Inserting records This allows you to insert a new record to the table with the attribute values you define in the select section. Syntax select attribute name , attribute name , ... insert into table ; Sample cURL command The following cURL command submits a query that inserts a new record with the specified attribute values to the table RoomOccupancyTable . For worker: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNo, 2 as people insert into RoomOccupancyTable;\" }' -k For editor: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNo, 2 as people insert into RoomOccupancyTable;\" }' -k Retrieving records This store query retrieves one or more records that match a given condition from a specified table/window/aggregator. Retrieving records from tables and windows This is the store query to retrieve records from a table or a window. Syntax from table/window on condition ? select attribute name , attribute name , ... group by ? having ? order by ? limit ? Sample cURL command The following cURL command submits a query that retrieves room numbers and types of the rooms starting from room no 10, from a table named roomTypeTable. The roomTypeTable table must be defined in the RoomService Siddhi application. For worker: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"from roomTypeTable on roomNo = 10 select roomNo, type; \" }' -k For editor: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"from roomTypeTable on roomNo = 10 select roomNo, type; \" }' -k Sample response The following is a sample response to the sample cURL command given above. {\"records\":[ [10,\"single\"], [11, \"triple\"], [12, \"double\"] ]} Retrieving records from aggregations This is the store query to retrieve records from an aggregation. Syntax from aggregation on condition ? within time range per time granularity select attribute name , attribute name , ... groupby ? having ? order by ? limit ? Sample cURL command The following cURL command submits a query that retrieves average price of a stock . For worker: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"StockAggregationAnalysis\", \"query\" : \"from TradeAggregation on symbol=='FB' within '2018-**-** +05:00' per 'hours' select AGG_TIMESTAMP, symbol, total, avgPrice\" }' -k For editor: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"StockAggregationAnalysis\", \"query\" : \"from TradeAggregation on symbol=='FB' within '2018-**-** +05:00' per 'hours' select AGG_TIMESTAMP, symbol, total, avgPrice\" }' -k Sample response The following is a sample response to the sample cURL command given above. {\"records\":[ [1531180800, 'FB', 10000.0, 250.0], [1531184400, 'FB', 11000.0, 260.0], [1531188000, 'FB', 9000.0, 240.0] ]} Updating records This store query updates selected attributes stored in a specific table based on a given condition. Syntax select attribute name , attribute name , ...? update table set table . attribute name = ( attribute name | expression )?, table . attribute name = ( attribute name | expression )?, ... on condition Sample cURL command The following cURL command updates the room occupancy for selected records in the table, RoomOccupancyTable The records that are updated are ones of which the roon number is greater than 10 . The room occupancy is updated by adding 1 to the existing value of the people attribute. For worker: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNumber, 1 as arrival update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + arrival on RoomTypeTable.roomNo == roomNumber;\" }' -k For editor: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNumber, 1 as arrival update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + arrival on RoomTypeTable.roomNo == roomNumber;\" }' -k Deleting records This store query deletes selected records from a specified table. Syntax select ? delete table on conditional expresssion Sample cURL command The following cURL command submits a query that deletes a record in the table named RoomTypeTable if it has value for the roomNo attribute that matches the value for the roomNumber attribute of the selection that has 10 as the actual value. For worker: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNumber delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber;;\" }' -k For editor: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNumber delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber;;\" }' -k Inserting/updating records Syntax select attribute name , attribute name , ... update or insert into table set table . attribute name = expression , table . attribute name = expression , ... on condition Sample cURL command The following cURL command submits a query that attempts to update selected records in the RoomAssigneeTable table. The records that are selected to be updated are ones with room numbers that match the numbers specified in the select clause. If matching records are not found, it inserts a new record with the values provided in the select clause. For worker: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNo, \"single\" as type, \"abc\" as assignee update or insert into RoomAssigneeTable set RoomAssigneeTable.assignee = assignee on RoomAssigneeTable.roomNo == roomNo;\" }' -k For editor: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNo, \"single\" as type, \"abc\" as assignee update or insert into RoomAssigneeTable set RoomAssigneeTable.assignee = assignee on RoomAssigneeTable.roomNo == roomNo;\" }' -k Info For more information and examples for store queries, see Siddhi Query Guide - Store Query .","title":"Managing Stored Data via REST APIs"},{"location":"guides/managing-Stored-Data-via-REST-APIs/#managing-stored-data-via-rest-apis","text":"The actions such as inserting, searching, updating, retrieving, and deleting records can be carried out by invoking the POST/stores/query REST API. These actions can be performed for tables as well as windows and aggregations. The following sections provide sample commands for each action. Retrieving records Updating records Deleting records Inserting/updating records Info Configuring Store Query Endpoint The Siddhi store query endpoint can be configured as follows in worker/editor profiles: In the siddhi.stores.query.api: section of the SP_HOME /conf/worker/deployment.yaml file, configure the following properties. The following is a sample configuration with default values. siddhi.stores.query.api: transportProperties: - name: \"server.bootstrap.socket.timeout\" value: 60 - name: \"client.bootstrap.socket.timeout\" value: 60 - name: \"latency.metrics.enabled\" value: true listenerConfigurations: - id: \"default\" host: \"0.0.0.0\" port: 7070 - id: \"msf4j-https\" host: \"0.0.0.0\" port: 7443 scheme: https keyStoreFile: \"${carbon.home}/resources/security/wso2carbon.jks\" keyStorePassword: wso2carbon certPass: wso2carbon - **` transportProperties `** - ` server.bootstrap.socket.timeout : ` The number of seconds after which the connection socket of the bootstrap server times out. - **` client.bootstrap.socket.timeout `** : The number of seconds after which the connection socket of the bootstrap server times out. - **` latency.metrics.enabled `** : If this is set to ` true ` , the latency metrics are enabled and logged for the HTTP transport. - **` listenerConfigurations `** : Multiple listeners can be configured as shown in the above sample. - **` id `** : A unique ID for the listener. - **` host `** : The host of the listener. - **` port `** : The port of the listener. - **` scheme `** : This specifies whether the transport scheme is HTTP or HTTPS. - **` keyStoreFile `** : If the transport scheme is HTTPS, this parameter specifies the path to the key store file. - **` keyStorePassword `** : If the transport scheme is HTTPS, this parameter specifies the key store password. In the siddhi.stores.query.api: section of the SP_HOME /conf/editor/deployment.yaml file, the following properties are configured by default: siddhi.stores.query.api: transportProperties: - name: \"server.bootstrap.socket.timeout\" value: 60 - name: \"client.bootstrap.socket.timeout\" value: 60 - name: \"latency.metrics.enabled\" value: true listenerConfigurations: - id: \"default\" host: \"0.0.0.0\" port: 7370 The same parameter descriptions provided for the ` lt;SP_HOME gt;/conf/worker/deployment.yaml ` file apply to this configuration. Inserting records This allows you to insert a new record to the table with the attribute values you define in the select section.","title":"Managing Stored Data via REST APIs"},{"location":"guides/managing-Stored-Data-via-REST-APIs/#syntax","text":"select attribute name , attribute name , ... insert into table ;","title":"Syntax"},{"location":"guides/managing-Stored-Data-via-REST-APIs/#sample-curl-command","text":"The following cURL command submits a query that inserts a new record with the specified attribute values to the table RoomOccupancyTable . For worker: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNo, 2 as people insert into RoomOccupancyTable;\" }' -k For editor: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNo, 2 as people insert into RoomOccupancyTable;\" }' -k","title":"Sample cURL command"},{"location":"guides/managing-Stored-Data-via-REST-APIs/#retrieving-records","text":"This store query retrieves one or more records that match a given condition from a specified table/window/aggregator.","title":"Retrieving records"},{"location":"guides/managing-Stored-Data-via-REST-APIs/#retrieving-records-from-tables-and-windows","text":"This is the store query to retrieve records from a table or a window.","title":"Retrieving records from tables and windows"},{"location":"guides/managing-Stored-Data-via-REST-APIs/#syntax_1","text":"from table/window on condition ? select attribute name , attribute name , ... group by ? having ? order by ? limit ?","title":"Syntax"},{"location":"guides/managing-Stored-Data-via-REST-APIs/#sample-curl-command_1","text":"The following cURL command submits a query that retrieves room numbers and types of the rooms starting from room no 10, from a table named roomTypeTable. The roomTypeTable table must be defined in the RoomService Siddhi application. For worker: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"from roomTypeTable on roomNo = 10 select roomNo, type; \" }' -k For editor: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"from roomTypeTable on roomNo = 10 select roomNo, type; \" }' -k","title":"Sample cURL command"},{"location":"guides/managing-Stored-Data-via-REST-APIs/#sample-response","text":"The following is a sample response to the sample cURL command given above. {\"records\":[ [10,\"single\"], [11, \"triple\"], [12, \"double\"] ]}","title":"Sample response"},{"location":"guides/managing-Stored-Data-via-REST-APIs/#retrieving-records-from-aggregations","text":"This is the store query to retrieve records from an aggregation.","title":"Retrieving records from aggregations"},{"location":"guides/managing-Stored-Data-via-REST-APIs/#syntax_2","text":"from aggregation on condition ? within time range per time granularity select attribute name , attribute name , ... groupby ? having ? order by ? limit ?","title":"Syntax"},{"location":"guides/managing-Stored-Data-via-REST-APIs/#sample-curl-command_2","text":"The following cURL command submits a query that retrieves average price of a stock . For worker: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"StockAggregationAnalysis\", \"query\" : \"from TradeAggregation on symbol=='FB' within '2018-**-** +05:00' per 'hours' select AGG_TIMESTAMP, symbol, total, avgPrice\" }' -k For editor: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"StockAggregationAnalysis\", \"query\" : \"from TradeAggregation on symbol=='FB' within '2018-**-** +05:00' per 'hours' select AGG_TIMESTAMP, symbol, total, avgPrice\" }' -k","title":"Sample cURL command"},{"location":"guides/managing-Stored-Data-via-REST-APIs/#sample-response_1","text":"The following is a sample response to the sample cURL command given above. {\"records\":[ [1531180800, 'FB', 10000.0, 250.0], [1531184400, 'FB', 11000.0, 260.0], [1531188000, 'FB', 9000.0, 240.0] ]}","title":"Sample response"},{"location":"guides/managing-Stored-Data-via-REST-APIs/#updating-records","text":"This store query updates selected attributes stored in a specific table based on a given condition.","title":"Updating records"},{"location":"guides/managing-Stored-Data-via-REST-APIs/#syntax_3","text":"select attribute name , attribute name , ...? update table set table . attribute name = ( attribute name | expression )?, table . attribute name = ( attribute name | expression )?, ... on condition","title":"Syntax"},{"location":"guides/managing-Stored-Data-via-REST-APIs/#sample-curl-command_3","text":"The following cURL command updates the room occupancy for selected records in the table, RoomOccupancyTable The records that are updated are ones of which the roon number is greater than 10 . The room occupancy is updated by adding 1 to the existing value of the people attribute. For worker: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNumber, 1 as arrival update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + arrival on RoomTypeTable.roomNo == roomNumber;\" }' -k For editor: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNumber, 1 as arrival update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + arrival on RoomTypeTable.roomNo == roomNumber;\" }' -k","title":"Sample cURL command"},{"location":"guides/managing-Stored-Data-via-REST-APIs/#deleting-records","text":"This store query deletes selected records from a specified table.","title":"Deleting records"},{"location":"guides/managing-Stored-Data-via-REST-APIs/#syntax_4","text":"select ? delete table on conditional expresssion","title":"Syntax"},{"location":"guides/managing-Stored-Data-via-REST-APIs/#sample-curl-command_4","text":"The following cURL command submits a query that deletes a record in the table named RoomTypeTable if it has value for the roomNo attribute that matches the value for the roomNumber attribute of the selection that has 10 as the actual value. For worker: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNumber delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber;;\" }' -k For editor: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNumber delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber;;\" }' -k","title":"Sample cURL command"},{"location":"guides/managing-Stored-Data-via-REST-APIs/#insertingupdating-records","text":"","title":"Inserting/updating records"},{"location":"guides/managing-Stored-Data-via-REST-APIs/#syntax_5","text":"select attribute name , attribute name , ... update or insert into table set table . attribute name = expression , table . attribute name = expression , ... on condition","title":"Syntax"},{"location":"guides/managing-Stored-Data-via-REST-APIs/#sample-curl-command_5","text":"The following cURL command submits a query that attempts to update selected records in the RoomAssigneeTable table. The records that are selected to be updated are ones with room numbers that match the numbers specified in the select clause. If matching records are not found, it inserts a new record with the values provided in the select clause. For worker: curl -X POST https://localhost:7443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNo, \"single\" as type, \"abc\" as assignee update or insert into RoomAssigneeTable set RoomAssigneeTable.assignee = assignee on RoomAssigneeTable.roomNo == roomNo;\" }' -k For editor: curl -X POST http://localhost:7370/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNo, \"single\" as type, \"abc\" as assignee update or insert into RoomAssigneeTable set RoomAssigneeTable.assignee = assignee on RoomAssigneeTable.roomNo == roomNo;\" }' -k Info For more information and examples for store queries, see Siddhi Query Guide - Store Query .","title":"Sample cURL command"},{"location":"guides/managing-Stored-Data-via-Streams/","text":"Managing Stored Data via Streams This section covers how to manage stored data in event tables via streams. Inserting records Retrieving records Updating a table Deleting Records Searching records Inserting/updating records Inserting records Prerequisites In order to insert events to a table: General prerequisites should be completed. The event stream from which the events to be inserted are taken should be defined. The table to which events are to be inserted should be defined. For more information, see Defining a table . Query syntax The following is the syntax to insert events into a table. from STREAM_NAME select ATTRIBUTE1_NAME , ATTRIBUTE2_NAME , ATTRIBUTE3_NAME ... insert into TABLE_NAME ; Example The following query inserts events from the FooStream stream to the FooTable table with the symbol , price , and volume attributes. from FooStream select symbol, price, volume insert into FooTable; Retrieving records Prerequisites In order to retrieve events from a table: General prerequisites should be completed. The table to be read should be already defined. For more information, see Defining a table . One or more events should be inserted into the table. For more information, see Inserting events . Query syntax The following is the query syntax to retrieve events from an existing table. For more information, please refer to Siddhi Query Guide - Join Table . from STREAM_NAME join TABLE_NAME on CONDITION select ( STREAM_NAME | TABLE_NAME ). ATTRIBUTE1_NAME , ( STREAM_NAME | TABLE_NAME ). ATTRIBUTE2_NAME , ... insert into OUTPUT_STREAM Example The following query joins the FooStream events with the events stored in the StockTable table. An output event is created for each matching pair of events, and it is inserted into another stream named OutputStream . from FooStream#window.length(1) join StockTable select FooStream.symbol as checkSymbol, StockTable.symbol as symbol, StockTable.volume as volume insert into OutputStream The information inserted with the output event is as follows. Source Value of Attribute name in the output event FooStream stream symbol attribute checkSymbol StockTable table symbol attribute symbol StockTable table volume attribute volume Updating a table This section explains how to update the selected records of an existing table. Prerequisites In order to update events in a table: General prerequisites should be completed. The table to be updated should be already defined. For more information, see Defining a table . The event stream with the events based on which the updates are made must be already defined. One or more events should be inserted into the table. For more information, see Inserting events . Query syntax from STREAM_NAME select ATTRIBUTE1_NAME , ATTRIBUTE2_NAME , ... update TABLE_NAME (for OUTPUT_EVENT_TYPE )? set TABLE_NAME . ATTRIBUTE_NAME = ( ATTRIBUTE_NAME | EXPRESSION )?, TABLE_NAME . ATTRIBUTE_NAME = ( ATTRIBUTE_NAME | EXPRESSION )?, ... on CONDITION Example The following query updates the events in the FooTable table with values from the latest events of the FooStream event stream. The events in the table are updated only if the existing record in the table has the same value as the new event for the symbol attribute, and a value greater than 50 for the price attribute. from FooStream select symbol, price, volume update FooTable set FooTable.symbol = symbol, FooTable.price = price, FooTable.volume = volume on (FooTable.symbol == symbol and price 50) Methods of Updating the columns in a table This section gives further information on methods of updating the columns in an existing table. The value used for updating a table column can be any of the following: A constant FROM sensorStream SELECT sensorId, temperature, humidity UPDATE sensorTable SET sensorTable.column_temp = 1 ON sensorId sensorTable.column_ID One of the attributes specified in the SELECT clause FROM fooStream SELECT roomNo, time: timestampInMilliseconds() as ts UPDATE barTable SET barTable.timestamp = ts ON barTable.room_no == roomNo AND roomNo 2 A basic arithmetic operation applied on an output attribute FROM sensorStream SELECT sensorId, temperature, humidity UPDATE sensorTable SET sensorTable.column_temp = temperature + 10 ON sensorId sensorTable.column_ID A basic arithmetic operation applied to a column value in the event table FROM sensorStream SELECT sensorId, temperature, humidity UPDATE sensorTable SET sensorTable.column_temp = sensorTable.column_temp + 10 ON sensorId sensorTable.column_ID Deleting Records This section explains how to delete existing records in a table based on a specific condition. Prerequisites In order to delete selected events in a table: General prerequisites should be completed. The table with the records to be deleted should be already defined. For more information, see Defining a table . The event stream with the events with which the records in the table are compared (i.e., to apply the condition based on which the events are deleted) must be already defined. One or more events should be inserted into the table. For more information, see Inserting events . Query syntax from STREAM_NAME select ATTRIBUTE1_NAME , ATTRIBUTE2_NAME , ... delete TABLE_NAME (for OUTPUT_EVENT_TYPE )? on CONDITION Example This query deletes the events in the RoomTypeTable table if its value for the roomNo attribute is equal to the roomNumber attribute value of DeleteStream. from DeleteStream delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber; Searching records This section explains how to check whether a specific record exists in an event table. Prerequisites In order to search for a record in a table that matches a specific condition: General prerequisites should be completed. The table to be searched should be already defined. For more information, see Defining a table . The event stream with the events with which the records in the table are compared (i.e., to apply the condition based on which the events are searched) must be already defined. One or more events should be inserted into the table. For more information, see Inserting events . Query syntax from STREAM_NAME [ CONDITION in TABLE_NAME ] select ATTRIBUTE1_NAME , ATTRIBUTE2_NAME , ... insert into OUTPUT_STREAM_NAME Example The following query matches events arriving from the FooStream event stream with the existing recored stored in the StockTable table. If the symbol attribute of an event saved in the table has the same value as the event from the FooStream stream, that event is inserted into the OutputStream stream. from FooStream[StockTable.symbol==symbol in StockTable] insert into OutputStream; Inserting/updating records This section explains how to update a selection of records in a table based on the new events from a specific event stream. The selection is made based on a specific condition that matches events from the stream with events in the table. When the events from the stream have no matching events in the table, they are inserted into the table as new events. Prerequisites General prerequisites should be completed. The table for which this operations is to be performed must be already defined. For more information, see Defining a table . The event stream from which the events with which the records in the table are compared (i.e., to apply the condition based on which the events are inserted/updated) must be already defined. Query syntax The query syntax to perform the insert/update operation for a table is as follows. from STREAM_NAME select ATTRIBUTE1_NAME , ATTRIBUTE2_NAME , ... update or insert into TABLE_NAME (for OUTPUT_EVENT_TYPE )? set TABLE_NAME . ATTRIBUTE_NAME = EXPRESSION , TABLE_NAME . ATTRIBUTE_NAME = EXPRESSION , ... on CONDITION Example This query matches events from the FooStream stream with the events stored in the StockTable table. When an event in the table has the same value for the symbol attribute as the matching new event from the event stream, it is updated based on the new event. If a new event from the event stream does not have a matching event in the table (i.e., an event with the same value for the symbol attribute), that event is inserted as a new event. from FooStream select * update or insert into StockTable on StockTable.symbol == symbol","title":"Managing Stored Data via Streams"},{"location":"guides/managing-Stored-Data-via-Streams/#managing-stored-data-via-streams","text":"This section covers how to manage stored data in event tables via streams. Inserting records Retrieving records Updating a table Deleting Records Searching records Inserting/updating records","title":"Managing Stored Data via Streams"},{"location":"guides/managing-Stored-Data-via-Streams/#inserting-records","text":"","title":"Inserting records"},{"location":"guides/managing-Stored-Data-via-Streams/#prerequisites","text":"In order to insert events to a table: General prerequisites should be completed. The event stream from which the events to be inserted are taken should be defined. The table to which events are to be inserted should be defined. For more information, see Defining a table .","title":"Prerequisites"},{"location":"guides/managing-Stored-Data-via-Streams/#query-syntax","text":"The following is the syntax to insert events into a table. from STREAM_NAME select ATTRIBUTE1_NAME , ATTRIBUTE2_NAME , ATTRIBUTE3_NAME ... insert into TABLE_NAME ;","title":"Query syntax"},{"location":"guides/managing-Stored-Data-via-Streams/#example","text":"The following query inserts events from the FooStream stream to the FooTable table with the symbol , price , and volume attributes. from FooStream select symbol, price, volume insert into FooTable;","title":"Example"},{"location":"guides/managing-Stored-Data-via-Streams/#retrieving-records","text":"","title":"Retrieving records"},{"location":"guides/managing-Stored-Data-via-Streams/#prerequisites_1","text":"In order to retrieve events from a table: General prerequisites should be completed. The table to be read should be already defined. For more information, see Defining a table . One or more events should be inserted into the table. For more information, see Inserting events .","title":"Prerequisites"},{"location":"guides/managing-Stored-Data-via-Streams/#query-syntax_1","text":"The following is the query syntax to retrieve events from an existing table. For more information, please refer to Siddhi Query Guide - Join Table . from STREAM_NAME join TABLE_NAME on CONDITION select ( STREAM_NAME | TABLE_NAME ). ATTRIBUTE1_NAME , ( STREAM_NAME | TABLE_NAME ). ATTRIBUTE2_NAME , ... insert into OUTPUT_STREAM","title":"Query syntax"},{"location":"guides/managing-Stored-Data-via-Streams/#example_1","text":"The following query joins the FooStream events with the events stored in the StockTable table. An output event is created for each matching pair of events, and it is inserted into another stream named OutputStream . from FooStream#window.length(1) join StockTable select FooStream.symbol as checkSymbol, StockTable.symbol as symbol, StockTable.volume as volume insert into OutputStream The information inserted with the output event is as follows. Source Value of Attribute name in the output event FooStream stream symbol attribute checkSymbol StockTable table symbol attribute symbol StockTable table volume attribute volume","title":"Example"},{"location":"guides/managing-Stored-Data-via-Streams/#updating-a-table","text":"This section explains how to update the selected records of an existing table.","title":"Updating a table"},{"location":"guides/managing-Stored-Data-via-Streams/#prerequisites_2","text":"In order to update events in a table: General prerequisites should be completed. The table to be updated should be already defined. For more information, see Defining a table . The event stream with the events based on which the updates are made must be already defined. One or more events should be inserted into the table. For more information, see Inserting events .","title":"Prerequisites"},{"location":"guides/managing-Stored-Data-via-Streams/#query-syntax_2","text":"from STREAM_NAME select ATTRIBUTE1_NAME , ATTRIBUTE2_NAME , ... update TABLE_NAME (for OUTPUT_EVENT_TYPE )? set TABLE_NAME . ATTRIBUTE_NAME = ( ATTRIBUTE_NAME | EXPRESSION )?, TABLE_NAME . ATTRIBUTE_NAME = ( ATTRIBUTE_NAME | EXPRESSION )?, ... on CONDITION","title":"Query syntax"},{"location":"guides/managing-Stored-Data-via-Streams/#example_2","text":"The following query updates the events in the FooTable table with values from the latest events of the FooStream event stream. The events in the table are updated only if the existing record in the table has the same value as the new event for the symbol attribute, and a value greater than 50 for the price attribute. from FooStream select symbol, price, volume update FooTable set FooTable.symbol = symbol, FooTable.price = price, FooTable.volume = volume on (FooTable.symbol == symbol and price 50)","title":"Example"},{"location":"guides/managing-Stored-Data-via-Streams/#methods-of-updating-the-columns-in-a-table","text":"This section gives further information on methods of updating the columns in an existing table. The value used for updating a table column can be any of the following: A constant FROM sensorStream SELECT sensorId, temperature, humidity UPDATE sensorTable SET sensorTable.column_temp = 1 ON sensorId sensorTable.column_ID One of the attributes specified in the SELECT clause FROM fooStream SELECT roomNo, time: timestampInMilliseconds() as ts UPDATE barTable SET barTable.timestamp = ts ON barTable.room_no == roomNo AND roomNo 2 A basic arithmetic operation applied on an output attribute FROM sensorStream SELECT sensorId, temperature, humidity UPDATE sensorTable SET sensorTable.column_temp = temperature + 10 ON sensorId sensorTable.column_ID A basic arithmetic operation applied to a column value in the event table FROM sensorStream SELECT sensorId, temperature, humidity UPDATE sensorTable SET sensorTable.column_temp = sensorTable.column_temp + 10 ON sensorId sensorTable.column_ID","title":"Methods of Updating the columns in a table"},{"location":"guides/managing-Stored-Data-via-Streams/#deleting-records","text":"This section explains how to delete existing records in a table based on a specific condition.","title":"Deleting Records"},{"location":"guides/managing-Stored-Data-via-Streams/#prerequisites_3","text":"In order to delete selected events in a table: General prerequisites should be completed. The table with the records to be deleted should be already defined. For more information, see Defining a table . The event stream with the events with which the records in the table are compared (i.e., to apply the condition based on which the events are deleted) must be already defined. One or more events should be inserted into the table. For more information, see Inserting events .","title":"Prerequisites"},{"location":"guides/managing-Stored-Data-via-Streams/#query-syntax_3","text":"from STREAM_NAME select ATTRIBUTE1_NAME , ATTRIBUTE2_NAME , ... delete TABLE_NAME (for OUTPUT_EVENT_TYPE )? on CONDITION","title":"Query syntax"},{"location":"guides/managing-Stored-Data-via-Streams/#example_3","text":"This query deletes the events in the RoomTypeTable table if its value for the roomNo attribute is equal to the roomNumber attribute value of DeleteStream. from DeleteStream delete RoomTypeTable on RoomTypeTable.roomNo == roomNumber;","title":"Example"},{"location":"guides/managing-Stored-Data-via-Streams/#searching-records","text":"This section explains how to check whether a specific record exists in an event table.","title":"Searching records"},{"location":"guides/managing-Stored-Data-via-Streams/#prerequisites_4","text":"In order to search for a record in a table that matches a specific condition: General prerequisites should be completed. The table to be searched should be already defined. For more information, see Defining a table . The event stream with the events with which the records in the table are compared (i.e., to apply the condition based on which the events are searched) must be already defined. One or more events should be inserted into the table. For more information, see Inserting events .","title":"Prerequisites"},{"location":"guides/managing-Stored-Data-via-Streams/#query-syntax_4","text":"from STREAM_NAME [ CONDITION in TABLE_NAME ] select ATTRIBUTE1_NAME , ATTRIBUTE2_NAME , ... insert into OUTPUT_STREAM_NAME","title":"Query syntax"},{"location":"guides/managing-Stored-Data-via-Streams/#example_4","text":"The following query matches events arriving from the FooStream event stream with the existing recored stored in the StockTable table. If the symbol attribute of an event saved in the table has the same value as the event from the FooStream stream, that event is inserted into the OutputStream stream. from FooStream[StockTable.symbol==symbol in StockTable] insert into OutputStream;","title":"Example"},{"location":"guides/managing-Stored-Data-via-Streams/#insertingupdating-records","text":"This section explains how to update a selection of records in a table based on the new events from a specific event stream. The selection is made based on a specific condition that matches events from the stream with events in the table. When the events from the stream have no matching events in the table, they are inserted into the table as new events.","title":"Inserting/updating records"},{"location":"guides/managing-Stored-Data-via-Streams/#prerequisites_5","text":"General prerequisites should be completed. The table for which this operations is to be performed must be already defined. For more information, see Defining a table . The event stream from which the events with which the records in the table are compared (i.e., to apply the condition based on which the events are inserted/updated) must be already defined.","title":"Prerequisites"},{"location":"guides/managing-Stored-Data-via-Streams/#query-syntax_5","text":"The query syntax to perform the insert/update operation for a table is as follows. from STREAM_NAME select ATTRIBUTE1_NAME , ATTRIBUTE2_NAME , ... update or insert into TABLE_NAME (for OUTPUT_EVENT_TYPE )? set TABLE_NAME . ATTRIBUTE_NAME = EXPRESSION , TABLE_NAME . ATTRIBUTE_NAME = EXPRESSION , ... on CONDITION","title":"Query syntax"},{"location":"guides/managing-Stored-Data-via-Streams/#example_5","text":"This query matches events from the FooStream stream with the events stored in the StockTable table. When an event in the table has the same value for the symbol attribute as the matching new event from the event stream, it is updated based on the new event. If a new event from the event stream does not have a matching event in the table (i.e., an event with the same value for the symbol attribute), that event is inserted as a new event. from FooStream select * update or insert into StockTable on StockTable.symbol == symbol","title":"Example"},{"location":"guides/processing-Streaming-Events/","text":"Processing Streaming Events Forrester defines Streaming Analytics as follows: Info \"Software that provides analytical operators to orchestrate data flow , calculate analytics , and detect patterns on event data from multiple, disparate live data sources to allow developers to build applications that sense, think, and act in real time . The stream processing capabilities of WSO2 SP allow you to capture high volume data flows and process them in real time, and present results in a streaming manner. Following are a few stream processing capabilities of WSO2 SP. Functions The following functions shipped with Siddhi, consume zero, one or more parameters from streaming events and produce a desired value as output. These functions are executed per event. For more information on Siddhi functions, please refer to Siddhi Query Guide - Functions. More functions are made available as Siddhi Extensions . eventTimestamp - Returns the timestamp of the processed event eventTimeStamp example from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream UUID - Generates a UUID (Universally Unique Identifier) UUID example from fooStream select UUID() as messageID, messageDetails insert into barStream; default - Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter. The function is given as default(\\ attribute>, \\ default value>) default example from fooStream select default(temp, 0.0) as temp, roomNum insert into barStream; cast - Converts the first parameter according to the cast-to parameter. Incompatible arguments cause Class Cast exceptions if further processed. cast example from fooStream select symbol as name, cast(temp, 'double') as temp insert into barStream; convert - Converts the first input parameter according to the convert-to parameter convert example from fooStream select convert(temp, 'double') as temp insert into barStream; ifThenElse - Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. The function is given as ifThenElse(\\ condition>, \\ if.expression>, \\ else.expression>) ifThenElse example from fooStream ifThenElse(sensorValue 35,'High','Low') insert into barStream; minimum - Returns the minimum value of the input parameters minimum example from fooStream select minimum(price1, price2, price3) as minPrice insert into barStream; maximum - Returns the maximum value of the input parameters. This function could be used similar to how 'minimum' function is used in a query. coalesce - Returns the value of the first input parameter that is not null. All input parameters have to be of the same type. coalesce example from fooStream select coalesce('123', null, '789') as value insert into barStream; instanceOfBoolean - Returns 'true' if the input is a instance of Boolean. Otherwise returns 'false'. instanceOfBoolean example from fooStream select instanceOfBoolean(switchState) as state insert into barStream; instanceOfDouble - Returns 'true' if the input is a instance of Double. Otherwise returns 'false'. This function could be used similar to how ' instanceOfBoolean ' function is used in a query. instanceOfFloat - Returns 'true' if the input is a instance of Float. Otherwise returns 'false'. This function could be used similar to how 'instanceOfBoolean' function is used in a query. instanceOfInteger - Returns 'true' if the input is a instance of Integer. Otherwise returns 'false'. This function could be used similar to how 'instanceOfBoolean' function is used in a query. instanceOfLong - Returns 'true' if the input is a instance of Long. Otherwise returns 'false'. This function could be used similar to how 'instanceOfBoolean' function is used in a query. instanceOfString - Returns 'true' if the input is a instance of String. Otherwise returns 'false'. This function could be used similar to how 'instanceOfBoolean' function is used in a query. Filters Filters are applied to input data received in streams to filter information based on given conditions. For more information, see Siddhi Query Guide - Filters . e.g., Filtering cash withdrawals from an ATM machine where the withdrawal amount is greater tha $100, and the withdrawal data is between 01/12/2017-15/12/2017. Windows Windows allow you to capture a subset of events based on a duration or number of events criterion, from an input stream for calculation. Each input stream can only have a maximum of one window. Criterion - Time windows vs length windows The subset of events can be captured based on one of the following. Time : This involves capturing all the events that arrive during a specific time interval (e.g., writing a query that is applicable to events that occur during a period of 10 minutes). Length : This involves capturing a subset of events based on the number of events (e.g., writing a query applicable to each group that consists of 10 events). Method of processing - Sliding windows vs batch windows Consider 10 events that have arrived in a stream. When a sliding length window is included in a Siddhi query, the following event groups are identified: Events 1-5 Events 2-6 Events 3-7 Events 4-8 Events 5-9 Events 6-10 When a batch window is included in a Siddhi query, the folowing event groups are identified: Events 1-5 Events 6-10 This window feature differs from the Defined Window concept elaborated in Siddhi Application Overview due to this being specific to a single query only. If a window is to be shared among queries, the Defined Window must be used For more information about windows, see Siddhi Query Guide - Window . Aggregate Functions Aggregation functions allow executing aggregations such as sum, avg, min, etc. on a set of events grouped by a window. If a window is not defined, the aggregation(s) would be calculated by considering all the events arriving at a stream. Consider the following events arriving at a stream, where the prices vary from one another. Event 1: price = 10.00 Event 2: price = 20.00 Event 3: price = 30.00 Event 4: price = 40.00 Event 5: price = 50.00 Consider the following two queries, where sum of price is calculated based on a length window of 2, and without a window respectively Query1: Aggregate based on length window from fooStream#window.length(2) select sum(price) as totalPrice insert into barStream; Query2: Aggregate without a window from fooStream select sum(price) as totalPrice insert into barStream; The following output would be generated for Query 1. totalPrice = 10.00 totalPrice = 30.00 totalPrice = 50.00 totalPrice = 70.00 totalPrice = 90.00 The following output would be generated for Query 2. totalPrice = 10.00 totalPrice = 30.00 totalPrice = 60.00 totalPrice = 100.00 totalPrice = 150.00 For more information on aggregate function, please refer to Siddhi Query Guide - Aggregate Functions . Group By With the group by functionality, events could be grouped based on a certain attribute, when performing aggregations. Consider the following events, which have a symbol attribute and a price attribute. Event 1: symbol = wso2, price = 10.00 Event 2: symbol = wso2, price = 20.00 Event 3: symbol = abc, price = 30.00 Event 4: symbol = abc, price = 40.00 Event 5: symbol = abc, price = 50.00 When the sum aggregation is calculated for a window of length 3, after grouping by symbol, the given output is generated Query1: Aggregate based on length window from fooStream#window.length(3) select symbol, sum(price) as totalPrice group by symbol insert into barStream; Output: symbol = wso2, totalPrice = 10.00 symbol = wso2, totalPrice = 30.00 symbol = abc, totalPrice = 30.00 symbol = abc, totalPrice = 70.00 symbol = abc, totalPrice = 120.00 For more information on group by, please refer to Siddhi Query Guide - Group By . Having Having allows to filter events after processing the select statement. This is useful if the filtering is based on some value derived by applying a function/ aggregation. For example, if you want to find all the events where maximum production total across 3 days is less than 1000 units, such filtering could be achieved with a query as follwos Having example from fooStream select item, maximum(productionOnDay1, productionOnDay2, productionOnDay3) as maxProduction having maxProduction 1000 insert into barStream; For more information on having clause, please refer to Siddhi Query Guide - Having . Join Join is an important feature of Siddhi, which allows combining pair of streams, pair of windows, stream with window, stream/ window with a table and stream/window with an aggregation The join logic can be defined with 'on' condition as well, which restricts the events combined in a join. For example, assume that we need to combine a transaction stream with a table containing blacklisted credit card numbers, to identify fraudulent transactions. Following query helps achieve such a requirement. Join query example from transactionStream as t join blacklistedCardsTable as b on t.cardNumber = b.cardNumber select t.cardNumber, t.transactionDetails, b.fraudDescription insert into suspiciousTransactionStream; For more information on join queries, please refer to Siddhi Query Guide - Join . Output Rate Limiting Output rate limiting allows queries to output events periodically based on a specified condition. This helps to limit continuously sending events as output. For more information on output rate limiting, please refer to Siddhi Query Guide - Output Rate Limiting Partitioning Partitioning in Siddhi allows to logically seperate events arriving at a stream, and to process them separately, in parallel. For example, assume that the total number of transactions per company needs to be monitored at a stock exchange. However, if all the transactions are arriving at a single stream, we would need to logically seperate them based on the company symbol. The following example depicts how this can be achieved with Siddhi partitioning. Partition example partition with (symbol of stockStream ) begin from stockStream select symbol, count() as transactionCount insert into transactionsPerSymbol; end; Partitioning can be done based on an attribute value as above, or based on a condition. For more information on partitioning, please refer to Siddhi Query Guide - Partitioning Trigger Triggers could be used to get events generated by the system itself, based on some time duration. An example for a trigger definition is as follows. Trigger example define trigger FiveMinTriggerStream at every 5 sec; This would generate an event every 5 seconds. The generated event would contain an attribute of type 'Long' named 'triggered_time', reflecting the time at which event was triggered in milliseconds (epoch time). Trigger could be defined as a time interval, a cron job or to generate an event when Siddhi is started. For more information on triggers, please refer to Siddhi Query Guide - Trigger Script Scripts allow to define function operations in a different programming language. An example is as follows. Script example define function concatFn[javascript] return string { var str1 = data[0]; var str2 = data[1]; var str3 = data[2]; var response = str1 + str2 + str3; return response; }; define stream TempStream(deviceID long, roomNo int, temp double); from TempStream select concatFn(roomNo,'-',deviceID) as id, temp insert into DeviceTempStream; For more information on scripts, please refer to Siddhi Query Guide - Script","title":"Processing Streaming Events"},{"location":"guides/processing-Streaming-Events/#processing-streaming-events","text":"Forrester defines Streaming Analytics as follows: Info \"Software that provides analytical operators to orchestrate data flow , calculate analytics , and detect patterns on event data from multiple, disparate live data sources to allow developers to build applications that sense, think, and act in real time . The stream processing capabilities of WSO2 SP allow you to capture high volume data flows and process them in real time, and present results in a streaming manner. Following are a few stream processing capabilities of WSO2 SP.","title":"Processing Streaming Events"},{"location":"guides/processing-Streaming-Events/#functions","text":"The following functions shipped with Siddhi, consume zero, one or more parameters from streaming events and produce a desired value as output. These functions are executed per event. For more information on Siddhi functions, please refer to Siddhi Query Guide - Functions. More functions are made available as Siddhi Extensions . eventTimestamp - Returns the timestamp of the processed event eventTimeStamp example from fooStream select symbol as name, eventTimestamp() as eventTimestamp insert into barStream UUID - Generates a UUID (Universally Unique Identifier) UUID example from fooStream select UUID() as messageID, messageDetails insert into barStream; default - Checks if the 'attribute' parameter is null and if so returns the value of the 'default' parameter. The function is given as default(\\ attribute>, \\ default value>) default example from fooStream select default(temp, 0.0) as temp, roomNum insert into barStream; cast - Converts the first parameter according to the cast-to parameter. Incompatible arguments cause Class Cast exceptions if further processed. cast example from fooStream select symbol as name, cast(temp, 'double') as temp insert into barStream; convert - Converts the first input parameter according to the convert-to parameter convert example from fooStream select convert(temp, 'double') as temp insert into barStream; ifThenElse - Evaluates the 'condition' parameter and returns value of the 'if.expression' parameter if the condition is true, or returns value of the 'else.expression' parameter if the condition is false. The function is given as ifThenElse(\\ condition>, \\ if.expression>, \\ else.expression>) ifThenElse example from fooStream ifThenElse(sensorValue 35,'High','Low') insert into barStream; minimum - Returns the minimum value of the input parameters minimum example from fooStream select minimum(price1, price2, price3) as minPrice insert into barStream; maximum - Returns the maximum value of the input parameters. This function could be used similar to how 'minimum' function is used in a query. coalesce - Returns the value of the first input parameter that is not null. All input parameters have to be of the same type. coalesce example from fooStream select coalesce('123', null, '789') as value insert into barStream; instanceOfBoolean - Returns 'true' if the input is a instance of Boolean. Otherwise returns 'false'. instanceOfBoolean example from fooStream select instanceOfBoolean(switchState) as state insert into barStream; instanceOfDouble - Returns 'true' if the input is a instance of Double. Otherwise returns 'false'. This function could be used similar to how ' instanceOfBoolean ' function is used in a query. instanceOfFloat - Returns 'true' if the input is a instance of Float. Otherwise returns 'false'. This function could be used similar to how 'instanceOfBoolean' function is used in a query. instanceOfInteger - Returns 'true' if the input is a instance of Integer. Otherwise returns 'false'. This function could be used similar to how 'instanceOfBoolean' function is used in a query. instanceOfLong - Returns 'true' if the input is a instance of Long. Otherwise returns 'false'. This function could be used similar to how 'instanceOfBoolean' function is used in a query. instanceOfString - Returns 'true' if the input is a instance of String. Otherwise returns 'false'. This function could be used similar to how 'instanceOfBoolean' function is used in a query.","title":"Functions"},{"location":"guides/processing-Streaming-Events/#filters","text":"Filters are applied to input data received in streams to filter information based on given conditions. For more information, see Siddhi Query Guide - Filters . e.g., Filtering cash withdrawals from an ATM machine where the withdrawal amount is greater tha $100, and the withdrawal data is between 01/12/2017-15/12/2017.","title":"Filters"},{"location":"guides/processing-Streaming-Events/#windows","text":"Windows allow you to capture a subset of events based on a duration or number of events criterion, from an input stream for calculation. Each input stream can only have a maximum of one window.","title":"Windows"},{"location":"guides/processing-Streaming-Events/#criterion-time-windows-vs-length-windows","text":"The subset of events can be captured based on one of the following. Time : This involves capturing all the events that arrive during a specific time interval (e.g., writing a query that is applicable to events that occur during a period of 10 minutes). Length : This involves capturing a subset of events based on the number of events (e.g., writing a query applicable to each group that consists of 10 events).","title":"Criterion - Time windows vs length windows"},{"location":"guides/processing-Streaming-Events/#method-of-processing-sliding-windows-vs-batch-windows","text":"Consider 10 events that have arrived in a stream. When a sliding length window is included in a Siddhi query, the following event groups are identified: Events 1-5 Events 2-6 Events 3-7 Events 4-8 Events 5-9 Events 6-10 When a batch window is included in a Siddhi query, the folowing event groups are identified: Events 1-5 Events 6-10 This window feature differs from the Defined Window concept elaborated in Siddhi Application Overview due to this being specific to a single query only. If a window is to be shared among queries, the Defined Window must be used For more information about windows, see Siddhi Query Guide - Window .","title":"Method of processing - Sliding windows vs batch windows"},{"location":"guides/processing-Streaming-Events/#aggregate-functions","text":"Aggregation functions allow executing aggregations such as sum, avg, min, etc. on a set of events grouped by a window. If a window is not defined, the aggregation(s) would be calculated by considering all the events arriving at a stream. Consider the following events arriving at a stream, where the prices vary from one another. Event 1: price = 10.00 Event 2: price = 20.00 Event 3: price = 30.00 Event 4: price = 40.00 Event 5: price = 50.00 Consider the following two queries, where sum of price is calculated based on a length window of 2, and without a window respectively Query1: Aggregate based on length window from fooStream#window.length(2) select sum(price) as totalPrice insert into barStream; Query2: Aggregate without a window from fooStream select sum(price) as totalPrice insert into barStream; The following output would be generated for Query 1. totalPrice = 10.00 totalPrice = 30.00 totalPrice = 50.00 totalPrice = 70.00 totalPrice = 90.00 The following output would be generated for Query 2. totalPrice = 10.00 totalPrice = 30.00 totalPrice = 60.00 totalPrice = 100.00 totalPrice = 150.00 For more information on aggregate function, please refer to Siddhi Query Guide - Aggregate Functions .","title":"Aggregate Functions"},{"location":"guides/processing-Streaming-Events/#group-by","text":"With the group by functionality, events could be grouped based on a certain attribute, when performing aggregations. Consider the following events, which have a symbol attribute and a price attribute. Event 1: symbol = wso2, price = 10.00 Event 2: symbol = wso2, price = 20.00 Event 3: symbol = abc, price = 30.00 Event 4: symbol = abc, price = 40.00 Event 5: symbol = abc, price = 50.00 When the sum aggregation is calculated for a window of length 3, after grouping by symbol, the given output is generated Query1: Aggregate based on length window from fooStream#window.length(3) select symbol, sum(price) as totalPrice group by symbol insert into barStream; Output: symbol = wso2, totalPrice = 10.00 symbol = wso2, totalPrice = 30.00 symbol = abc, totalPrice = 30.00 symbol = abc, totalPrice = 70.00 symbol = abc, totalPrice = 120.00 For more information on group by, please refer to Siddhi Query Guide - Group By .","title":"Group By"},{"location":"guides/processing-Streaming-Events/#having","text":"Having allows to filter events after processing the select statement. This is useful if the filtering is based on some value derived by applying a function/ aggregation. For example, if you want to find all the events where maximum production total across 3 days is less than 1000 units, such filtering could be achieved with a query as follwos Having example from fooStream select item, maximum(productionOnDay1, productionOnDay2, productionOnDay3) as maxProduction having maxProduction 1000 insert into barStream; For more information on having clause, please refer to Siddhi Query Guide - Having .","title":"Having"},{"location":"guides/processing-Streaming-Events/#join","text":"Join is an important feature of Siddhi, which allows combining pair of streams, pair of windows, stream with window, stream/ window with a table and stream/window with an aggregation The join logic can be defined with 'on' condition as well, which restricts the events combined in a join. For example, assume that we need to combine a transaction stream with a table containing blacklisted credit card numbers, to identify fraudulent transactions. Following query helps achieve such a requirement. Join query example from transactionStream as t join blacklistedCardsTable as b on t.cardNumber = b.cardNumber select t.cardNumber, t.transactionDetails, b.fraudDescription insert into suspiciousTransactionStream; For more information on join queries, please refer to Siddhi Query Guide - Join .","title":"Join"},{"location":"guides/processing-Streaming-Events/#output-rate-limiting","text":"Output rate limiting allows queries to output events periodically based on a specified condition. This helps to limit continuously sending events as output. For more information on output rate limiting, please refer to Siddhi Query Guide - Output Rate Limiting","title":"Output Rate Limiting"},{"location":"guides/processing-Streaming-Events/#partitioning","text":"Partitioning in Siddhi allows to logically seperate events arriving at a stream, and to process them separately, in parallel. For example, assume that the total number of transactions per company needs to be monitored at a stock exchange. However, if all the transactions are arriving at a single stream, we would need to logically seperate them based on the company symbol. The following example depicts how this can be achieved with Siddhi partitioning. Partition example partition with (symbol of stockStream ) begin from stockStream select symbol, count() as transactionCount insert into transactionsPerSymbol; end; Partitioning can be done based on an attribute value as above, or based on a condition. For more information on partitioning, please refer to Siddhi Query Guide - Partitioning","title":"Partitioning"},{"location":"guides/processing-Streaming-Events/#trigger","text":"Triggers could be used to get events generated by the system itself, based on some time duration. An example for a trigger definition is as follows. Trigger example define trigger FiveMinTriggerStream at every 5 sec; This would generate an event every 5 seconds. The generated event would contain an attribute of type 'Long' named 'triggered_time', reflecting the time at which event was triggered in milliseconds (epoch time). Trigger could be defined as a time interval, a cron job or to generate an event when Siddhi is started. For more information on triggers, please refer to Siddhi Query Guide - Trigger","title":"Trigger"},{"location":"guides/processing-Streaming-Events/#script","text":"Scripts allow to define function operations in a different programming language. An example is as follows. Script example define function concatFn[javascript] return string { var str1 = data[0]; var str2 = data[1]; var str3 = data[2]; var response = str1 + str2 + str3; return response; }; define stream TempStream(deviceID long, roomNo int, temp double); from TempStream select concatFn(roomNo,'-',deviceID) as id, temp insert into DeviceTempStream; For more information on scripts, please refer to Siddhi Query Guide - Script","title":"Script"},{"location":"guides/publishing-Events/","text":"Publishing Events Once data is analyzed by WSO2 SP, the resulting data is output via a selected transport to the required interface in the required data format. In order to output results, a Siddhi application that processes events must have one or more sinks configured in it. A sink configuration specifies the following: Source types Event format Source types Each sink configuration must specify the transport type via which the events to be published by the Siddhi appliction should be published. The parameters to be configured for the transport differs based on the transport types. The following is the list of transport types supported by WSO2 SP. For detailed instructions to configure a sink of a specific transport type, click on the relevant link. HTTP Kafka TCP In-memory WSO2Event Email JMS File RabbitMQ MQTT Info A sink could also be defined externally, and referred to from several siddhi applications as described below. A \\ PROFILE> could refer to dashboard, editor, manager or worker. Multiple sinks can be defined in the SP HOME /conf/ PROFILE /deployment.yaml file. The following is a sample configuration of a sink. siddhi: refs: - ref: name: 'sink1' type: ' sink.type ' properties: property1 : value1 property2 : value2 You can refer to a source configured in the ` lt;SP HOME gt;/conf/ lt;PROFILE gt;/deployment.yaml ` file from a Siddhi application as shown in the example below. @Sink(ref='sink1', @map(type='json', @attributes( name='$.name', amount='$.quantity'))) define stream SweetProductionStream (name string, amount double); Event format The format in which the output events need to be published is specified as the mapping type in the sink configuration. The parameters related to mapping that need to be configured differ based on the mapping type. The following is a list of supported mapping types. For detailed instructions to configure each type, click on the relevant link. WSO2Event XML TEXT JSON Binary Key Value","title":"Publishing Events"},{"location":"guides/publishing-Events/#publishing-events","text":"Once data is analyzed by WSO2 SP, the resulting data is output via a selected transport to the required interface in the required data format. In order to output results, a Siddhi application that processes events must have one or more sinks configured in it. A sink configuration specifies the following: Source types Event format","title":"Publishing Events"},{"location":"guides/publishing-Events/#source-types","text":"Each sink configuration must specify the transport type via which the events to be published by the Siddhi appliction should be published. The parameters to be configured for the transport differs based on the transport types. The following is the list of transport types supported by WSO2 SP. For detailed instructions to configure a sink of a specific transport type, click on the relevant link. HTTP Kafka TCP In-memory WSO2Event Email JMS File RabbitMQ MQTT Info A sink could also be defined externally, and referred to from several siddhi applications as described below. A \\ PROFILE> could refer to dashboard, editor, manager or worker. Multiple sinks can be defined in the SP HOME /conf/ PROFILE /deployment.yaml file. The following is a sample configuration of a sink. siddhi: refs: - ref: name: 'sink1' type: ' sink.type ' properties: property1 : value1 property2 : value2 You can refer to a source configured in the ` lt;SP HOME gt;/conf/ lt;PROFILE gt;/deployment.yaml ` file from a Siddhi application as shown in the example below. @Sink(ref='sink1', @map(type='json', @attributes( name='$.name', amount='$.quantity'))) define stream SweetProductionStream (name string, amount double);","title":"Source types"},{"location":"guides/publishing-Events/#event-format","text":"The format in which the output events need to be published is specified as the mapping type in the sink configuration. The parameters related to mapping that need to be configured differ based on the mapping type. The following is a list of supported mapping types. For detailed instructions to configure each type, click on the relevant link. WSO2Event XML TEXT JSON Binary Key Value","title":"Event format"},{"location":"guides/storage-Integration/","text":"Storage Integration The following sections cover how storage of events is handled in WSO2 Stream Processor. Defining Data Tables Managing Stored Data via Streams Managing Stored Data via REST APIs Accessing and Manipulating Data in Multiple Tables","title":"Storage Integration"},{"location":"guides/storage-Integration/#storage-integration","text":"The following sections cover how storage of events is handled in WSO2 Stream Processor. Defining Data Tables Managing Stored Data via Streams Managing Stored Data via REST APIs Accessing and Manipulating Data in Multiple Tables","title":"Storage Integration"},{"location":"guides/transforming-data/","text":"","title":"Transforming Data"},{"location":"guides/triggering-integration-flows/","text":"","title":"Triggering Integration Flows"},{"location":"guides/working-with-micro-integrator/","text":"Working with the Micro Integrator","title":"Working with micro integrator"},{"location":"guides/working-with-micro-integrator/#working-with-the-micro-integrator","text":"","title":"Working with the Micro Integrator"},{"location":"overview/about_this_release/","text":"About this Release WSO2 EI 7.0.0 is the successor of WSO2 EI 6.5.0.","title":"About this Release"},{"location":"overview/about_this_release/#about-this-release","text":"WSO2 EI 7.0.0 is the successor of WSO2 EI 6.5.0.","title":"About this Release"},{"location":"overview/architecture/","text":"Architecture of Streaming Integrator Profile","title":"Architecture"},{"location":"overview/architecture/#architecture-of-streaming-integrator-profile","text":"","title":"Architecture of Streaming Integrator Profile"},{"location":"overview/overview/","text":"Overview of WSO2 Streaming Integrator The Streaming Integrator profile of WSO2 EI consumes streaming data, applies stream processing techniques to process this data, and then publishes the processed data to downstream applications reliably, or for the purpose of triggering an action. The streaming data can be consumed from applications, cloud, databases, files and from streams in multiple formats. The Streaming Integrator applies the Stream Processing logic of the Siddhi Query Language to process this data. The processed data is in turn can be published to multiple applications, clouds, databases, files and streams in multiple formarts. The main use cases addressed by the Streaming Integrator profile are as follows: Transforming Data This involves transforming ther data from one format to another (e.g., to/from XML, JSON, AVRO, etc.) Enriching data This involves enriching data received from a specific source by combining it with databases, services, and via inline calculations Creating joins This involves joining multiple data streams to create an aggregate stream. Cleansing data This involves cleaning data by filtering it and applying custom mappings. Deriving insights This involves deriving insights via Patterns and Sequences. Summarizing data. This involves summarizing data as an when it is generated using time windows or length windows.","title":"Overview"},{"location":"overview/overview/#overview-of-wso2-streaming-integrator","text":"The Streaming Integrator profile of WSO2 EI consumes streaming data, applies stream processing techniques to process this data, and then publishes the processed data to downstream applications reliably, or for the purpose of triggering an action. The streaming data can be consumed from applications, cloud, databases, files and from streams in multiple formats. The Streaming Integrator applies the Stream Processing logic of the Siddhi Query Language to process this data. The processed data is in turn can be published to multiple applications, clouds, databases, files and streams in multiple formarts. The main use cases addressed by the Streaming Integrator profile are as follows: Transforming Data This involves transforming ther data from one format to another (e.g., to/from XML, JSON, AVRO, etc.) Enriching data This involves enriching data received from a specific source by combining it with databases, services, and via inline calculations Creating joins This involves joining multiple data streams to create an aggregate stream. Cleansing data This involves cleaning data by filtering it and applying custom mappings. Deriving insights This involves deriving insights via Patterns and Sequences. Summarizing data. This involves summarizing data as an when it is generated using time windows or length windows.","title":"Overview of WSO2 Streaming Integrator"},{"location":"quick-start-guide/quick-start-guide/","text":"Quick Start Guide WSO2 Enterprise Integrator (WSO2 EI) is a comprehensive solution that allows you to seamlessly integrate applications, services, data, and business processes to support modern enterprise integration requirements. For this quick start guide, let's consider a basic Health Care System where WSO2 EI is used as the integration software. In this guide, an external party (a patient) wants to make a doctor's reservation at a given hospital. Before you begin, 1. Get a free trial subscription that enables you to download the product with the latest updates. Then, download the product installer from here , and run the installer. Download the back-end service and copy it to the EI_HOME /wso2/msf4j/deployment/microservices directory. The back-end service is now deployed in the MSF4J profile, which will run microservices for your integration flows. Start the MSF4J profile: Open a terminal and execute the following command: wso2ei-6.4.0-msf4j Go to Start Menu - Programs - WSO2 - Enterprise Integrator 6.4.0 MSF4J . This will open a terminal and start the MSF4J profile. If you are on a Windows OS, install cURL. For more information, see the cURL Releases and Downloads . Let's get started!","title":"Quick Start Guide"},{"location":"quick-start-guide/quick-start-guide/#quick-start-guide","text":"WSO2 Enterprise Integrator (WSO2 EI) is a comprehensive solution that allows you to seamlessly integrate applications, services, data, and business processes to support modern enterprise integration requirements. For this quick start guide, let's consider a basic Health Care System where WSO2 EI is used as the integration software. In this guide, an external party (a patient) wants to make a doctor's reservation at a given hospital. Before you begin, 1. Get a free trial subscription that enables you to download the product with the latest updates. Then, download the product installer from here , and run the installer. Download the back-end service and copy it to the EI_HOME /wso2/msf4j/deployment/microservices directory. The back-end service is now deployed in the MSF4J profile, which will run microservices for your integration flows. Start the MSF4J profile: Open a terminal and execute the following command: wso2ei-6.4.0-msf4j Go to Start Menu - Programs - WSO2 - Enterprise Integrator 6.4.0 MSF4J . This will open a terminal and start the MSF4J profile. If you are on a Windows OS, install cURL. For more information, see the cURL Releases and Downloads . Let's get started!","title":"Quick Start Guide"},{"location":"ref/authentication-APIs/","text":"Authentication APIs Log in to a dashboard application Log out of the dashboard application Redirect URL for login using authorization grant type Log in to a dashboard application Overview Overview Logs in to the apps in dashboard runtime such as portal, monitoring or business-rules app. API Context /login/{appName} HTTP Method POST Request/Response Format application/x-www-form-urlencoded Runtime Dashboard Parameter description Parameter Type Description Possible Values appName Path param The application to which you need to log in. portal/monitoring/business-rules username Body param Username for the login password Body param Password for the login grantType Body param Grant type used for the login password/ refresh_token authorization_code rememberMe Body param Whether remember me function enabled false/true curl command syntax curl -X POST \"https://analytics.wso2.com/login/{appName}\" -H \"accept: application/json\" -H \"Content-Type: application/x-www-form-urlencoded\" -d \"username={username} password={password} grantType={grantTypr} rememberMe={rememberMe}\" Sample curl command curl -X POST \"https://localhost:9643/login/portal\" -H \"Content-Type: application/x-www-form-urlencoded\" -d \"username=admin password=admin grantType=password\" Sample output {\"authUser\":\"admin\",\"pID\":\"71368eff-cc71-44ef\",\"lID\":\"a60c1098-3de0-42fb\",\"validityPeriod\":3600} Response HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Log out of the dashboard application Overview Overview Logs out of the dashboard application. API Context /logout/{appName} HTTP Method POST Request/Response Format application/json Runtime Dashboard curl command syntax curl -X POST \"https://analytics.wso2.com/logout/{appName}\" -H \"accept: application/json\" -H \"Authorzation: Bearer {access token}\" Sample curl command curl -X POST \"https://analytics.wso2.com/logout/portal\" -H \"accept: application/json\" -H \"Authorzation: Bearer 123456\" Sample output N/A Response HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Redirect URL for login using authorization grant type Overview Overview Redirects URL by the IS in authorization grant type - OAuth2. API Context /login/callback/{appName} HTTP Method GET Request/Response Format JSON Runtime Dashbaord Parameter description Parameter Description {appName} The application of which the URL needs to be redirected. curl command syntax Sample curl command curl -X GET \"https://localhost:9643/login/callback/portal\" Sample output Response HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Authentication APIs"},{"location":"ref/authentication-APIs/#authentication-apis","text":"Log in to a dashboard application Log out of the dashboard application Redirect URL for login using authorization grant type","title":"Authentication APIs"},{"location":"ref/authentication-APIs/#log-in-to-a-dashboard-application","text":"","title":"Log in to a dashboard application"},{"location":"ref/authentication-APIs/#overview","text":"Overview Logs in to the apps in dashboard runtime such as portal, monitoring or business-rules app. API Context /login/{appName} HTTP Method POST Request/Response Format application/x-www-form-urlencoded Runtime Dashboard","title":"Overview"},{"location":"ref/authentication-APIs/#parameter-description","text":"Parameter Type Description Possible Values appName Path param The application to which you need to log in. portal/monitoring/business-rules username Body param Username for the login password Body param Password for the login grantType Body param Grant type used for the login password/ refresh_token authorization_code rememberMe Body param Whether remember me function enabled false/true","title":"Parameter description"},{"location":"ref/authentication-APIs/#curl-command-syntax","text":"curl -X POST \"https://analytics.wso2.com/login/{appName}\" -H \"accept: application/json\" -H \"Content-Type: application/x-www-form-urlencoded\" -d \"username={username} password={password} grantType={grantTypr} rememberMe={rememberMe}\"","title":"curl command syntax"},{"location":"ref/authentication-APIs/#sample-curl-command","text":"curl -X POST \"https://localhost:9643/login/portal\" -H \"Content-Type: application/x-www-form-urlencoded\" -d \"username=admin password=admin grantType=password\"","title":"Sample curl command"},{"location":"ref/authentication-APIs/#sample-output","text":"{\"authUser\":\"admin\",\"pID\":\"71368eff-cc71-44ef\",\"lID\":\"a60c1098-3de0-42fb\",\"validityPeriod\":3600}","title":"Sample output"},{"location":"ref/authentication-APIs/#response","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/authentication-APIs/#log-out-of-the-dashboard-application","text":"","title":"Log out of the dashboard application"},{"location":"ref/authentication-APIs/#overview_1","text":"Overview Logs out of the dashboard application. API Context /logout/{appName} HTTP Method POST Request/Response Format application/json Runtime Dashboard","title":"Overview"},{"location":"ref/authentication-APIs/#curl-command-syntax_1","text":"curl -X POST \"https://analytics.wso2.com/logout/{appName}\" -H \"accept: application/json\" -H \"Authorzation: Bearer {access token}\"","title":"curl command syntax"},{"location":"ref/authentication-APIs/#sample-curl-command_1","text":"curl -X POST \"https://analytics.wso2.com/logout/portal\" -H \"accept: application/json\" -H \"Authorzation: Bearer 123456\"","title":"Sample curl command"},{"location":"ref/authentication-APIs/#sample-output_1","text":"N/A","title":"Sample output"},{"location":"ref/authentication-APIs/#response_1","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/authentication-APIs/#redirect-url-for-login-using-authorization-grant-type","text":"","title":"Redirect URL for login using authorization grant type"},{"location":"ref/authentication-APIs/#overview_2","text":"Overview Redirects URL by the IS in authorization grant type - OAuth2. API Context /login/callback/{appName} HTTP Method GET Request/Response Format JSON Runtime Dashbaord","title":"Overview"},{"location":"ref/authentication-APIs/#parameter-description_1","text":"Parameter Description {appName} The application of which the URL needs to be redirected.","title":"Parameter description"},{"location":"ref/authentication-APIs/#curl-command-syntax_2","text":"","title":"curl command syntax"},{"location":"ref/authentication-APIs/#sample-curl-command_2","text":"curl -X GET \"https://localhost:9643/login/callback/portal\"","title":"Sample curl command"},{"location":"ref/authentication-APIs/#sample-output_2","text":"","title":"Sample output"},{"location":"ref/authentication-APIs/#response_2","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/authorization-Permission-Model/","text":"Authorization Permission Model This page will provide information about the permission model of REST APIs available in each runtime of WSO2 Stream Processor. If you want the complete set of REST APIs which available in WSO2 SP, please find it here . You can find the REST APIs for each runtime and its permission model in the below pages. Worker Runtime - REST APIs Permission Model Manager Runtime - REST APIs Permission Model","title":"Authorization & Permission Model"},{"location":"ref/authorization-Permission-Model/#authorization-permission-model","text":"This page will provide information about the permission model of REST APIs available in each runtime of WSO2 Stream Processor. If you want the complete set of REST APIs which available in WSO2 SP, please find it here . You can find the REST APIs for each runtime and its permission model in the below pages. Worker Runtime - REST APIs Permission Model Manager Runtime - REST APIs Permission Model","title":"Authorization &amp; Permission Model"},{"location":"ref/business-Rules-APIs/","text":"Business Rules APIs Lists available business rule instances Delete business rule with given UUID Fetch template group with the given UUID Fetch rule templates of the template group with given UUID Fetch rule template of specific UUID available under a template group with specific UUID Fetch available template groups Fetch business rule instance with given UUID Create and save a business rule Update business rules instance with given UUID Lists available business rule instances Overview Description Returns the list of business rule instances that are currently available. API Context /business-rules/instances HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard curl command syntax Sample curl command curl -X GET \"https://localhost:9643/business-rules/instances\" -u admin:admin -k Sample output Response HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Delete business rule with given UUID Overview Description Deletes the business rule with the given UUID. API Context /business-rules/instances/{businessRuleInstanceID}?force-delete=false HTTP Method DELETE Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard Parameter description Parameter Description {businessRuleInstanceID} The UUID (Uniquely Identifiable ID) of the business rules instance to be deleted. curl command syntax Sample curl command curl -X DELETE \"https://localhost:9643/business-rules/instances/business-rule-1?force-delete=false\" -H \"accept: application/json\" -u admin:adm Sample output Response HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Fetch template group with the given UUID Overview Description Returns the template group that has the given UUID. API Context /business-rules/template-groups/{templateGroupID} HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard Parameter description Parameter Description {templateGroupID} The UUID of the template group to be fetched. curl command syntax Sample curl command curl -X GET \"https://localhost:9643/business-rules/template-groups/sweet-factory\" -u admin:admin -k Sample output Response HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Fetch rule templates of the template group with given UUID Overview Description Returns the rule templates of the template group with the given UUID. API Context /business-rules/template-groups/{templateGroupID}/templates HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard Parameter description Parameter Description {templateGroupID} The UUID of the template group of which the rule templates need to be fetched. curl command syntax Sample curl command curl -X GET \"https://localhost:9643/business-rules/template-groups/sweet-factory/templates\" -u admin:admin -k Sample output Response HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Fetch rule template of specific UUID available under a template group with specific UUID Overview Description Returns the rule template with the specified UUID that is defined under the template group with the specified UUID. API Context /business-rules /template-groups/{templateGroupID}/templates/{ruleTemplateID} HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard Parameter description Parameter Description {templateGroupID} The UUID of the template group from which the specified rule template needs to be retrieved. {ruleTemplateID} The UUID of the rule template that needs to be retrieved from the specified template group. curl command syntax Sample curl command curl -X GET \"https://localhost:9643/business-rules/template-groups/sweet-factory/templates/identifying-continuous-production-decrease\" -u admin:admin -k Sample output Response HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Fetch available template groups Overview Description Returns all the template groups that are currently available in the SP setup. API Context /business-rules/template-groups HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard curl command syntax Sample curl command curl -X GET \"https://localhost:9643/business-rules/template-groups\" -u admin:admin -k Sample output Response HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Fetch business rule instance with given UUID Overview Description Returns the business rule instance with the given UUID. API Context /business-rules/instances/{businessRuleInstanceID} HTTP Method GET Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard Parameter description Parameter Description {businessRuleInstanceID} The UUID of the business rules instance to be fetched. curl command syntax Sample curl command curl -X GET \"https://localhost:9643/business-rules/instances/business-rule-1\" -H \"accept: application/json\" -u admin:admin -k Sample output Response HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Create and save a business rule Overview Description Creates and saves a business rule. API Context /business-rules /instances?deploy={deploymentStatus} HTTP Method POST Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard Parameter description Parameter Description {deploymentStatus} curl command syntax Sample curl command curl -X POST \"https://localhost:9643/business-rules/instances?deploy=true\" -H \"accept: application/json\" -H \"content-type: multipart/form-data\" -F 'businessRule={\"name\":\"Business Rule 5\",\"uuid\":\"business-rule-5\",\"type\":\"template\",\"templateGroupUUID\":\"sweet-factory\",\"ruleTemplateUUID\":\"identifying-continuous-production-decrease\",\"properties\":{\"timeInterval\":\"6\",\"timeRangeInput\":\"5\",\"email\":\"example@email.com\"}}' -u admin:admin -k Sample output Response HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Update business rules instance with given UUID Overview Description Updates the business rules instance with the given UUID. API Context /business-rules /instances/{businessRuleInstanceID}?deploy={deploymentStatus} HTTP Method PUT Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard Parameter description Parameter Description {businessRuleInstanceID} The UUID of the business rules instance to be updated. {deploymentStatus} curl command syntax Sample curl command curl -X PUT \"https://localhost:9643/business-rules/instances/business-rule-5?deploy=true\" -H \"accept: application/json\" -H \"content-type: application/json\" -d '{\"name\":\"Business Rule 5\",\"uuid\":\"business-rule-5\",\"type\":\"template\",\"templateGroupUUID\":\"sweet-factory\",\"ruleTemplateUUID\":\"identifying-continuous-production-decrease\",\"properties\":{\"timeInterval\":\"9\",\"timeRangeInput\":\"8\",\"email\":\"newexample@email.com\"}}' -u admin:admin -k Sample output Response HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Business Rules APIs"},{"location":"ref/business-Rules-APIs/#business-rules-apis","text":"Lists available business rule instances Delete business rule with given UUID Fetch template group with the given UUID Fetch rule templates of the template group with given UUID Fetch rule template of specific UUID available under a template group with specific UUID Fetch available template groups Fetch business rule instance with given UUID Create and save a business rule Update business rules instance with given UUID","title":"Business Rules APIs"},{"location":"ref/business-Rules-APIs/#lists-available-business-rule-instances","text":"","title":"Lists available business rule instances"},{"location":"ref/business-Rules-APIs/#overview","text":"Description Returns the list of business rule instances that are currently available. API Context /business-rules/instances HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/business-Rules-APIs/#curl-command-syntax","text":"","title":"curl command syntax"},{"location":"ref/business-Rules-APIs/#sample-curl-command","text":"curl -X GET \"https://localhost:9643/business-rules/instances\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/business-Rules-APIs/#sample-output","text":"","title":"Sample output"},{"location":"ref/business-Rules-APIs/#response","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/business-Rules-APIs/#delete-business-rule-with-given-uuid","text":"","title":"Delete business rule with given UUID"},{"location":"ref/business-Rules-APIs/#overview_1","text":"Description Deletes the business rule with the given UUID. API Context /business-rules/instances/{businessRuleInstanceID}?force-delete=false HTTP Method DELETE Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/business-Rules-APIs/#parameter-description","text":"Parameter Description {businessRuleInstanceID} The UUID (Uniquely Identifiable ID) of the business rules instance to be deleted.","title":"Parameter description"},{"location":"ref/business-Rules-APIs/#curl-command-syntax_1","text":"","title":"curl command syntax"},{"location":"ref/business-Rules-APIs/#sample-curl-command_1","text":"curl -X DELETE \"https://localhost:9643/business-rules/instances/business-rule-1?force-delete=false\" -H \"accept: application/json\" -u admin:adm","title":"Sample curl command"},{"location":"ref/business-Rules-APIs/#sample-output_1","text":"","title":"Sample output"},{"location":"ref/business-Rules-APIs/#response_1","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/business-Rules-APIs/#fetch-template-group-with-the-given-uuid","text":"","title":"Fetch template group with the given UUID"},{"location":"ref/business-Rules-APIs/#overview_2","text":"Description Returns the template group that has the given UUID. API Context /business-rules/template-groups/{templateGroupID} HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/business-Rules-APIs/#parameter-description_1","text":"Parameter Description {templateGroupID} The UUID of the template group to be fetched.","title":"Parameter description"},{"location":"ref/business-Rules-APIs/#curl-command-syntax_2","text":"","title":"curl command syntax"},{"location":"ref/business-Rules-APIs/#sample-curl-command_2","text":"curl -X GET \"https://localhost:9643/business-rules/template-groups/sweet-factory\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/business-Rules-APIs/#sample-output_2","text":"","title":"Sample output"},{"location":"ref/business-Rules-APIs/#response_2","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/business-Rules-APIs/#fetch-rule-templates-of-the-template-group-with-given-uuid","text":"","title":"Fetch rule templates of the template group with given UUID"},{"location":"ref/business-Rules-APIs/#overview_3","text":"Description Returns the rule templates of the template group with the given UUID. API Context /business-rules/template-groups/{templateGroupID}/templates HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/business-Rules-APIs/#parameter-description_2","text":"Parameter Description {templateGroupID} The UUID of the template group of which the rule templates need to be fetched.","title":"Parameter description"},{"location":"ref/business-Rules-APIs/#curl-command-syntax_3","text":"","title":"curl command syntax"},{"location":"ref/business-Rules-APIs/#sample-curl-command_3","text":"curl -X GET \"https://localhost:9643/business-rules/template-groups/sweet-factory/templates\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/business-Rules-APIs/#sample-output_3","text":"","title":"Sample output"},{"location":"ref/business-Rules-APIs/#response_3","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/business-Rules-APIs/#fetch-rule-template-of-specific-uuid-available-under-a-template-group-with-specific-uuid","text":"","title":"Fetch rule template of specific UUID available under a template group with specific UUID"},{"location":"ref/business-Rules-APIs/#overview_4","text":"Description Returns the rule template with the specified UUID that is defined under the template group with the specified UUID. API Context /business-rules /template-groups/{templateGroupID}/templates/{ruleTemplateID} HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/business-Rules-APIs/#parameter-description_3","text":"Parameter Description {templateGroupID} The UUID of the template group from which the specified rule template needs to be retrieved. {ruleTemplateID} The UUID of the rule template that needs to be retrieved from the specified template group.","title":"Parameter description"},{"location":"ref/business-Rules-APIs/#curl-command-syntax_4","text":"","title":"curl command syntax"},{"location":"ref/business-Rules-APIs/#sample-curl-command_4","text":"curl -X GET \"https://localhost:9643/business-rules/template-groups/sweet-factory/templates/identifying-continuous-production-decrease\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/business-Rules-APIs/#sample-output_4","text":"","title":"Sample output"},{"location":"ref/business-Rules-APIs/#response_4","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/business-Rules-APIs/#fetch-available-template-groups","text":"","title":"Fetch available template groups"},{"location":"ref/business-Rules-APIs/#overview_5","text":"Description Returns all the template groups that are currently available in the SP setup. API Context /business-rules/template-groups HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/business-Rules-APIs/#curl-command-syntax_5","text":"","title":"curl command syntax"},{"location":"ref/business-Rules-APIs/#sample-curl-command_5","text":"curl -X GET \"https://localhost:9643/business-rules/template-groups\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/business-Rules-APIs/#sample-output_5","text":"","title":"Sample output"},{"location":"ref/business-Rules-APIs/#response_5","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/business-Rules-APIs/#fetch-business-rule-instance-with-given-uuid","text":"","title":"Fetch business rule instance with given UUID"},{"location":"ref/business-Rules-APIs/#overview_6","text":"Description Returns the business rule instance with the given UUID. API Context /business-rules/instances/{businessRuleInstanceID} HTTP Method GET Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/business-Rules-APIs/#parameter-description_4","text":"Parameter Description {businessRuleInstanceID} The UUID of the business rules instance to be fetched.","title":"Parameter description"},{"location":"ref/business-Rules-APIs/#curl-command-syntax_6","text":"","title":"curl command syntax"},{"location":"ref/business-Rules-APIs/#sample-curl-command_6","text":"curl -X GET \"https://localhost:9643/business-rules/instances/business-rule-1\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/business-Rules-APIs/#sample-output_6","text":"","title":"Sample output"},{"location":"ref/business-Rules-APIs/#response_6","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/business-Rules-APIs/#create-and-save-a-business-rule","text":"","title":"Create and save a business rule"},{"location":"ref/business-Rules-APIs/#overview_7","text":"Description Creates and saves a business rule. API Context /business-rules /instances?deploy={deploymentStatus} HTTP Method POST Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/business-Rules-APIs/#parameter-description_5","text":"Parameter Description {deploymentStatus}","title":"Parameter description"},{"location":"ref/business-Rules-APIs/#curl-command-syntax_7","text":"","title":"curl command syntax"},{"location":"ref/business-Rules-APIs/#sample-curl-command_7","text":"curl -X POST \"https://localhost:9643/business-rules/instances?deploy=true\" -H \"accept: application/json\" -H \"content-type: multipart/form-data\" -F 'businessRule={\"name\":\"Business Rule 5\",\"uuid\":\"business-rule-5\",\"type\":\"template\",\"templateGroupUUID\":\"sweet-factory\",\"ruleTemplateUUID\":\"identifying-continuous-production-decrease\",\"properties\":{\"timeInterval\":\"6\",\"timeRangeInput\":\"5\",\"email\":\"example@email.com\"}}' -u admin:admin -k","title":"Sample curl command"},{"location":"ref/business-Rules-APIs/#sample-output_7","text":"","title":"Sample output"},{"location":"ref/business-Rules-APIs/#response_7","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/business-Rules-APIs/#update-business-rules-instance-with-given-uuid","text":"","title":"Update business rules instance with given UUID"},{"location":"ref/business-Rules-APIs/#overview_8","text":"Description Updates the business rules instance with the given UUID. API Context /business-rules /instances/{businessRuleInstanceID}?deploy={deploymentStatus} HTTP Method PUT Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Dashboard","title":"Overview"},{"location":"ref/business-Rules-APIs/#parameter-description_6","text":"Parameter Description {businessRuleInstanceID} The UUID of the business rules instance to be updated. {deploymentStatus}","title":"Parameter description"},{"location":"ref/business-Rules-APIs/#curl-command-syntax_8","text":"","title":"curl command syntax"},{"location":"ref/business-Rules-APIs/#sample-curl-command_8","text":"curl -X PUT \"https://localhost:9643/business-rules/instances/business-rule-5?deploy=true\" -H \"accept: application/json\" -H \"content-type: application/json\" -d '{\"name\":\"Business Rule 5\",\"uuid\":\"business-rule-5\",\"type\":\"template\",\"templateGroupUUID\":\"sweet-factory\",\"ruleTemplateUUID\":\"identifying-continuous-production-decrease\",\"properties\":{\"timeInterval\":\"9\",\"timeRangeInput\":\"8\",\"email\":\"newexample@email.com\"}}' -u admin:admin -k","title":"Sample curl command"},{"location":"ref/business-Rules-APIs/#sample-output_8","text":"","title":"Sample output"},{"location":"ref/business-Rules-APIs/#response_8","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/configuring-default-ports/","text":"Configuring Default Ports This page describes the default ports that are used for each runtime when the port offset is 0 . Common Ports Worker Runtime Manager Runtime Dashboard Runtime Editor Runtime Clustering Ports Distributed deployment: Manager Node: Worker Node(Resource Node): Minimum High Availability (HA) Deployment: Worker Node: Multi Datacenter High Availability Deployment Common Ports The following ports are common to all runtimes. 7611 Thrift TCP port to receive events from clients. 7711 Thrift SSL port for secure transport where the client is authenticated. 9611 Binary TCP port to receive events from clients. 9711 Binary SSL port for secure transport where the client is authenticated. You can offset binary and thrift by configuring the offset parameter in the SP_HOME /conf/ PROFILE /deployment.yaml file. The following is a sample configuration. # Carbon Configuration Parameters wso2.carbon: # value to uniquely identify a server id: wso2-sp # server name name: WSO2 Stream Processor # server type # type: wso2-sp # ports used by this server ports: # port offset offset: 1 Worker Runtime 9090 HTTP netty transport 9443 HTTPS netty transport Manager Runtime 9190 HTTP netty transport 9543 HTTPS netty transport Dashboard Runtime 9290 HTTP netty transport 9643 HTTPS netty transport Editor Runtime 9390 HTTP netty transport 9743 HTTPS netty transport Tip The following example shows how to overide the default netty port for the Editor profile by updating the required parameters in the SP_HOME /conf/editor/deployment.yaml file. wso2.transport.http: transportProperties: listenerConfigurations: - id: \"default\" port: 9390 - id: \"msf4j-https\" port: 9743 Clustering Ports Ports that are required for clustering deployment: Distributed deployment: Manager Node: 9190 HTTP netty transport. 9190 Specify the port of the node for the httpInterface parameter in the cluster.config section . The HTTP netty transport port is considered the default port for HTTP interface. 9543 HTTPS netty transport. 9092 The Kafka Server bootstrapURLs Port. This is the default port where the external kafka server starts and the manager communicates. 2181 Zookeeper Server zooKeeperURLs Port. This is the default port where the external zookeeper server starts and the manager communicates. Worker Node(Resource Node): 9090 HTTP netty transport. 9090 Specify the port of the node for the httpInterface parameter in the cluster.config section. The HTTP netty transport port is considered the default port for HTTP interface. 9191 Specify the port of the resource manager for the resourceManagers parameter in the cluster.config section. The netty transport port of the manager node is considered the default port. 9443 HTTPS netty transport. Minimum High Availability (HA) Deployment: Worker Node: 9090 HTTP netty transport. 9090 Specify the port of the node for the advertisedPort parameter in the liveSync section. The HTTP netty transport port is considered the default port. 9443 HTTPS netty transport. Multi Datacenter High Availability Deployment Other than the ports used in clustering setups (i.e., a Minimum HA Deployment or a Fully Distributed Deployment) , the following is required: 9092 Ports of the two separate instances of the broker deployed in each data center. (e.g., bootstrap.servers= 'host1:9092, host2:9092', default 9092 where the external kafka servers start )","title":"Configuring Default Ports"},{"location":"ref/configuring-default-ports/#configuring-default-ports","text":"This page describes the default ports that are used for each runtime when the port offset is 0 . Common Ports Worker Runtime Manager Runtime Dashboard Runtime Editor Runtime Clustering Ports Distributed deployment: Manager Node: Worker Node(Resource Node): Minimum High Availability (HA) Deployment: Worker Node: Multi Datacenter High Availability Deployment","title":"Configuring Default Ports"},{"location":"ref/configuring-default-ports/#common-ports","text":"The following ports are common to all runtimes. 7611 Thrift TCP port to receive events from clients. 7711 Thrift SSL port for secure transport where the client is authenticated. 9611 Binary TCP port to receive events from clients. 9711 Binary SSL port for secure transport where the client is authenticated. You can offset binary and thrift by configuring the offset parameter in the SP_HOME /conf/ PROFILE /deployment.yaml file. The following is a sample configuration. # Carbon Configuration Parameters wso2.carbon: # value to uniquely identify a server id: wso2-sp # server name name: WSO2 Stream Processor # server type # type: wso2-sp # ports used by this server ports: # port offset offset: 1","title":"Common Ports"},{"location":"ref/configuring-default-ports/#worker-runtime","text":"9090 HTTP netty transport 9443 HTTPS netty transport","title":"Worker Runtime"},{"location":"ref/configuring-default-ports/#manager-runtime","text":"9190 HTTP netty transport 9543 HTTPS netty transport","title":"Manager Runtime"},{"location":"ref/configuring-default-ports/#dashboard-runtime","text":"9290 HTTP netty transport 9643 HTTPS netty transport","title":"Dashboard Runtime"},{"location":"ref/configuring-default-ports/#editor-runtime","text":"9390 HTTP netty transport 9743 HTTPS netty transport Tip The following example shows how to overide the default netty port for the Editor profile by updating the required parameters in the SP_HOME /conf/editor/deployment.yaml file. wso2.transport.http: transportProperties: listenerConfigurations: - id: \"default\" port: 9390 - id: \"msf4j-https\" port: 9743","title":"Editor Runtime"},{"location":"ref/configuring-default-ports/#clustering-ports","text":"Ports that are required for clustering deployment:","title":"Clustering Ports"},{"location":"ref/configuring-default-ports/#distributed-deployment","text":"","title":"Distributed deployment:"},{"location":"ref/configuring-default-ports/#manager-node","text":"9190 HTTP netty transport. 9190 Specify the port of the node for the httpInterface parameter in the cluster.config section . The HTTP netty transport port is considered the default port for HTTP interface. 9543 HTTPS netty transport. 9092 The Kafka Server bootstrapURLs Port. This is the default port where the external kafka server starts and the manager communicates. 2181 Zookeeper Server zooKeeperURLs Port. This is the default port where the external zookeeper server starts and the manager communicates.","title":"Manager Node:"},{"location":"ref/configuring-default-ports/#worker-noderesource-node","text":"9090 HTTP netty transport. 9090 Specify the port of the node for the httpInterface parameter in the cluster.config section. The HTTP netty transport port is considered the default port for HTTP interface. 9191 Specify the port of the resource manager for the resourceManagers parameter in the cluster.config section. The netty transport port of the manager node is considered the default port. 9443 HTTPS netty transport.","title":"Worker Node(Resource Node):"},{"location":"ref/configuring-default-ports/#minimum-high-availability-ha-deployment","text":"","title":"Minimum High Availability (HA) Deployment:"},{"location":"ref/configuring-default-ports/#worker-node","text":"9090 HTTP netty transport. 9090 Specify the port of the node for the advertisedPort parameter in the liveSync section. The HTTP netty transport port is considered the default port. 9443 HTTPS netty transport.","title":"Worker Node:"},{"location":"ref/configuring-default-ports/#multi-datacenter-high-availability-deployment","text":"Other than the ports used in clustering setups (i.e., a Minimum HA Deployment or a Fully Distributed Deployment) , the following is required: 9092 Ports of the two separate instances of the broker deployed in each data center. (e.g., bootstrap.servers= 'host1:9092, host2:9092', default 9092 where the external kafka servers start )","title":"Multi Datacenter High Availability Deployment"},{"location":"ref/hTTP-Status-Codes/","text":"HTTP Status Codes When REST API requests are sent to carryout various actions, various HTTP status codes will be returned based on the state of the action (success or failure) and the HTTP method ( POST, GET, PUT, DELETE ) executed. The following are the definitions of the various HTTP status codes that are returned. Success HTTP status codes Error HTTP status codes Success HTTP status codes Code Code Summary Description 200 Ok HTTP request was successful. The output corresponding to the HTTP request will be returned. Generally used as a response to a successful GET and PUT REST API HTTP methods. 201 Created HTTP request was successfully processed and a new resource was created. Generally used as a response to a successful POST REST API HTTP method. 204 No content HTTP request was successfully processed. No content will be returned. Generally used as a response to a successful DELETE REST API HTTP method. 202 Accepted HTTP request was accepted for processing, but the processing has not been completed. This generally occurs when your successful in trying to undeploy an application. Error HTTP status codes Code Code Summary Description 404 Not found Requested resource not found. Generally used as a response for unsuccessful GET and PUT REST API HTTP methods. 409 Conflict Request could not be processed because of conflict in the request. This generally occurs when you are trying to add a resource that already exists. For example, when trying to add an auto-scaling policy that has an already existing ID. 500 Internal server error Server error occurred.","title":"HTTP Status Codes"},{"location":"ref/hTTP-Status-Codes/#http-status-codes","text":"When REST API requests are sent to carryout various actions, various HTTP status codes will be returned based on the state of the action (success or failure) and the HTTP method ( POST, GET, PUT, DELETE ) executed. The following are the definitions of the various HTTP status codes that are returned. Success HTTP status codes Error HTTP status codes","title":"HTTP Status Codes"},{"location":"ref/hTTP-Status-Codes/#success-http-status-codes","text":"Code Code Summary Description 200 Ok HTTP request was successful. The output corresponding to the HTTP request will be returned. Generally used as a response to a successful GET and PUT REST API HTTP methods. 201 Created HTTP request was successfully processed and a new resource was created. Generally used as a response to a successful POST REST API HTTP method. 204 No content HTTP request was successfully processed. No content will be returned. Generally used as a response to a successful DELETE REST API HTTP method. 202 Accepted HTTP request was accepted for processing, but the processing has not been completed. This generally occurs when your successful in trying to undeploy an application.","title":"Success HTTP status codes"},{"location":"ref/hTTP-Status-Codes/#error-http-status-codes","text":"Code Code Summary Description 404 Not found Requested resource not found. Generally used as a response for unsuccessful GET and PUT REST API HTTP methods. 409 Conflict Request could not be processed because of conflict in the request. This generally occurs when you are trying to add a resource that already exists. For example, when trying to add an auto-scaling policy that has an already existing ID. 500 Internal server error Server error occurred.","title":"Error HTTP status codes"},{"location":"ref/healthcheck-APIs/","text":"Healthcheck APIs Overview Description API Context HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime curl command syntax Sample curl command curl -k -X GET http://localhost:9090/health Sample output Response HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Healthcheck APIs"},{"location":"ref/healthcheck-APIs/#healthcheck-apis","text":"","title":"Healthcheck APIs"},{"location":"ref/healthcheck-APIs/#overview","text":"Description API Context HTTP Method GET Request/Response Format Authentication Basic Username admin Password admin Runtime","title":"Overview"},{"location":"ref/healthcheck-APIs/#curl-command-syntax","text":"","title":"curl command syntax"},{"location":"ref/healthcheck-APIs/#sample-curl-command","text":"curl -k -X GET http://localhost:9090/health","title":"Sample curl command"},{"location":"ref/healthcheck-APIs/#sample-output","text":"","title":"Sample output"},{"location":"ref/healthcheck-APIs/#response","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/performance-analysis-results/","text":"Performance Analysis Results","title":"Performance Analysis Results"},{"location":"ref/performance-analysis-results/#performance-analysis-results","text":"","title":"Performance Analysis Results"},{"location":"ref/public-APIs/","text":"Public APIs The following topics list the APIs supported for WSO2 Stream processor from its Worker, Manager, Editor and Dashboard runtimes. Siddhi Application Management APIs Event Simulation APIs Status Monitoring APIs Dashboard APIs Authentication APIs Permission APIs Business Rules APIs Store APIs Healthcheck APIs","title":"Public APIs"},{"location":"ref/public-APIs/#public-apis","text":"The following topics list the APIs supported for WSO2 Stream processor from its Worker, Manager, Editor and Dashboard runtimes. Siddhi Application Management APIs Event Simulation APIs Status Monitoring APIs Dashboard APIs Authentication APIs Permission APIs Business Rules APIs Store APIs Healthcheck APIs","title":"Public APIs"},{"location":"ref/si-profiles/","text":"Streaming Integrator Profiles","title":"Si profiles"},{"location":"ref/si-profiles/#streaming-integrator-profiles","text":"","title":"Streaming Integrator Profiles"},{"location":"ref/si-rest-api-guide/","text":"Streaming Integrator REST API Guide","title":"Streaming Integration REST API Guide"},{"location":"ref/si-rest-api-guide/#streaming-integrator-rest-api-guide","text":"","title":"Streaming Integrator REST API Guide"},{"location":"ref/siddhi-Application-Management-APIs/","text":"Siddhi Application Management APIs Updating a Siddhi Application Deleting a Siddhi application Listing all active Siddhi applications Retrieving a specific Siddhi application Fetching the status of a Siddhi Application Taking a snapshot of a Siddhi Application Restoring a Siddhi Application via a snapshot Returning real-time statistics of a worker Enabling/disabling worker statistics Returning general details of a worker Returning detailed statistics of all Siddhi applications Enabling/disabling the statistics of a specific Siddhi application Enabling/disabling the statistics of all Siddhi applications Creating a Siddhi application Overview Description Creates a new Siddhi Application. API Context /siddhi-apps HTTP Method POST Request/Response format Request : text/plain Response : application/json Authentication Basic Username admin Password admin Runtime worker/manager curl command syntax curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @TestSiddhiApp.siddhi -u admin:admin -k Sample curl command curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @TestSiddhiApp.siddhi -u admin:admin -k Sample output The response for the sample curl command given above can be one of the following. If API request is valid and there is no existing Siddhi application with the given name, a response similar to the following is generated with response code 201. This response contains a location header with the path of the newly created file from product root home. If the API request is valid, but a Siddhi application with the given name already exists, a response similar to the following is generated with response code 409. { \"type\": \"conflict\", \"message\": \"There is a Siddhi App already exists with same name\" } If the API request is invalid due to invalid content in the Siddhi queries you have included in the request body, a response similar to the following is generated is generated with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"You have an error in your SiddhiQL at line 8:8, missing INTO at 'BarStream'\" } If the API request is valid, but an exception occured during file processing or saving, the following response is generated with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": error-message } Response HTTP Status Code Possible codes are 201, 409, 400, and 500. For descriptions of the HTTP status codes, see HTTP Status Codes . Updating a Siddhi Application Overview Description Updates a Siddhi Application. API Context /siddhi-apps HTTP Method PUT Request/Response format Request : text/plain Response : application/json Authentication Basic Username admin Password admin Runtime worker/manager curl command syntax curl -X PUT \"http://localhost:9090/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @ SIDDHI_APPLICATION_NAME .siddhi -u admin:admin -k Sample curl command curl -X PUT \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @TestSiddhiApp.siddhi -u admin:admin -k Sample output If the API request is invalid due to invalid content in the Siddhi query, a response similar to the following is returned with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"You have an error in your SiddhiQL at line 8:8, missing INTO at 'BarStream'\" } If the API request is valid, but an exception occured when saving or processing files, a response similar to the following is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": error-message } Response HTTP Status Code Possible codes are 200, 201, 400, and 500. For descriptions of the HTTP status codes, see HTTP Status Codes . Deleting a Siddhi application Overview Description Sends the name of a Siddhi application as a URL parameter. API Context /siddhi-apps/{appName} HTTP Method DELETE Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description Parameter Description {appName} The name of the Siddhi application to be deleted. curl command syntax curl -X DELETE \"http://localhost:9090/siddhi-apps/{app-name}\" -H \"accept: application/json\" -u admin:admin -k Sample curl command curl -X DELETE \"https://localhost:9443/siddhi-apps/TestSiddhiApp\" -H \"accept: application/json\" -u admin:admin -k Sample output The respose for the sample curl command given above can be one of the following: If the API request is valid and a Siddhi application with the given name exists, the following response is received with response code 200. http://localhost:9090/siddhi-apps/TestExecutionPlan1 If the API request is valid, but a Siddhi application with the given name is not deployed, the following response is received with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception occured when deleting the given Siddhi application, the following response is received with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": error-message } If the API request is valid, but there are restricted characters in the given Siddhi application name, the following response is received with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"File name contains restricted path elements . : ../../siddhiApp2'\" } Response HTTP Status Code 200, 404, 500 or 400. For descriptions of the HTTP status codes, see HTTP Status Codes . Listing all active Siddhi applications Overview Description Lists all the currently active Siddhi applications. If the isActive=true parameter is set, all the active Siddhi Applications are listed. If not, all the inactive Siddhi applications are listed. API Context /siddhi-apps HTTP Method GET Request/Response format Request content type : any Response content type : application/json Authentication Basic Username admin Password admin Runtime worker/manager curl command syntax curl -X GET \"http://localhost:9090/siddhi-apps\" -H \"accept: application/json\" -u admin:admin -k Sample curl command curl -X GET \"https://localhost:9443/siddhi-apps?isActive=true\" -H \"accept: application/json\" -u admin:admin -k Sample output Possible responses are as follows: If the API request is valid and there are Siddhi applications deployed in your SP setup, a response similar to the following is returned with response code 200. [\"TestExecutionPlan3\", \"TestExecutionPlan4\"] If the API request is valid, there are Siddhi applications deployed in your SP setup, and a query parameter is defined in the request, a response similar to the following is returned with response code 200. This response only contains Siddhi applications that are active. !!! info If these conditions are met, but the ` isActive ` parameter is set to ` false ` , the response contains only inactive Siddhi applications. [\"TestExecutionPlan3\"] If the API request is valid, but there are no Siddhi applications deployed in your SP setup, the following response is returned. [] Response HTTP Status Code 200 For descriptions of the HTTP status codes, see HTTP Status Codes . Retrieving a specific Siddhi application Overview Description Retrieves the given Siddhi application. API Context /siddhi-apps/{appName} HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description Parameter Description {appName} The name of the Siddhi application to be retrieved. curl command syntax curl -X GET \"http://localhost:9090/siddhi-apps/{app-name}\" -H \"accept: application/json\" -u admin:admin -k Sample curl command curl -X GET \"https://localhost:9443/siddhi-apps/SiddhiTestApp\" -H \"accept: application/json\" -u admin:admin -k Sample output The possible outputs are as follows: If the API request is valid and a Siddhi application of the given name exists, a response similar to the following is returned with response code 200. { \"content\": \"\\n@Plan:name('TestExecutionPlan')\\ndefine stream FooStream (symbol string, price float, volume long);\\n\\n@source(type='inMemory', topic='symbol', @map(type='passThrough'))Define stream BarStream (symbol string, price float, volume long);\\n\\nfrom FooStream\\nselect symbol, price, volume\\ninsert into BarStream;\" } If the API request is valid, but a Siddhi application of the given name is not deployed, a response similar to the following is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } Response HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Fetching the status of a Siddhi Application Overview Description This fetches the status of the specified Siddhi application API Context /siddhi-apps/{appName}/status HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description Parameter Description {appName} The name of the Siddhi application of which the status needs to be fetched. curl command syntax curl -X GET \"http://localhost:9090/siddhi-apps/{app-file-name}/status\" -H \"accept: application/json\" -u admin:admin -k Sample curl command curl -X GET \"https://localhost:9443/siddhi-apps/TestSiddhiApp/status\" -H \"accept: application/json\" -u admin:admin -k Sample output If the Siddhi application is active, the following is returned with response code 200. {\"status\":\"active\"} If the Siddhi application is inactive, the following is returned with response code 200. {\"status\":\"inactive\"} If the Siddhi application does not exist, but the REST API call is valid, the following is returned with the response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } Response HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Taking a snapshot of a Siddhi Application Overview Description This takes a snapshot of the specific Siddhi application. API Context /siddhi-apps/{appName}/backup HTTP Method POST Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description Parameter Description {appName} The name of the Siddhi application of which a snapshot needs to be taken. curl command syntax curl -X POST \"http://localhost:9090/siddhi-apps/{appName}/backup\" -H \"accept: application/json\" -u admin:admin -k Sample curl command curl -X POST \"https://localhost:9443/siddhi-apps/TestSiddhiApp/backup\" -H \"accept: application/json\" -u admin:admin -k Sample output The output can be one of the following: If the API request is valid and a Siddhi application exists with the given name, an output similar to the following (i.e., with the snapshot revision number) is returned with response code 201. {\"revision\": \"89489242494242\"} If the API request is valid, but no Siddhi application with the given name is deployed, an output similar to the following is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception has occured when backing up the state at Siddhi level, an output similar to the following is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": error-message } Response HTTP Status Code 201, 404, or 500. For descriptions of the HTTP status codes, see HTTP Status Codes . Restoring a Siddhi Application via a snapshot Info In order to call this API, you need tohave already taken a snapshot of the Siddhi application to be restored. For more information about the API via which the snapshow is taken, see Taking a snapshot of a Siddhi application . Overview Description This restores a Siddhi application using a snapshot of the same that you have previously taken. API Context To restore without considering the version : /siddhi-apps/{appName}/restore To restore a specific version : /siddhi-apps/{appName}/restore?version= HTTP Method POST Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description Parameter Description {appName} The name of the Siddhi application that needs to be restored. curl command syntax curl -X POST \"http://localhost:9090/siddhi-apps/{appName}/restore\" -H \"accept: application/json\" -u admin:admin -k Sample curl command curl -X POST \"https://localhost:9443/siddhi-apps/TestSiddhiApp/restore?revision=1514981290838_TestSiddhiApp\" -H \"accept: application/json\" -u admin:admin -k Sample output The above sample curl command can generate either one of the following responses: If the API request is valid, a Siddhi application with the given name exists, and no revision information is passed as a query parameter, the following response is returned with response code 200. { \"type\": \"success\", \"message\": \"State restored to last revision for Siddhi App :TestExecutionPlan\" } If the API request is valid, a Siddhi application with the given name exists, and revision information is passed as a query parameter, the following response is returned with response code 200. In this scenario, the Siddhi snapshot is created in the file system. { \"type\": \"success\", \"message\": \"State restored to revision 1234563 for Siddhi App :TestExecutionPlan\" } If the API request is valid, but no Siddhi application is deployed with the given name, the following response is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception occured when restoring the state at Siddhi level, the following response is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": error-message } Response HTTP Status Code 200, 404 or 500. For descriptions of the HTTP status codes, see HTTP Status Codes . Returning real-time statistics of a worker Overview Description Returns the real-time statistics of a worker. API Context /statistics HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description curl command syntax Sample curl command curl -X GET \"https://localhost:9443/statistics\" -H \"accept: application/json\" -u admin:admin -k Sample output Response HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Enabling/disabling worker statistics Overview Description Enables/diables generating statistics for worker nodes. API Context /statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description curl command syntax Sample curl command curl -X PUT \"https://localhost:9443/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k Sample output Response HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Returning general details of a worker Overview Description Returns general details of a worker. API Context /system-details HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description curl command syntax Sample curl command curl -X GET \"https://localhost:9443/system-details\" -H \"accept: application/json\" -u admin:admin -k Sample output Response HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Returning detailed statistics of all Siddhi applications Overview Description Returns the detailed statistics of all the Siddhi applications currently deployed in the SP setup. API Context /siddhi-apps/statistics HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description curl command syntax Sample curl command curl -X GET \"https://localhost:9443/siddhi-apps/statistics\" -H \"accept: application/json\" -u admin:admin -k Sample output Response HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Enabling/disabling the statistics of a specific Siddhi application Overview Description Enables/disables statistics for a specified Siddhi application. API Context /siddhi-apps/{appName}/statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description Parameter Description appName The name of the Siddhi application for which the Siddhi applications need to be enabled/disabled. curl command syntax Sample curl command curl -X PUT \"https://localhost:9443/siddhi-apps/TestSiddhiApp/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k Sample output Response HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes . Enabling/disabling the statistics of all Siddhi applications Overview Description Enables/disables statistics for all the Siddhi applications. API Context /siddhi-apps/statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager Parameter Description curl command syntax Sample curl command curl -X PUT \"https://localhost:9443/siddhi-apps/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k Sample output Response HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Siddhi Application Management APIs"},{"location":"ref/siddhi-Application-Management-APIs/#siddhi-application-management-apis","text":"Updating a Siddhi Application Deleting a Siddhi application Listing all active Siddhi applications Retrieving a specific Siddhi application Fetching the status of a Siddhi Application Taking a snapshot of a Siddhi Application Restoring a Siddhi Application via a snapshot Returning real-time statistics of a worker Enabling/disabling worker statistics Returning general details of a worker Returning detailed statistics of all Siddhi applications Enabling/disabling the statistics of a specific Siddhi application Enabling/disabling the statistics of all Siddhi applications","title":"Siddhi Application Management APIs"},{"location":"ref/siddhi-Application-Management-APIs/#creating-a-siddhi-application","text":"","title":"Creating a Siddhi application"},{"location":"ref/siddhi-Application-Management-APIs/#overview","text":"Description Creates a new Siddhi Application. API Context /siddhi-apps HTTP Method POST Request/Response format Request : text/plain Response : application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax","text":"curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @TestSiddhiApp.siddhi -u admin:admin -k","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command","text":"curl -X POST \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @TestSiddhiApp.siddhi -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output","text":"The response for the sample curl command given above can be one of the following. If API request is valid and there is no existing Siddhi application with the given name, a response similar to the following is generated with response code 201. This response contains a location header with the path of the newly created file from product root home. If the API request is valid, but a Siddhi application with the given name already exists, a response similar to the following is generated with response code 409. { \"type\": \"conflict\", \"message\": \"There is a Siddhi App already exists with same name\" } If the API request is invalid due to invalid content in the Siddhi queries you have included in the request body, a response similar to the following is generated is generated with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"You have an error in your SiddhiQL at line 8:8, missing INTO at 'BarStream'\" } If the API request is valid, but an exception occured during file processing or saving, the following response is generated with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": error-message }","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response","text":"HTTP Status Code Possible codes are 201, 409, 400, and 500. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#updating-a-siddhi-application","text":"","title":"Updating a Siddhi Application"},{"location":"ref/siddhi-Application-Management-APIs/#overview_1","text":"Description Updates a Siddhi Application. API Context /siddhi-apps HTTP Method PUT Request/Response format Request : text/plain Response : application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_1","text":"curl -X PUT \"http://localhost:9090/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @ SIDDHI_APPLICATION_NAME .siddhi -u admin:admin -k","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_1","text":"curl -X PUT \"https://localhost:9443/siddhi-apps\" -H \"accept: application/json\" -H \"Content-Type: text/plain\" -d @TestSiddhiApp.siddhi -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_1","text":"If the API request is invalid due to invalid content in the Siddhi query, a response similar to the following is returned with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"You have an error in your SiddhiQL at line 8:8, missing INTO at 'BarStream'\" } If the API request is valid, but an exception occured when saving or processing files, a response similar to the following is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": error-message }","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_1","text":"HTTP Status Code Possible codes are 200, 201, 400, and 500. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#deleting-a-siddhi-application","text":"","title":"Deleting a Siddhi application"},{"location":"ref/siddhi-Application-Management-APIs/#overview_2","text":"Description Sends the name of a Siddhi application as a URL parameter. API Context /siddhi-apps/{appName} HTTP Method DELETE Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description","text":"Parameter Description {appName} The name of the Siddhi application to be deleted.","title":"Parameter Description"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_2","text":"curl -X DELETE \"http://localhost:9090/siddhi-apps/{app-name}\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_2","text":"curl -X DELETE \"https://localhost:9443/siddhi-apps/TestSiddhiApp\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_2","text":"The respose for the sample curl command given above can be one of the following: If the API request is valid and a Siddhi application with the given name exists, the following response is received with response code 200. http://localhost:9090/siddhi-apps/TestExecutionPlan1 If the API request is valid, but a Siddhi application with the given name is not deployed, the following response is received with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception occured when deleting the given Siddhi application, the following response is received with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": error-message } If the API request is valid, but there are restricted characters in the given Siddhi application name, the following response is received with response code 400. { \"code\": 800101, \"type\": \"validation error\", \"message\": \"File name contains restricted path elements . : ../../siddhiApp2'\" }","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_2","text":"HTTP Status Code 200, 404, 500 or 400. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#listing-all-active-siddhi-applications","text":"","title":"Listing all active Siddhi applications"},{"location":"ref/siddhi-Application-Management-APIs/#overview_3","text":"Description Lists all the currently active Siddhi applications. If the isActive=true parameter is set, all the active Siddhi Applications are listed. If not, all the inactive Siddhi applications are listed. API Context /siddhi-apps HTTP Method GET Request/Response format Request content type : any Response content type : application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_3","text":"curl -X GET \"http://localhost:9090/siddhi-apps\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_3","text":"curl -X GET \"https://localhost:9443/siddhi-apps?isActive=true\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_3","text":"Possible responses are as follows: If the API request is valid and there are Siddhi applications deployed in your SP setup, a response similar to the following is returned with response code 200. [\"TestExecutionPlan3\", \"TestExecutionPlan4\"] If the API request is valid, there are Siddhi applications deployed in your SP setup, and a query parameter is defined in the request, a response similar to the following is returned with response code 200. This response only contains Siddhi applications that are active. !!! info If these conditions are met, but the ` isActive ` parameter is set to ` false ` , the response contains only inactive Siddhi applications. [\"TestExecutionPlan3\"] If the API request is valid, but there are no Siddhi applications deployed in your SP setup, the following response is returned. []","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_3","text":"HTTP Status Code 200 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#retrieving-a-specific-siddhi-application","text":"","title":"Retrieving a specific Siddhi application"},{"location":"ref/siddhi-Application-Management-APIs/#overview_4","text":"Description Retrieves the given Siddhi application. API Context /siddhi-apps/{appName} HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_1","text":"Parameter Description {appName} The name of the Siddhi application to be retrieved.","title":"Parameter Description"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_4","text":"curl -X GET \"http://localhost:9090/siddhi-apps/{app-name}\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_4","text":"curl -X GET \"https://localhost:9443/siddhi-apps/SiddhiTestApp\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_4","text":"The possible outputs are as follows: If the API request is valid and a Siddhi application of the given name exists, a response similar to the following is returned with response code 200. { \"content\": \"\\n@Plan:name('TestExecutionPlan')\\ndefine stream FooStream (symbol string, price float, volume long);\\n\\n@source(type='inMemory', topic='symbol', @map(type='passThrough'))Define stream BarStream (symbol string, price float, volume long);\\n\\nfrom FooStream\\nselect symbol, price, volume\\ninsert into BarStream;\" } If the API request is valid, but a Siddhi application of the given name is not deployed, a response similar to the following is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" }","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_4","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#fetching-the-status-of-a-siddhi-application","text":"","title":"Fetching the status of a Siddhi Application"},{"location":"ref/siddhi-Application-Management-APIs/#overview_5","text":"Description This fetches the status of the specified Siddhi application API Context /siddhi-apps/{appName}/status HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_2","text":"Parameter Description {appName} The name of the Siddhi application of which the status needs to be fetched.","title":"Parameter Description"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_5","text":"curl -X GET \"http://localhost:9090/siddhi-apps/{app-file-name}/status\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_5","text":"curl -X GET \"https://localhost:9443/siddhi-apps/TestSiddhiApp/status\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_5","text":"If the Siddhi application is active, the following is returned with response code 200. {\"status\":\"active\"} If the Siddhi application is inactive, the following is returned with response code 200. {\"status\":\"inactive\"} If the Siddhi application does not exist, but the REST API call is valid, the following is returned with the response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" }","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_5","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#taking-a-snapshot-of-a-siddhi-application","text":"","title":"Taking a snapshot of a Siddhi Application"},{"location":"ref/siddhi-Application-Management-APIs/#overview_6","text":"Description This takes a snapshot of the specific Siddhi application. API Context /siddhi-apps/{appName}/backup HTTP Method POST Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_3","text":"Parameter Description {appName} The name of the Siddhi application of which a snapshot needs to be taken.","title":"Parameter Description"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_6","text":"curl -X POST \"http://localhost:9090/siddhi-apps/{appName}/backup\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_6","text":"curl -X POST \"https://localhost:9443/siddhi-apps/TestSiddhiApp/backup\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_6","text":"The output can be one of the following: If the API request is valid and a Siddhi application exists with the given name, an output similar to the following (i.e., with the snapshot revision number) is returned with response code 201. {\"revision\": \"89489242494242\"} If the API request is valid, but no Siddhi application with the given name is deployed, an output similar to the following is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception has occured when backing up the state at Siddhi level, an output similar to the following is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": error-message }","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_6","text":"HTTP Status Code 201, 404, or 500. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#restoring-a-siddhi-application-via-a-snapshot","text":"Info In order to call this API, you need tohave already taken a snapshot of the Siddhi application to be restored. For more information about the API via which the snapshow is taken, see Taking a snapshot of a Siddhi application .","title":"Restoring a\u00a0Siddhi Application via a snapshot"},{"location":"ref/siddhi-Application-Management-APIs/#overview_7","text":"Description This restores a Siddhi application using a snapshot of the same that you have previously taken. API Context To restore without considering the version : /siddhi-apps/{appName}/restore To restore a specific version : /siddhi-apps/{appName}/restore?version= HTTP Method POST Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_4","text":"Parameter Description {appName} The name of the Siddhi application that needs to be restored.","title":"Parameter Description"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_7","text":"curl -X POST \"http://localhost:9090/siddhi-apps/{appName}/restore\" -H \"accept: application/json\" -u admin:admin -k","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_7","text":"curl -X POST \"https://localhost:9443/siddhi-apps/TestSiddhiApp/restore?revision=1514981290838_TestSiddhiApp\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_7","text":"The above sample curl command can generate either one of the following responses: If the API request is valid, a Siddhi application with the given name exists, and no revision information is passed as a query parameter, the following response is returned with response code 200. { \"type\": \"success\", \"message\": \"State restored to last revision for Siddhi App :TestExecutionPlan\" } If the API request is valid, a Siddhi application with the given name exists, and revision information is passed as a query parameter, the following response is returned with response code 200. In this scenario, the Siddhi snapshot is created in the file system. { \"type\": \"success\", \"message\": \"State restored to revision 1234563 for Siddhi App :TestExecutionPlan\" } If the API request is valid, but no Siddhi application is deployed with the given name, the following response is returned with response code 404. { \"type\": \"not found\", \"message\": \"There is no Siddhi App exist with provided name : TestExecutionPlan1\" } If the API request is valid, but an exception occured when restoring the state at Siddhi level, the following response is returned with response code 500. { \"code\": 800102, \"type\": \"file processing error\", \"message\": error-message }","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_7","text":"HTTP Status Code 200, 404 or 500. For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#returning-real-time-statistics-of-a-worker","text":"","title":"Returning real-time statistics of a worker"},{"location":"ref/siddhi-Application-Management-APIs/#overview_8","text":"Description Returns the real-time statistics of a worker. API Context /statistics HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_5","text":"","title":"Parameter Description"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_8","text":"","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_8","text":"curl -X GET \"https://localhost:9443/statistics\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_8","text":"","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_8","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#enablingdisabling-worker-statistics","text":"","title":"Enabling/disabling worker statistics"},{"location":"ref/siddhi-Application-Management-APIs/#overview_9","text":"Description Enables/diables generating statistics for worker nodes. API Context /statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_6","text":"","title":"Parameter Description"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_9","text":"","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_9","text":"curl -X PUT \"https://localhost:9443/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_9","text":"","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_9","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#returning-general-details-of-a-worker","text":"","title":"Returning general details of a worker"},{"location":"ref/siddhi-Application-Management-APIs/#overview_10","text":"Description Returns general details of a worker. API Context /system-details HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_7","text":"","title":"Parameter Description"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_10","text":"","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_10","text":"curl -X GET \"https://localhost:9443/system-details\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_10","text":"","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_10","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#returning-detailed-statistics-of-all-siddhi-applications","text":"","title":"Returning detailed statistics of all Siddhi applications"},{"location":"ref/siddhi-Application-Management-APIs/#overview_11","text":"Description Returns the detailed statistics of all the Siddhi applications currently deployed in the SP setup. API Context /siddhi-apps/statistics HTTP Method GET Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_8","text":"","title":"Parameter Description"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_11","text":"","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_11","text":"curl -X GET \"https://localhost:9443/siddhi-apps/statistics\" -H \"accept: application/json\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_11","text":"","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_11","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#enablingdisabling-the-statistics-of-a-specific-siddhi-application","text":"","title":"Enabling/disabling the statistics of a specific Siddhi application"},{"location":"ref/siddhi-Application-Management-APIs/#overview_12","text":"Description Enables/disables statistics for a specified Siddhi application. API Context /siddhi-apps/{appName}/statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_9","text":"Parameter Description appName The name of the Siddhi application for which the Siddhi applications need to be enabled/disabled.","title":"Parameter Description"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_12","text":"","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_12","text":"curl -X PUT \"https://localhost:9443/siddhi-apps/TestSiddhiApp/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_12","text":"","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_12","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/siddhi-Application-Management-APIs/#enablingdisabling-the-statistics-of-all-siddhi-applications","text":"","title":"Enabling/disabling the statistics of all Siddhi applications"},{"location":"ref/siddhi-Application-Management-APIs/#overview_13","text":"Description Enables/disables statistics for all the Siddhi applications. API Context /siddhi-apps/statistics HTTP Method PUT Request/Response format application/json Authentication Basic Username admin Password admin Runtime worker/manager","title":"Overview"},{"location":"ref/siddhi-Application-Management-APIs/#parameter-description_10","text":"","title":"Parameter Description"},{"location":"ref/siddhi-Application-Management-APIs/#curl-command-syntax_13","text":"","title":"curl command syntax"},{"location":"ref/siddhi-Application-Management-APIs/#sample-curl-command_13","text":"curl -X PUT \"https://localhost:9443/siddhi-apps/statistics\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\u201cstatsEnable\u201d:\u201dtrue\u201d}\" -u admin:admin -k","title":"Sample curl command"},{"location":"ref/siddhi-Application-Management-APIs/#sample-output_13","text":"","title":"Sample output"},{"location":"ref/siddhi-Application-Management-APIs/#response_13","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/store-APIs/","text":"Store APIs Query records in Siddhi store Query records in Siddhi store Overview Description Queries records in the Siddhi store. For more information, see Managing Stored Data via REST API . API Context /stores/query HTTP Method POST Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Worker curl command syntax curl -X POST https://localhost:9443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"AggregationTest\", \"query\" : \"from stockAggregation select *\" }' -k Sample curl command curl -X POST https://localhost:9443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNumber, 1 as arrival update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + arrival on RoomTypeTable.roomNo == roomNumber;\" }' -k Sample output Response HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Store APIs"},{"location":"ref/store-APIs/#store-apis","text":"Query records in Siddhi store","title":"Store APIs"},{"location":"ref/store-APIs/#query-records-in-siddhi-store","text":"","title":"Query records in Siddhi store"},{"location":"ref/store-APIs/#overview","text":"Description Queries records in the Siddhi store. For more information, see Managing Stored Data via REST API . API Context /stores/query HTTP Method POST Request/Response Format application/json Authentication Basic Username admin Password admin Runtime Worker","title":"Overview"},{"location":"ref/store-APIs/#curl-command-syntax","text":"curl -X POST https://localhost:9443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"AggregationTest\", \"query\" : \"from stockAggregation select *\" }' -k","title":"curl command syntax"},{"location":"ref/store-APIs/#sample-curl-command","text":"curl -X POST https://localhost:9443/stores/query -H \"content-type: application/json\" -u \"admin:admin\" -d '{\"appName\" : \"RoomService\", \"query\" : \"select 10 as roomNumber, 1 as arrival update RoomTypeTable set RoomTypeTable.people = RoomTypeTable.people + arrival on RoomTypeTable.roomNo == roomNumber;\" }' -k","title":"Sample curl command"},{"location":"ref/store-APIs/#sample-output","text":"","title":"Sample output"},{"location":"ref/store-APIs/#response","text":"HTTP Status Code 200 or 404 For descriptions of the HTTP status codes, see HTTP Status Codes .","title":"Response"},{"location":"ref/stream-Processor-REST-API-Guide/","text":"Stream Processor REST API Guide The following topics cover information relating to the public APIs exposed from WSO2 Stream Processor. Public APIs HTTP Status Codes","title":"Stream Processor REST API Guide"},{"location":"ref/stream-Processor-REST-API-Guide/#stream-processor-rest-api-guide","text":"The following topics cover information relating to the public APIs exposed from WSO2 Stream Processor. Public APIs HTTP Status Codes","title":"Stream Processor REST API Guide"},{"location":"ref/wSO2-Stream-Processor-Profiles/","text":"WSO2 Stream Processor Profiles WSO2 Stream Processor has four profiles in order to run different functions. When you start and run any of these profiles, you are running only the subset of Stream Processor features that are specific to that profile. Some use cases supported require you to run two or more of these profiles in parallel. The four profiles are as follows. Editor profile Dashboard profile Worker profile Manager profile Editor profile Purpose This runs the developer environment where the following can be carried out: Creating Siddhi applications/Siddhi application templates. Testing and debugging Siddhi applications to determine whether they are ready to be used in a production environment. !!! info For more information about creating and testing Siddhi applications, see Understanding the Development Environment . For more information about creating business templates, see Creating a Business Rule Template . Starting and running the profile Navigate to the SP_HOME /bin directory and issue one of the following commands: For Windows: editor.bat For Linux: ./editor.sh Deployment To deploy a Siddhi application in this profile, place the relevant SIDDHI_APPLICATION_NAME .siddhi file in the SP_HOME /wso2/editor/deployment/workspace directory. !!! info The Siddhi files created and saved via the Stream Processor Studio are stored in the SP_HOME /wso2/editor/deployment/workspace directory by default. Tools shipped When you start WSO2 SP in the editor profile, the URLs to access the following tools appear in the start-up logs. Stream Processor Studio Template Editor Dashboard profile Purpose This profile is available for the following purposes: Visualizing processed data via dashboards. For more information, see Visualizing Data . Deploying and managing business templates and business rules. For more information, see Creating Business Rules . Running the Status Dashboard to monitor the health of your WSO2 SP deployment. For more information, see Monitoring Stream Processor . Starting and running the profile Navigate to the SP_HOME /bin directory and issue one of the following commands: For Windows: dashboard.bat For Linux: ./dashboard.sh Deployment The following deployments are possible: Custom widgets can be deployed by placing the compiled react code in the SP_HOME /wso2/dashboard/deployment/web-ui-apps/portal/extensions/widgets directory. For more information, see Creating Custom Widgets . Dashboards that need to be imported can be deployed by placing the JSON file with the dashboard configuration in the SP_HOME /wso2/dashboard/resources/dashboards directory. For more information, see Importing and Exporting Dashboards . Business rules that need to deployed can be added in the SP_HOME /wso2/dashboard/resources/business-rules directory. For more information, see Managing Business Rules - Deploying Business Rules . Tools When you start WSO2 SP in the dashboard profile, the URLs to access the following tools appear in the start-up logs. Dashboard Portal Business Rules Manager Status Dashboard Worker profile Purpose This profile runs the Siddhi applications in a production environment when WSO2 SP is deployed in a single node or as a minimum HA cluster. For more information, see Deploying Streaming Applications . Starting and running the profile Navigate to the SP_HOME /bin directory and issue one of the following commands: For Windows: worker.bat For Linux: ./worker.sh Deployment To deploy a Siddhi application in this profile, place the relevant SIDDHI_APPLICATION_NAME .siddhi in the SP_HOME /wso2/worker/deployment/siddhi-files directory. Manager profile Purpose This profile runs the distributed Siddhi applications in a production environment when WSO2 SP is set up as a fully distributed deployment. For more information, see Fully Distributed Deployment . Starting and running the profile Navigate to the SP_HOME /bin directory and issue one of the following commands: For Windows: manager.bat For Linux: ./manager.sh Deployment To deploy a Siddhi application in this profile, place the relevant SIDDHI_APPLICATION_NAME .siddhi file in the SP_HOME /wso2/manager/deployment/siddhi-files directory.","title":"Streaming Integration Profiles"},{"location":"ref/wSO2-Stream-Processor-Profiles/#wso2-stream-processor-profiles","text":"WSO2 Stream Processor has four profiles in order to run different functions. When you start and run any of these profiles, you are running only the subset of Stream Processor features that are specific to that profile. Some use cases supported require you to run two or more of these profiles in parallel. The four profiles are as follows. Editor profile Dashboard profile Worker profile Manager profile","title":"WSO2 Stream Processor Profiles"},{"location":"ref/wSO2-Stream-Processor-Profiles/#editor-profile","text":"Purpose This runs the developer environment where the following can be carried out: Creating Siddhi applications/Siddhi application templates. Testing and debugging Siddhi applications to determine whether they are ready to be used in a production environment. !!! info For more information about creating and testing Siddhi applications, see Understanding the Development Environment . For more information about creating business templates, see Creating a Business Rule Template . Starting and running the profile Navigate to the SP_HOME /bin directory and issue one of the following commands: For Windows: editor.bat For Linux: ./editor.sh Deployment To deploy a Siddhi application in this profile, place the relevant SIDDHI_APPLICATION_NAME .siddhi file in the SP_HOME /wso2/editor/deployment/workspace directory. !!! info The Siddhi files created and saved via the Stream Processor Studio are stored in the SP_HOME /wso2/editor/deployment/workspace directory by default. Tools shipped When you start WSO2 SP in the editor profile, the URLs to access the following tools appear in the start-up logs. Stream Processor Studio Template Editor","title":"Editor profile"},{"location":"ref/wSO2-Stream-Processor-Profiles/#dashboard-profile","text":"Purpose This profile is available for the following purposes: Visualizing processed data via dashboards. For more information, see Visualizing Data . Deploying and managing business templates and business rules. For more information, see Creating Business Rules . Running the Status Dashboard to monitor the health of your WSO2 SP deployment. For more information, see Monitoring Stream Processor . Starting and running the profile Navigate to the SP_HOME /bin directory and issue one of the following commands: For Windows: dashboard.bat For Linux: ./dashboard.sh Deployment The following deployments are possible: Custom widgets can be deployed by placing the compiled react code in the SP_HOME /wso2/dashboard/deployment/web-ui-apps/portal/extensions/widgets directory. For more information, see Creating Custom Widgets . Dashboards that need to be imported can be deployed by placing the JSON file with the dashboard configuration in the SP_HOME /wso2/dashboard/resources/dashboards directory. For more information, see Importing and Exporting Dashboards . Business rules that need to deployed can be added in the SP_HOME /wso2/dashboard/resources/business-rules directory. For more information, see Managing Business Rules - Deploying Business Rules . Tools When you start WSO2 SP in the dashboard profile, the URLs to access the following tools appear in the start-up logs. Dashboard Portal Business Rules Manager Status Dashboard","title":"Dashboard profile"},{"location":"ref/wSO2-Stream-Processor-Profiles/#worker-profile","text":"Purpose This profile runs the Siddhi applications in a production environment when WSO2 SP is deployed in a single node or as a minimum HA cluster. For more information, see Deploying Streaming Applications . Starting and running the profile Navigate to the SP_HOME /bin directory and issue one of the following commands: For Windows: worker.bat For Linux: ./worker.sh Deployment To deploy a Siddhi application in this profile, place the relevant SIDDHI_APPLICATION_NAME .siddhi in the SP_HOME /wso2/worker/deployment/siddhi-files directory.","title":"Worker profile"},{"location":"ref/wSO2-Stream-Processor-Profiles/#manager-profile","text":"Purpose This profile runs the distributed Siddhi applications in a production environment when WSO2 SP is set up as a fully distributed deployment. For more information, see Fully Distributed Deployment . Starting and running the profile Navigate to the SP_HOME /bin directory and issue one of the following commands: For Windows: manager.bat For Linux: ./manager.sh Deployment To deploy a Siddhi application in this profile, place the relevant SIDDHI_APPLICATION_NAME .siddhi file in the SP_HOME /wso2/manager/deployment/siddhi-files directory.","title":"Manager profile"},{"location":"ref/worker-Runtime---REST-APIs-Permission-Model/","text":"Worker Runtime - REST APIs Permission Model There are two sets of REST APIs available in worker runtime. Stream Processor APIs and Event Simulator APIs have following permission model. You need to have appropriate permission to invoke these APIS. Stream Processor APIs Method API Context Required Permission POST /siddhi-apps PermissionString - siddhiApp.manage AppName - SAPP PUT /siddhi-apps PermissionString - siddhiApp.manage AppName - SAPP DELETE /siddhi-apps/{appName} PermissionString - siddhiApp.manage AppName - SAPP GET /siddhi-apps PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP GET /siddhi-apps/{appName} PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP GET /siddhi-apps/{appName}/status PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP POST /siddhi-apps/{appName}/backup PermissionString - siddhiApp.manage AppName - SAPP POST /siddhi-apps/{appName}/restore /siddhi-apps/{appName}/restore?version= PermissionString - siddhiApp.manage AppName - SAPP GET /statistics PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP PUT /statistics PermissionString - siddhiApp.manage AppName - SAPP GET /system-details PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP GET /siddhi-apps/statistics PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP PUT /siddhi-apps/{appName}/statistics PermissionString - siddhiApp.manage AppName - SAPP PUT /siddhi-apps/statistics PermissionString - siddhiApp.manage AppName - SAPP Event Simulator APIs Method API Context Required Permission POST /simulation/single PermissionString - simulator.manage AppName - SIM POST /simulation/feed PermissionString - simulator.manage AppName - SIM GET /simulation/feed PermissionString - simulator.manage or simulator.view AppName - SIM PUT /simulation/feed/{simulationName} PermissionString - simulator.manage AppName - SIM GET /simulation/feed/{simulationName} PermissionString - simulator.manage or simulator.view AppName - SIM DELETE /simulation/feed/{simulationName} PermissionString - simulator.manage AppName - SIM POST /simulation/feed/{simulationName}?action=run PermissionString - simulator.manage AppName - SIM POST /simulation/feed/{simulationName}?action=pause PermissionString - simulator.manage AppName - SIM POST /simulation/feed/{simulationName}?action=resume PermissionString - simulator.manage AppName - SIM POST /simulation/feed/{simulationName}?action=stop PermissionString - simulator.manage AppName - SIM POST /simulation/feed/{simulationName}?action=resume PermissionString - simulator.manage AppName - SIM GET /simulation/feed/{simulationName}/status PermissionString - simulator.manage or simulator.view AppName - SIM POST /simulation/files PermissionString - simulator.manage AppName - SIM GET /simulation/files PermissionString - simulator.manage or simulator.view AppName - SIM PUT /simulation/files/{fileName} PermissionString - simulator.manage AppName - SIM DELETE /simulation/files/{fileName} PermissionString - simulator.manage AppName - SIM POST /simulation/ connectToDatabase PermissionString - simulator.manage AppName - SIM POST /simulation/ connectToDatabase / retrieveTableNames PermissionString - simulator.manage AppName - SIM POST /simulation/ connectToDatabase/{tableName}/ retrieveColumnNames PermissionString - simulator.manage AppName - SIM simulator","title":"Worker Runtime - REST APIs Permission Model"},{"location":"ref/worker-Runtime---REST-APIs-Permission-Model/#worker-runtime-rest-apis-permission-model","text":"There are two sets of REST APIs available in worker runtime. Stream Processor APIs and Event Simulator APIs have following permission model. You need to have appropriate permission to invoke these APIS.","title":"Worker Runtime - REST APIs Permission Model"},{"location":"ref/worker-Runtime---REST-APIs-Permission-Model/#stream-processor-apis","text":"Method API Context Required Permission POST /siddhi-apps PermissionString - siddhiApp.manage AppName - SAPP PUT /siddhi-apps PermissionString - siddhiApp.manage AppName - SAPP DELETE /siddhi-apps/{appName} PermissionString - siddhiApp.manage AppName - SAPP GET /siddhi-apps PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP GET /siddhi-apps/{appName} PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP GET /siddhi-apps/{appName}/status PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP POST /siddhi-apps/{appName}/backup PermissionString - siddhiApp.manage AppName - SAPP POST /siddhi-apps/{appName}/restore /siddhi-apps/{appName}/restore?version= PermissionString - siddhiApp.manage AppName - SAPP GET /statistics PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP PUT /statistics PermissionString - siddhiApp.manage AppName - SAPP GET /system-details PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP GET /siddhi-apps/statistics PermissionString - siddhiApp.manage or siddhiApp.view AppName - SAPP PUT /siddhi-apps/{appName}/statistics PermissionString - siddhiApp.manage AppName - SAPP PUT /siddhi-apps/statistics PermissionString - siddhiApp.manage AppName - SAPP","title":"Stream Processor\u00a0 APIs"},{"location":"ref/worker-Runtime---REST-APIs-Permission-Model/#event-simulator-apis","text":"Method API Context Required Permission POST /simulation/single PermissionString - simulator.manage AppName - SIM POST /simulation/feed PermissionString - simulator.manage AppName - SIM GET /simulation/feed PermissionString - simulator.manage or simulator.view AppName - SIM PUT /simulation/feed/{simulationName} PermissionString - simulator.manage AppName - SIM GET /simulation/feed/{simulationName} PermissionString - simulator.manage or simulator.view AppName - SIM DELETE /simulation/feed/{simulationName} PermissionString - simulator.manage AppName - SIM POST /simulation/feed/{simulationName}?action=run PermissionString - simulator.manage AppName - SIM POST /simulation/feed/{simulationName}?action=pause PermissionString - simulator.manage AppName - SIM POST /simulation/feed/{simulationName}?action=resume PermissionString - simulator.manage AppName - SIM POST /simulation/feed/{simulationName}?action=stop PermissionString - simulator.manage AppName - SIM POST /simulation/feed/{simulationName}?action=resume PermissionString - simulator.manage AppName - SIM GET /simulation/feed/{simulationName}/status PermissionString - simulator.manage or simulator.view AppName - SIM POST /simulation/files PermissionString - simulator.manage AppName - SIM GET /simulation/files PermissionString - simulator.manage or simulator.view AppName - SIM PUT /simulation/files/{fileName} PermissionString - simulator.manage AppName - SIM DELETE /simulation/files/{fileName} PermissionString - simulator.manage AppName - SIM POST /simulation/ connectToDatabase PermissionString - simulator.manage AppName - SIM POST /simulation/ connectToDatabase / retrieveTableNames PermissionString - simulator.manage AppName - SIM POST /simulation/ connectToDatabase/{tableName}/ retrieveColumnNames PermissionString - simulator.manage AppName - SIM simulator","title":"Event Simulator APIs"},{"location":"ref/working-with-data-providers/","text":"Working with Data Providers Data providers are the sources from which information is fetched to be displayed in widgets. This section describes how to configure the data providers that are currently supported in WSO2 Stream Processor are as follows. These configurations determine the parameters that are available to be configured for each data provider when creating widgets via the Widget Generation wizard. For more information, see Generating Widgets . RDBMS Batch Data Provider RDBMS Streaming Data Provider Siddhi Store Data Provider Web Socket Provider RDBMS Batch Data Provider This data provider queries static tables. The following configuration is an example of an RDBMS Batch Data Provider: \"config\": { \"datasourceName\": \"Twitter_Analytics\", \"queryData\": { \"query\": \"select type as Sentiment, count(TweetID) as Rate from sentiment where PARSEDATETIME(timestamp, 'yyyy-mm-dd hh:mm:ss','en') CURRENT_TIMESTAMP()-86400 group by type\" }, \"tableName\": \"sentiment\", \"incrementalColumn\": \"Sentiment\", \"publishingInterval\": 60 } RDBMS Streaming Data Provider This data provider queries dynamic tables. Here, the newer records are published as soon as the table is updated. The following configuration is an example of an RDBMS Streaming Data Provider: \"configs\": { \"type\": \"RDBMSStreamingDataProvider\", \"config\": { \"datasourceName\": \"Twitter_Analytics\", \"queryData\": { \"query\": \"select id,TweetID from sentiment\" }, \"tableName\": \"sentiment\", \"incrementalColumn\": \"id\", \"publishingInterval\": 5, \"publishingLimit\": 5, \"purgingInterval\": 6, \"purgingLimit\": 6, \"isPurgingEnable\": false } } Siddhi Store Data Provider This data provider runs a siddhi store query. The following configuration is an example of a Siddhi Store Data Provider: Info The Siddhi application included in this query must not contain any source configurations. \"configs\":{ \"type\":\"SiddhiStoreDataProvider\", \"config\":{ \"siddhiApp\":\"@App:name(\\\"HTTPAnalytics\\\") define stream ProcessedRequestsStream(timestamp long, serverName string, serviceName string, serviceMethod string, responseTime double, httpRespGroup string, userAgent string, requestIP string); define aggregation RequestAggregation from ProcessedRequestsStream select serverName, serviceName, serviceMethod, httpRespGroup, count() as numRequests, avg(responseTime) as avgRespTime group by serverName, serviceName, serviceMethod, httpRespGroup aggregate by timestamp every sec...year;\", \"queryData\":{ \"query\":\"from RequestAggregation within \\\"2018-**-** **:**:**\\\" per \\\"days\\\" select AGG_TIMESTAMP, serverName, avg(avgRespTime) as avgRespTime\" }, \"publishingInterval\":60, \"timeColumns\": \"AGG_TIMESTAMP\" } } Web Socket Provider This data provider utilizes web siddhi-io-web socket sink to provide data to the clients. It creates endpoints as follows for the web socket sinks to connect and publish information. wss://host:port/websocket-provider/{topic} Info The host and port will be the host and port of the Portal Web application The following configuration is an example of a web socket data provider. { configs: { type: 'WebSocketProvider', config: { subscriberTopic: 'sampleStream', mapType: 'json' } } }","title":"Working with Data Providers"},{"location":"ref/working-with-data-providers/#working-with-data-providers","text":"Data providers are the sources from which information is fetched to be displayed in widgets. This section describes how to configure the data providers that are currently supported in WSO2 Stream Processor are as follows. These configurations determine the parameters that are available to be configured for each data provider when creating widgets via the Widget Generation wizard. For more information, see Generating Widgets . RDBMS Batch Data Provider RDBMS Streaming Data Provider Siddhi Store Data Provider Web Socket Provider","title":"Working with Data Providers"},{"location":"ref/working-with-data-providers/#rdbms-batch-data-provider","text":"This data provider queries static tables. The following configuration is an example of an RDBMS Batch Data Provider: \"config\": { \"datasourceName\": \"Twitter_Analytics\", \"queryData\": { \"query\": \"select type as Sentiment, count(TweetID) as Rate from sentiment where PARSEDATETIME(timestamp, 'yyyy-mm-dd hh:mm:ss','en') CURRENT_TIMESTAMP()-86400 group by type\" }, \"tableName\": \"sentiment\", \"incrementalColumn\": \"Sentiment\", \"publishingInterval\": 60 }","title":"RDBMS Batch Data Provider"},{"location":"ref/working-with-data-providers/#rdbms-streaming-data-provider","text":"This data provider queries dynamic tables. Here, the newer records are published as soon as the table is updated. The following configuration is an example of an RDBMS Streaming Data Provider: \"configs\": { \"type\": \"RDBMSStreamingDataProvider\", \"config\": { \"datasourceName\": \"Twitter_Analytics\", \"queryData\": { \"query\": \"select id,TweetID from sentiment\" }, \"tableName\": \"sentiment\", \"incrementalColumn\": \"id\", \"publishingInterval\": 5, \"publishingLimit\": 5, \"purgingInterval\": 6, \"purgingLimit\": 6, \"isPurgingEnable\": false } }","title":"RDBMS Streaming Data Provider"},{"location":"ref/working-with-data-providers/#siddhi-store-data-provider","text":"This data provider runs a siddhi store query. The following configuration is an example of a Siddhi Store Data Provider: Info The Siddhi application included in this query must not contain any source configurations. \"configs\":{ \"type\":\"SiddhiStoreDataProvider\", \"config\":{ \"siddhiApp\":\"@App:name(\\\"HTTPAnalytics\\\") define stream ProcessedRequestsStream(timestamp long, serverName string, serviceName string, serviceMethod string, responseTime double, httpRespGroup string, userAgent string, requestIP string); define aggregation RequestAggregation from ProcessedRequestsStream select serverName, serviceName, serviceMethod, httpRespGroup, count() as numRequests, avg(responseTime) as avgRespTime group by serverName, serviceName, serviceMethod, httpRespGroup aggregate by timestamp every sec...year;\", \"queryData\":{ \"query\":\"from RequestAggregation within \\\"2018-**-** **:**:**\\\" per \\\"days\\\" select AGG_TIMESTAMP, serverName, avg(avgRespTime) as avgRespTime\" }, \"publishingInterval\":60, \"timeColumns\": \"AGG_TIMESTAMP\" } }","title":"Siddhi Store Data Provider"},{"location":"ref/working-with-data-providers/#web-socket-provider","text":"This data provider utilizes web siddhi-io-web socket sink to provide data to the clients. It creates endpoints as follows for the web socket sinks to connect and publish information. wss://host:port/websocket-provider/{topic} Info The host and port will be the host and port of the Portal Web application The following configuration is an example of a web socket data provider. { configs: { type: 'WebSocketProvider', config: { subscriberTopic: 'sampleStream', mapType: 'json' } } }","title":"Web Socket Provider"},{"location":"setup/configuring-Datasources/","text":"Configuring Datasources In WSO2 SP, there are datasources specific to each runtime (i.e., worker, editor, manager, and dashboard runtimes). The datasources of each runtime are defined in the SP_HOME /conf/ runtime /deployment.yaml file. e.g., To configure a datasource in the worker runtime, the relevant configurations need to be added in the SP_Home /conf/worker/deployment.yaml file. To view a sample datasource confuguration for each database type supported, click on the following links: Info If the database driver is not an OSGI bundle, then it should be converted to OSGI (using jartobundle.sh) before placing it in the SP_HOME /lib directory. For detailed instructions, see Adding Third Party Non OSGi Libraries . e.g., sh WSO2_SP_HOME/bin/jartobundle.sh ojdbc6.jar WSO2_SP_HOME/lib/ The database should be tuned to handle the total maxPoolSize ( The maximum number of threads that should be reserved at any given time to handle events ) which defined in deployment.yaml {.expand-control-image} MySQL wso2.datasources: dataSources: description: The datasource used for test database jndiConfig: definition: type: RDBMS configuration: jdbcUrl: jdbc:mysql://hostname:port/testdb username: root password: root driverClassName: com.mysql.jdbc.Driver minIdle: 5 maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false ![](images/icons/grey_arrow_down.png){.expand-control-image} POSTGRES wso2.datasources: dataSources: description: The datasource used for test database jndiConfig: definition: type: RDBMS configuration: jdbcUrl: jdbc:postgresql://hostname:port/testdb username: root password: root driverClassName: org.postgresql.Driver minIdle: 5 maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false ![](images/icons/grey_arrow_down.png){.expand-control-image} Oracle There are two ways to configure this database type. If you have a System Identifier (SID), use this (older) format: ` jdbc:oracle:thin:@[HOST][:PORT]:SID ` wso2.datasources: dataSources: description: The datasource used for test database jndiConfig: definition: type: RDBMS configuration: jdbcUrl: jdbc:oracle:thin:@hostname:port:SID username: testdb password: root driverClassName: oracle.jdbc.driver.OracleDriver minIdle: 5 maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false If you have an Oracle service name, use this (newer) format: ` jdbc:oracle:thin:@//[HOST][:PORT]/SERVICE ` wso2.datasources: dataSources: description: The datasource used for test database jndiConfig: definition: type: RDBMS configuration: jdbcUrl: jdbc:oracle:thin:@hostname:port/SERVICE username: testdb password: root driverClassName: oracle.jdbc.driver.OracleDriver minIdle: 5 maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false The Oracle driver need to be converted to OSGi (using ` jartobundle.sh ` ) before put into ` WSO2_SP_HOME/lib ` directory. For detailed instructions, see [Adding Third Party Non OSGi Libraries](https://docs.wso2.com/display/SP420/Adding+Third+Party+Non+OSGi+Libraries) . ![](images/icons/grey_arrow_down.png){.expand-control-image} MSSQL wso2.datasources: dataSources: description: The datasource used for test database jndiConfig: definition: type: RDBMS configuration: jdbcUrl: jdbc:sqlserver://hostname:port;databaseName=testdb username: root password: root driverClassName: com.microsoft.sqlserver.jdbc.SQLServerDriver minIdle: 5 maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false The following sections explain the default datasources configured in various WSO2 SP components for different purposes, and how to change them. - [RDBMS data provider](#ConfiguringDatasources-RDBMSdataproviderRDBMSDataProvider) - [Carbon coordination](#ConfiguringDatasources-CarboncoordinationCarbonCoordination) - [Stream Processor core - persistence](#ConfiguringDatasources-StreamProcessorcore-persistenceStreamProcessorCore-Persistence) - [Stream Processor - Status Dashboard](#ConfiguringDatasources-StreamProcessor-StatusDashboardStreamProcessor-StatusDashboard) - [Siddhi RDBMS store](#ConfiguringDatasources-SiddhiRDBMSstoreSiddhiRDBMSStore) - [Carbon Dashboards](#ConfiguringDatasources-CarbonDashboardsCarbonDashboards) - [Business Rules](#ConfiguringDatasources-BusinessRulesBusinessRules) - [IdP client](#ConfiguringDatasources-IdPclientIdPClient) - [Permission provider](#ConfiguringDatasources-PermissionproviderPermissionProvider) - [Distributed Message Tracer](#ConfiguringDatasources-DistributedMessageTracerDistributedMessageTracer) #### RDBMS data provider lt;table gt; lt;colgroup gt; lt;col style= width: 13% / gt; lt;col style= width: 86% / gt; lt;/colgroup gt; lt;tbody gt; lt;tr class= odd gt; lt;td gt;Database Access Requirement lt;br / gt; lt;/td gt; lt;td gt;The RDBMS provider publishes records from RDBMS tables into generated widgets. It can also be configured to purge records in tables. In order to carry out these actions, this provider requires access to read and delete records in user defined tables of the database. For more information about the RDBMS data provider, see lt;a href= https://docs.wso2.com/display/SP440/Generating+Widgets gt;Generating Widgets lt;/a gt; . lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Required/Optional lt;/td gt; lt;td gt;This is required if you select a datasource when generating the widget or use existing widgets that connect to the RDBMS data provider when you run the dashboard profile of WSO2 SP. lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Default Datasource Name lt;/td gt; lt;td gt; lt;code gt; SAMPLE_DB lt;/code gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Default Database lt;/td gt; lt;td gt;The default lt;code gt; H2 lt;/code gt; database location is lt;code gt; amp;lt;SP_HOME amp;gt;/wso2/dashboard/database/SAMPLE_DB lt;/code gt; . lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Tables lt;/td gt; lt;td gt;The default database shipped with a sample table named lt;code gt; TRANSACTION_TABLE lt;/code gt; . lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Schemas and Queries lt;/td gt; lt;td gt; lt;p gt;The schema for the sample table is lt;code gt; TRANSACTIONS_TABLE (creditCardNo VARCHAR(50), country VARCHAR(50), transaction VARCHAR(50), amount INT) lt;/code gt; lt;/p gt; lt;p gt;The default queries can be viewed lt;a href= https://github.com/wso2/carbon-analytics/blob/v2.0.250/components/org.wso2.carbon.data.provider/src/main/resources/queries.yaml gt;here lt;/a gt; . lt;/p gt; lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Tested Database Types lt;/td gt; lt;td gt;H2, MySQL, Postgres, Mssql, Oracle 11g lt;/td gt; lt;/tr gt; lt;/tbody gt; lt;/table gt; #### Carbon coordination lt;table gt; lt;colgroup gt; lt;col style= width: 13% / gt; lt;col style= width: 86% / gt; lt;/colgroup gt; lt;tbody gt; lt;tr class= odd gt; lt;td gt;Database Access Requirement lt;/td gt; lt;td gt;Carbon coordination supports zookeeper and RDBMS based coordination. In RDBMS coordination, database access is required for updating the heartbeats of the nodes. In addition, database access is required to update the coordinator and the other members in the cluster. For more information, see lt;a href= _Configuring_Cluster_Coordination_ gt;Configuring Cluster Coordination lt;/a gt; . lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Required/Optional lt;/td gt; lt;td gt;This is required. However, you can also use Zookeeper coordination instead of RDBMS. lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Default Datasource Name lt;/td gt; lt;td gt;The carbon datasources are used. The default datasource varies depending on the deployment as follows: lt;ul gt; lt;li gt;If SP is deployed in as a lt;a href= https://docs.wso2.com/display/SP440/Minimum+High+Availability+Deployment gt;minimum HA cluster lt;/a gt; , lt;code gt; WSO2_CARBON_DB lt;/code gt; is the default database. lt;/li gt; lt;li gt;If SP is deployed as a lt;a href= https://docs.wso2.com/display/SP430/Fully+Distributed+Deployment gt;fully distributed cluster lt;/a gt; , lt;code gt; SP_MGT_DB lt;/code gt; is the default database. lt;/li gt; lt;/ul gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tables lt;/td gt; lt;td gt; lt;code gt; LEADER_STATUS_TABLE lt;/code gt; , lt;code gt; MEMBERSHIP_EVENT_TABLE lt;/code gt; , lt;code gt; REMOVED_MEMBERS_TABLE lt;/code gt; , lt;code gt; CLUSTER_NODE_STATUS_TABLE lt;/code gt; lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Schemas and Queries lt;/td gt; lt;td gt; lt;p gt;Information about the default queries and the schema can be viewed lt;a href= https://github.com/wso2/carbon-coordination/blob/v2.0.12/components/cluster-coordinator/org.wso2.carbon.cluster.coordinator/org.wso2.carbon.cluster.coordinator.rdbms/src/main/java/org/wso2/carbon/cluster/coordinator/rdbms/util/RDBMSConstants.java gt;here lt;/a gt; . lt;/p gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tested Database Types lt;/td gt; lt;td gt;MySQL, Postgres, Mssql, Oracle 11g lt;/td gt; lt;/tr gt; lt;/tbody gt; lt;/table gt; #### Stream Processor core - persistence lt;table gt; lt;colgroup gt; lt;col style= width: 13% / gt; lt;col style= width: 86% / gt; lt;/colgroup gt; lt;tbody gt; lt;tr class= odd gt; lt;td gt;Database Access Requirement lt;/td gt; lt;td gt;This involves persisting the state of Siddhi Applications periodically in the database. State persistence is enabled by selecting the lt;code gt; org.wso2.carbon.stream.processor.core.persistence.DBPersistenceStore lt;/code gt; class in the lt;code gt; state.persistence lt;/code gt; section of the lt;code gt; amp;lt;SP_Home amp;gt;/conf/ amp;lt;worker/manager amp;gt;/deployment.yaml lt;/code gt; file. For more information, see lt;a href= _Configuring_Database_and_File_System_State_Persistence_ gt;Configuring Database and File System State Persistence lt;/a gt; . lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Required/Optional lt;/td gt; lt;td gt;This is optional. WSO2 is configured to persist the state of Siddhi applications by default. lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Default Datasource Name lt;/td gt; lt;td gt;N/A. If state persistence is required, you need to configure the datasource in the lt;code gt; amp;lt;SP_Home amp;gt;/conf/ amp;lt;worker/manager amp;gt;/deployment.yaml lt;/code gt; file under lt;code gt; state.persistence lt;/code gt; amp;gt; lt;code gt; config lt;/code gt; amp;gt; lt;code gt; datasource lt;/code gt; . lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tables lt;/td gt; lt;td gt;N/A. If state persistence is required, you need to specify the table name to be used when persisting the state in the lt;code gt; amp;lt;SP_Home amp;gt;/conf/ amp;lt;worker/manager amp;gt;/deployment.yaml lt;/code gt; file under lt;code gt; state.persistence lt;/code gt; amp;gt; lt;code gt; config lt;/code gt; amp;gt; lt;code gt; table lt;/code gt; . lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Schemas and Queries lt;/td gt; lt;td gt; lt;p gt;Information about the default queries and schema can be viewed lt;a href= https://github.com/wso2/carbon-analytics/blob/master/components/org.wso2.carbon.stream.processor.core/src/main/resources/queries.yaml gt;here lt;/a gt; . lt;/p gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tested Database Types lt;/td gt; lt;td gt;H2, MySQL, Postgres, Mssql, Oracle 11g lt;/td gt; lt;/tr gt; lt;/tbody gt; lt;/table gt; #### Stream Processor - Status Dashboard lt;table gt; lt;colgroup gt; lt;col style= width: 13% / gt; lt;col style= width: 86% / gt; lt;/colgroup gt; lt;tbody gt; lt;tr class= odd gt; lt;td gt;Database Access Requirement lt;/td gt; lt;td gt;To display information relating to the status of your SP deployment, the Status Dashboard needs to retrive carbon metrics data, registered SP worker details and authentication details within the cluster from the database. For more information, see lt;a href= https://docs.wso2.com/display/SP440/Monitoring+Stream+Processor gt;Monitoring Stream Processor lt;/a gt; . lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Required/Optional lt;/td gt; lt;td gt;Required lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Default Datasource Name lt;/td gt; lt;td gt; lt;code gt; WSO2_STATUS_DASHBOARD_DB lt;/code gt; , lt;code gt; WSO2_METRICS_DB lt;/code gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tables lt;/td gt; lt;td gt; lt;code gt; METRIC_COUNTER lt;/code gt; , lt;code gt; METRIC_GAUGE lt;/code gt; , lt;code gt; METRIC_HISTOGRAM, lt;/code gt; lt;code gt; METRIC_METER lt;/code gt; , lt;code gt; METRIC_TIMER lt;/code gt; , lt;code gt; WORKERS_CONFIGURATIONS lt;/code gt; , lt;code gt; WORKERS_DETAILS lt;/code gt; lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Schemas and Queries lt;/td gt; lt;td gt; lt;p gt;Information about the default queries and schema: lt;a href= https://github.com/wso2/carbon-analytics/blob/v2.0.250/components/org.wso2.carbon.status.dashboard.core/src/main/resources/queries.yaml gt;https://github.com/wso2/carbon-analytics/blob/v2.0.250/components/org.wso2.carbon.status.dashboard.core/src/main/resources/queries.yaml lt;/a gt; lt;/p gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tested Database Types lt;/td gt; lt;td gt;H2, MySQL, Mssql, Oracle 11g ( Postgres is tested with Carbon-Metrics only) lt;/td gt; lt;/tr gt; lt;/tbody gt; lt;/table gt; #### Siddhi RDBMS store lt;table gt; lt;colgroup gt; lt;col style= width: 13% / gt; lt;col style= width: 86% / gt; lt;/colgroup gt; lt;tbody gt; lt;tr class= odd gt; lt;td gt;Database Access Requirement lt;/td gt; lt;td gt;It gives the capability of creating the tables at the siddhi app runtime and access the existing tablesif a user defined carbon datasource or JNDI property in a siddhi app. Documentation can be found in lt;a href= https://wso2-extensions.github.io/siddhi-store-rdbms/api/4.0.15/ gt;https://wso2-extensions.github.io/siddhi-store-rdbms/api/4.0.15/ lt;/a gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Required/Optional lt;/td gt; lt;td gt;Optional lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Default Datasource Name lt;/td gt; lt;td gt;No such default Datasource. User has to create the datasource in the Siddhi app lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tables lt;/td gt; lt;td gt;No such default tables. User has to define the tables lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Schemas and Queries lt;/td gt; lt;td gt; lt;p gt;Information about the default queries and schema: lt;a href= https://github.com/wso2-extensions/siddhi-store-rdbms/blob/v4.0.15/component/src/main/resources/rdbms-table-config.xml gt;https://github.com/wso2-extensions/siddhi-store-rdbms/blob/v4.0.15/component/src/main/resources/rdbms-table-config.xml lt;/a gt; lt;/p gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tested Database Types lt;/td gt; lt;td gt;H2, MySQL, Mssql, Oracle 11g, DB2, PostgreSQL lt;/td gt; lt;/tr gt; lt;/tbody gt; lt;/table gt; #### Carbon Dashboards lt;table gt; lt;colgroup gt; lt;col style= width: 13% / gt; lt;col style= width: 86% / gt; lt;/colgroup gt; lt;tbody gt; lt;tr class= odd gt; lt;td gt;Database Access Requirement lt;/td gt; lt;td gt;Carbon Dashboard feature uses its datasource to persist the dashboard related information lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Required/Optional lt;/td gt; lt;td gt;Optional lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Default Datasource Name lt;/td gt; lt;td gt;WSO2_DASHBOARD_DB lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tables lt;/td gt; lt;td gt;DASHBOARD_RESOURCE lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Schemas and Queries lt;/td gt; lt;td gt; lt;p gt; lt;a href= https://github.com/wso2/carbon-dashboards/tree/master/features/org.wso2.carbon.dashboards.api.feature/src/main/resources/sql gt;https://github.com/wso2/carbon-dashboards/tree/master/features/org.wso2.carbon.dashboards.api.feature/src/main/resources/sql lt;/a gt; lt;/p gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tested Database Types lt;/td gt; lt;td gt;H2, MySQL, Postgres lt;/td gt; lt;/tr gt; lt;/tbody gt; lt;/table gt; #### Business Rules lt;table gt; lt;colgroup gt; lt;col style= width: 13% / gt; lt;col style= width: 86% / gt; lt;/colgroup gt; lt;tbody gt; lt;tr class= odd gt; lt;td gt;Database Access Requirement lt;/td gt; lt;td gt;Business Rules feature uses database to persist the derived business rules lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Required/Optional lt;/td gt; lt;td gt;Mandatory lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Default Datasource Name lt;/td gt; lt;td gt;BUSINESS_RULES_DB lt;br / gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tables lt;/td gt; lt;td gt;BUSINESS_RULES, RULES_TEMPLATES lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Schemas and Queries lt;/td gt; lt;td gt; lt;p gt; lt;a href= https://github.com/wso2/carbon-analytics/blob/v2.0.250/components/org.wso2.carbon.business.rules.core/src/main/resources/queries.yaml gt;https://github.com/wso2/carbon-analytics/blob/v2.0.250/components/org.wso2.carbon.business.rules.core/src/main/resources/queries.yaml lt;/a gt; lt;/p gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tested Database Types lt;/td gt; lt;td gt;H2, MySQL, Oracle 11g lt;/td gt; lt;/tr gt; lt;/tbody gt; lt;/table gt; #### IdP client lt;table gt; lt;colgroup gt; lt;col style= width: 13% / gt; lt;col style= width: 86% / gt; lt;/colgroup gt; lt;tbody gt; lt;tr class= odd gt; lt;td gt;Database Access Requirement lt;/td gt; lt;td gt;IdP client access the DB layer to persist the client id and the client secret of dynamic client registration lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Required/Optional lt;/td gt; lt;td gt;Mandatory for external IdP client lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Default Datasource Name lt;/td gt; lt;td gt;DB_AUTH_DB lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tables lt;/td gt; lt;td gt;OAUTH_APPS lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Schemas and Queries lt;/td gt; lt;td gt; lt;p gt; lt;a href= https://github.com/wso2/carbon-analytics-common/blob/v6.0.52/components/authentication/org.wso2.carbon.analytics.idp.client/src/main/resources/queries.yaml gt;https://github.com/wso2/carbon-analytics-common/blob/v6.0.52/components/authentication/org.wso2.carbon.analytics.idp.client/src/main/resources/queries.yaml lt;/a gt; lt;/p gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tested Database Types lt;/td gt; lt;td gt;H2, MySQL, Oracle 11g lt;/td gt; lt;/tr gt; lt;/tbody gt; lt;/table gt; #### Permission provider lt;table gt; lt;colgroup gt; lt;col style= width: 13% / gt; lt;col style= width: 86% / gt; lt;/colgroup gt; lt;tbody gt; lt;tr class= odd gt; lt;td gt;Database Access Requirement lt;/td gt; lt;td gt;Permission provider will access the DB to p ersist permissions and role - permission mappings. lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Required/Optional lt;/td gt; lt;td gt;Mandatory, default is in H2 lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Default Datasource Name lt;/td gt; lt;td gt;PERMISSIONS_DB lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tables lt;/td gt; lt;td gt;PERMISSIONS, ROLE_PERMISSIONS lt;br / gt; lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Schemas and Queries lt;/td gt; lt;td gt; lt;p gt; lt;a href= https://github.com/wso2/carbon-analytics-common/blob/v6.0.52/components/permission-provider/org.wso2.carbon.analytics.permissions/src/main/resources/queries.yaml gt;https://github.com/wso2/carbon-analytics-common/blob/v6.0.52/components/permission-provider/org.wso2.carbon.analytics.permissions/src/main/resources/queries.yaml lt;/a gt; lt;/p gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tested Database Types lt;/td gt; lt;td gt;H2, MySQL, Mssql, Oracle 11g , Postgres lt;br / gt; lt;/td gt; lt;/tr gt; lt;/tbody gt; lt;/table gt; #### Distributed Message Tracer lt;table gt; lt;colgroup gt; lt;col style= width: 13% / gt; lt;col style= width: 86% / gt; lt;/colgroup gt; lt;tbody gt; lt;tr class= odd gt; lt;td gt;Database Access Requirement lt;/td gt; lt;td gt;The Siddhi application and the dashbioard configured for the lt;a href= https://docs.wso2.com/display/SP440/Distributed+Message+Tracer gt;Distributed Message Tracer lt;/a gt; solution that is shipped with WSO2 SP by default access this database to both read and write data. lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Required/Optional lt;/td gt; lt;td gt;Optional, default is in H2. This database is only needed when lt;a href= https://docs.wso2.com/display/SP440/Distributed+Message+Tracer gt;Distribution Message Tracing lt;/a gt; is enabled. lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Default Datasource Name lt;/td gt; lt;td gt;Message_Tracing_DB lt;br / gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tables lt;/td gt; lt;td gt;SpanTable lt;br / gt; lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Schemas and Queries lt;/td gt; lt;td gt;This database uses Siddhi Queries to insert data. It reads data from lt;a href= https://github.com/wso2/analytics-solutions/blob/089b98ee6d7d6f7574f9547c8b30db140eec38c4/components/sp-solutions/org.wso2.analytics.solutions.sp.message.tracer/widgets/OpenTracingList/src/resources/widgetConf.json#L19 gt;TracingListGadget lt;/a gt; , lt;a href= https://github.com/wso2/analytics-solutions/blob/089b98ee6d7d6f7574f9547c8b30db140eec38c4/components/sp-solutions/org.wso2.analytics.solutions.sp.message.tracer/widgets/OpenTracingSearch/src/resources/widgetConf.json#L18 gt;TracingSearchGadget lt;/a gt; , lt;a href= https://github.com/wso2/analytics-solutions/blob/089b98ee6d7d6f7574f9547c8b30db140eec38c4/components/sp-solutions/org.wso2.analytics.solutions.sp.message.tracer/widgets/OpenTracingVisTimeline/src/resources/widgetConf.json#L19 gt;TracingTimelineGadget lt;/a gt; lt;p gt; lt;br / gt; lt;/p gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tested Database Types lt;/td gt; lt;td gt;H2, MySQL, Mssql, Oracle 11g lt;br / gt; lt;/td gt; lt;/tr gt; lt;/tbody gt; lt;/table gt;","title":"Configuring Datasources"},{"location":"setup/configuring-Datasources/#configuring-datasources","text":"In WSO2 SP, there are datasources specific to each runtime (i.e., worker, editor, manager, and dashboard runtimes). The datasources of each runtime are defined in the SP_HOME /conf/ runtime /deployment.yaml file. e.g., To configure a datasource in the worker runtime, the relevant configurations need to be added in the SP_Home /conf/worker/deployment.yaml file. To view a sample datasource confuguration for each database type supported, click on the following links: Info If the database driver is not an OSGI bundle, then it should be converted to OSGI (using jartobundle.sh) before placing it in the SP_HOME /lib directory. For detailed instructions, see Adding Third Party Non OSGi Libraries . e.g., sh WSO2_SP_HOME/bin/jartobundle.sh ojdbc6.jar WSO2_SP_HOME/lib/ The database should be tuned to handle the total maxPoolSize ( The maximum number of threads that should be reserved at any given time to handle events ) which defined in deployment.yaml {.expand-control-image} MySQL wso2.datasources: dataSources: description: The datasource used for test database jndiConfig: definition: type: RDBMS configuration: jdbcUrl: jdbc:mysql://hostname:port/testdb username: root password: root driverClassName: com.mysql.jdbc.Driver minIdle: 5 maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false ![](images/icons/grey_arrow_down.png){.expand-control-image} POSTGRES wso2.datasources: dataSources: description: The datasource used for test database jndiConfig: definition: type: RDBMS configuration: jdbcUrl: jdbc:postgresql://hostname:port/testdb username: root password: root driverClassName: org.postgresql.Driver minIdle: 5 maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false ![](images/icons/grey_arrow_down.png){.expand-control-image} Oracle There are two ways to configure this database type. If you have a System Identifier (SID), use this (older) format: ` jdbc:oracle:thin:@[HOST][:PORT]:SID ` wso2.datasources: dataSources: description: The datasource used for test database jndiConfig: definition: type: RDBMS configuration: jdbcUrl: jdbc:oracle:thin:@hostname:port:SID username: testdb password: root driverClassName: oracle.jdbc.driver.OracleDriver minIdle: 5 maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false If you have an Oracle service name, use this (newer) format: ` jdbc:oracle:thin:@//[HOST][:PORT]/SERVICE ` wso2.datasources: dataSources: description: The datasource used for test database jndiConfig: definition: type: RDBMS configuration: jdbcUrl: jdbc:oracle:thin:@hostname:port/SERVICE username: testdb password: root driverClassName: oracle.jdbc.driver.OracleDriver minIdle: 5 maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false The Oracle driver need to be converted to OSGi (using ` jartobundle.sh ` ) before put into ` WSO2_SP_HOME/lib ` directory. For detailed instructions, see [Adding Third Party Non OSGi Libraries](https://docs.wso2.com/display/SP420/Adding+Third+Party+Non+OSGi+Libraries) . ![](images/icons/grey_arrow_down.png){.expand-control-image} MSSQL wso2.datasources: dataSources: description: The datasource used for test database jndiConfig: definition: type: RDBMS configuration: jdbcUrl: jdbc:sqlserver://hostname:port;databaseName=testdb username: root password: root driverClassName: com.microsoft.sqlserver.jdbc.SQLServerDriver minIdle: 5 maxPoolSize: 50 idleTimeout: 60000 connectionTestQuery: SELECT 1 validationTimeout: 30000 isAutoCommit: false The following sections explain the default datasources configured in various WSO2 SP components for different purposes, and how to change them. - [RDBMS data provider](#ConfiguringDatasources-RDBMSdataproviderRDBMSDataProvider) - [Carbon coordination](#ConfiguringDatasources-CarboncoordinationCarbonCoordination) - [Stream Processor core - persistence](#ConfiguringDatasources-StreamProcessorcore-persistenceStreamProcessorCore-Persistence) - [Stream Processor - Status Dashboard](#ConfiguringDatasources-StreamProcessor-StatusDashboardStreamProcessor-StatusDashboard) - [Siddhi RDBMS store](#ConfiguringDatasources-SiddhiRDBMSstoreSiddhiRDBMSStore) - [Carbon Dashboards](#ConfiguringDatasources-CarbonDashboardsCarbonDashboards) - [Business Rules](#ConfiguringDatasources-BusinessRulesBusinessRules) - [IdP client](#ConfiguringDatasources-IdPclientIdPClient) - [Permission provider](#ConfiguringDatasources-PermissionproviderPermissionProvider) - [Distributed Message Tracer](#ConfiguringDatasources-DistributedMessageTracerDistributedMessageTracer) #### RDBMS data provider lt;table gt; lt;colgroup gt; lt;col style= width: 13% / gt; lt;col style= width: 86% / gt; lt;/colgroup gt; lt;tbody gt; lt;tr class= odd gt; lt;td gt;Database Access Requirement lt;br / gt; lt;/td gt; lt;td gt;The RDBMS provider publishes records from RDBMS tables into generated widgets. It can also be configured to purge records in tables. In order to carry out these actions, this provider requires access to read and delete records in user defined tables of the database. For more information about the RDBMS data provider, see lt;a href= https://docs.wso2.com/display/SP440/Generating+Widgets gt;Generating Widgets lt;/a gt; . lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Required/Optional lt;/td gt; lt;td gt;This is required if you select a datasource when generating the widget or use existing widgets that connect to the RDBMS data provider when you run the dashboard profile of WSO2 SP. lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Default Datasource Name lt;/td gt; lt;td gt; lt;code gt; SAMPLE_DB lt;/code gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Default Database lt;/td gt; lt;td gt;The default lt;code gt; H2 lt;/code gt; database location is lt;code gt; amp;lt;SP_HOME amp;gt;/wso2/dashboard/database/SAMPLE_DB lt;/code gt; . lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Tables lt;/td gt; lt;td gt;The default database shipped with a sample table named lt;code gt; TRANSACTION_TABLE lt;/code gt; . lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Schemas and Queries lt;/td gt; lt;td gt; lt;p gt;The schema for the sample table is lt;code gt; TRANSACTIONS_TABLE (creditCardNo VARCHAR(50), country VARCHAR(50), transaction VARCHAR(50), amount INT) lt;/code gt; lt;/p gt; lt;p gt;The default queries can be viewed lt;a href= https://github.com/wso2/carbon-analytics/blob/v2.0.250/components/org.wso2.carbon.data.provider/src/main/resources/queries.yaml gt;here lt;/a gt; . lt;/p gt; lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Tested Database Types lt;/td gt; lt;td gt;H2, MySQL, Postgres, Mssql, Oracle 11g lt;/td gt; lt;/tr gt; lt;/tbody gt; lt;/table gt; #### Carbon coordination lt;table gt; lt;colgroup gt; lt;col style= width: 13% / gt; lt;col style= width: 86% / gt; lt;/colgroup gt; lt;tbody gt; lt;tr class= odd gt; lt;td gt;Database Access Requirement lt;/td gt; lt;td gt;Carbon coordination supports zookeeper and RDBMS based coordination. In RDBMS coordination, database access is required for updating the heartbeats of the nodes. In addition, database access is required to update the coordinator and the other members in the cluster. For more information, see lt;a href= _Configuring_Cluster_Coordination_ gt;Configuring Cluster Coordination lt;/a gt; . lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Required/Optional lt;/td gt; lt;td gt;This is required. However, you can also use Zookeeper coordination instead of RDBMS. lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Default Datasource Name lt;/td gt; lt;td gt;The carbon datasources are used. The default datasource varies depending on the deployment as follows: lt;ul gt; lt;li gt;If SP is deployed in as a lt;a href= https://docs.wso2.com/display/SP440/Minimum+High+Availability+Deployment gt;minimum HA cluster lt;/a gt; , lt;code gt; WSO2_CARBON_DB lt;/code gt; is the default database. lt;/li gt; lt;li gt;If SP is deployed as a lt;a href= https://docs.wso2.com/display/SP430/Fully+Distributed+Deployment gt;fully distributed cluster lt;/a gt; , lt;code gt; SP_MGT_DB lt;/code gt; is the default database. lt;/li gt; lt;/ul gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tables lt;/td gt; lt;td gt; lt;code gt; LEADER_STATUS_TABLE lt;/code gt; , lt;code gt; MEMBERSHIP_EVENT_TABLE lt;/code gt; , lt;code gt; REMOVED_MEMBERS_TABLE lt;/code gt; , lt;code gt; CLUSTER_NODE_STATUS_TABLE lt;/code gt; lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Schemas and Queries lt;/td gt; lt;td gt; lt;p gt;Information about the default queries and the schema can be viewed lt;a href= https://github.com/wso2/carbon-coordination/blob/v2.0.12/components/cluster-coordinator/org.wso2.carbon.cluster.coordinator/org.wso2.carbon.cluster.coordinator.rdbms/src/main/java/org/wso2/carbon/cluster/coordinator/rdbms/util/RDBMSConstants.java gt;here lt;/a gt; . lt;/p gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tested Database Types lt;/td gt; lt;td gt;MySQL, Postgres, Mssql, Oracle 11g lt;/td gt; lt;/tr gt; lt;/tbody gt; lt;/table gt; #### Stream Processor core - persistence lt;table gt; lt;colgroup gt; lt;col style= width: 13% / gt; lt;col style= width: 86% / gt; lt;/colgroup gt; lt;tbody gt; lt;tr class= odd gt; lt;td gt;Database Access Requirement lt;/td gt; lt;td gt;This involves persisting the state of Siddhi Applications periodically in the database. State persistence is enabled by selecting the lt;code gt; org.wso2.carbon.stream.processor.core.persistence.DBPersistenceStore lt;/code gt; class in the lt;code gt; state.persistence lt;/code gt; section of the lt;code gt; amp;lt;SP_Home amp;gt;/conf/ amp;lt;worker/manager amp;gt;/deployment.yaml lt;/code gt; file. For more information, see lt;a href= _Configuring_Database_and_File_System_State_Persistence_ gt;Configuring Database and File System State Persistence lt;/a gt; . lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Required/Optional lt;/td gt; lt;td gt;This is optional. WSO2 is configured to persist the state of Siddhi applications by default. lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Default Datasource Name lt;/td gt; lt;td gt;N/A. If state persistence is required, you need to configure the datasource in the lt;code gt; amp;lt;SP_Home amp;gt;/conf/ amp;lt;worker/manager amp;gt;/deployment.yaml lt;/code gt; file under lt;code gt; state.persistence lt;/code gt; amp;gt; lt;code gt; config lt;/code gt; amp;gt; lt;code gt; datasource lt;/code gt; . lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tables lt;/td gt; lt;td gt;N/A. If state persistence is required, you need to specify the table name to be used when persisting the state in the lt;code gt; amp;lt;SP_Home amp;gt;/conf/ amp;lt;worker/manager amp;gt;/deployment.yaml lt;/code gt; file under lt;code gt; state.persistence lt;/code gt; amp;gt; lt;code gt; config lt;/code gt; amp;gt; lt;code gt; table lt;/code gt; . lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Schemas and Queries lt;/td gt; lt;td gt; lt;p gt;Information about the default queries and schema can be viewed lt;a href= https://github.com/wso2/carbon-analytics/blob/master/components/org.wso2.carbon.stream.processor.core/src/main/resources/queries.yaml gt;here lt;/a gt; . lt;/p gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tested Database Types lt;/td gt; lt;td gt;H2, MySQL, Postgres, Mssql, Oracle 11g lt;/td gt; lt;/tr gt; lt;/tbody gt; lt;/table gt; #### Stream Processor - Status Dashboard lt;table gt; lt;colgroup gt; lt;col style= width: 13% / gt; lt;col style= width: 86% / gt; lt;/colgroup gt; lt;tbody gt; lt;tr class= odd gt; lt;td gt;Database Access Requirement lt;/td gt; lt;td gt;To display information relating to the status of your SP deployment, the Status Dashboard needs to retrive carbon metrics data, registered SP worker details and authentication details within the cluster from the database. For more information, see lt;a href= https://docs.wso2.com/display/SP440/Monitoring+Stream+Processor gt;Monitoring Stream Processor lt;/a gt; . lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Required/Optional lt;/td gt; lt;td gt;Required lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Default Datasource Name lt;/td gt; lt;td gt; lt;code gt; WSO2_STATUS_DASHBOARD_DB lt;/code gt; , lt;code gt; WSO2_METRICS_DB lt;/code gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tables lt;/td gt; lt;td gt; lt;code gt; METRIC_COUNTER lt;/code gt; , lt;code gt; METRIC_GAUGE lt;/code gt; , lt;code gt; METRIC_HISTOGRAM, lt;/code gt; lt;code gt; METRIC_METER lt;/code gt; , lt;code gt; METRIC_TIMER lt;/code gt; , lt;code gt; WORKERS_CONFIGURATIONS lt;/code gt; , lt;code gt; WORKERS_DETAILS lt;/code gt; lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Schemas and Queries lt;/td gt; lt;td gt; lt;p gt;Information about the default queries and schema: lt;a href= https://github.com/wso2/carbon-analytics/blob/v2.0.250/components/org.wso2.carbon.status.dashboard.core/src/main/resources/queries.yaml gt;https://github.com/wso2/carbon-analytics/blob/v2.0.250/components/org.wso2.carbon.status.dashboard.core/src/main/resources/queries.yaml lt;/a gt; lt;/p gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tested Database Types lt;/td gt; lt;td gt;H2, MySQL, Mssql, Oracle 11g ( Postgres is tested with Carbon-Metrics only) lt;/td gt; lt;/tr gt; lt;/tbody gt; lt;/table gt; #### Siddhi RDBMS store lt;table gt; lt;colgroup gt; lt;col style= width: 13% / gt; lt;col style= width: 86% / gt; lt;/colgroup gt; lt;tbody gt; lt;tr class= odd gt; lt;td gt;Database Access Requirement lt;/td gt; lt;td gt;It gives the capability of creating the tables at the siddhi app runtime and access the existing tablesif a user defined carbon datasource or JNDI property in a siddhi app. Documentation can be found in lt;a href= https://wso2-extensions.github.io/siddhi-store-rdbms/api/4.0.15/ gt;https://wso2-extensions.github.io/siddhi-store-rdbms/api/4.0.15/ lt;/a gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Required/Optional lt;/td gt; lt;td gt;Optional lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Default Datasource Name lt;/td gt; lt;td gt;No such default Datasource. User has to create the datasource in the Siddhi app lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tables lt;/td gt; lt;td gt;No such default tables. User has to define the tables lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Schemas and Queries lt;/td gt; lt;td gt; lt;p gt;Information about the default queries and schema: lt;a href= https://github.com/wso2-extensions/siddhi-store-rdbms/blob/v4.0.15/component/src/main/resources/rdbms-table-config.xml gt;https://github.com/wso2-extensions/siddhi-store-rdbms/blob/v4.0.15/component/src/main/resources/rdbms-table-config.xml lt;/a gt; lt;/p gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tested Database Types lt;/td gt; lt;td gt;H2, MySQL, Mssql, Oracle 11g, DB2, PostgreSQL lt;/td gt; lt;/tr gt; lt;/tbody gt; lt;/table gt; #### Carbon Dashboards lt;table gt; lt;colgroup gt; lt;col style= width: 13% / gt; lt;col style= width: 86% / gt; lt;/colgroup gt; lt;tbody gt; lt;tr class= odd gt; lt;td gt;Database Access Requirement lt;/td gt; lt;td gt;Carbon Dashboard feature uses its datasource to persist the dashboard related information lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Required/Optional lt;/td gt; lt;td gt;Optional lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Default Datasource Name lt;/td gt; lt;td gt;WSO2_DASHBOARD_DB lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tables lt;/td gt; lt;td gt;DASHBOARD_RESOURCE lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Schemas and Queries lt;/td gt; lt;td gt; lt;p gt; lt;a href= https://github.com/wso2/carbon-dashboards/tree/master/features/org.wso2.carbon.dashboards.api.feature/src/main/resources/sql gt;https://github.com/wso2/carbon-dashboards/tree/master/features/org.wso2.carbon.dashboards.api.feature/src/main/resources/sql lt;/a gt; lt;/p gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tested Database Types lt;/td gt; lt;td gt;H2, MySQL, Postgres lt;/td gt; lt;/tr gt; lt;/tbody gt; lt;/table gt; #### Business Rules lt;table gt; lt;colgroup gt; lt;col style= width: 13% / gt; lt;col style= width: 86% / gt; lt;/colgroup gt; lt;tbody gt; lt;tr class= odd gt; lt;td gt;Database Access Requirement lt;/td gt; lt;td gt;Business Rules feature uses database to persist the derived business rules lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Required/Optional lt;/td gt; lt;td gt;Mandatory lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Default Datasource Name lt;/td gt; lt;td gt;BUSINESS_RULES_DB lt;br / gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tables lt;/td gt; lt;td gt;BUSINESS_RULES, RULES_TEMPLATES lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Schemas and Queries lt;/td gt; lt;td gt; lt;p gt; lt;a href= https://github.com/wso2/carbon-analytics/blob/v2.0.250/components/org.wso2.carbon.business.rules.core/src/main/resources/queries.yaml gt;https://github.com/wso2/carbon-analytics/blob/v2.0.250/components/org.wso2.carbon.business.rules.core/src/main/resources/queries.yaml lt;/a gt; lt;/p gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tested Database Types lt;/td gt; lt;td gt;H2, MySQL, Oracle 11g lt;/td gt; lt;/tr gt; lt;/tbody gt; lt;/table gt; #### IdP client lt;table gt; lt;colgroup gt; lt;col style= width: 13% / gt; lt;col style= width: 86% / gt; lt;/colgroup gt; lt;tbody gt; lt;tr class= odd gt; lt;td gt;Database Access Requirement lt;/td gt; lt;td gt;IdP client access the DB layer to persist the client id and the client secret of dynamic client registration lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Required/Optional lt;/td gt; lt;td gt;Mandatory for external IdP client lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Default Datasource Name lt;/td gt; lt;td gt;DB_AUTH_DB lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tables lt;/td gt; lt;td gt;OAUTH_APPS lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Schemas and Queries lt;/td gt; lt;td gt; lt;p gt; lt;a href= https://github.com/wso2/carbon-analytics-common/blob/v6.0.52/components/authentication/org.wso2.carbon.analytics.idp.client/src/main/resources/queries.yaml gt;https://github.com/wso2/carbon-analytics-common/blob/v6.0.52/components/authentication/org.wso2.carbon.analytics.idp.client/src/main/resources/queries.yaml lt;/a gt; lt;/p gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tested Database Types lt;/td gt; lt;td gt;H2, MySQL, Oracle 11g lt;/td gt; lt;/tr gt; lt;/tbody gt; lt;/table gt; #### Permission provider lt;table gt; lt;colgroup gt; lt;col style= width: 13% / gt; lt;col style= width: 86% / gt; lt;/colgroup gt; lt;tbody gt; lt;tr class= odd gt; lt;td gt;Database Access Requirement lt;/td gt; lt;td gt;Permission provider will access the DB to p ersist permissions and role - permission mappings. lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Required/Optional lt;/td gt; lt;td gt;Mandatory, default is in H2 lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Default Datasource Name lt;/td gt; lt;td gt;PERMISSIONS_DB lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tables lt;/td gt; lt;td gt;PERMISSIONS, ROLE_PERMISSIONS lt;br / gt; lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Schemas and Queries lt;/td gt; lt;td gt; lt;p gt; lt;a href= https://github.com/wso2/carbon-analytics-common/blob/v6.0.52/components/permission-provider/org.wso2.carbon.analytics.permissions/src/main/resources/queries.yaml gt;https://github.com/wso2/carbon-analytics-common/blob/v6.0.52/components/permission-provider/org.wso2.carbon.analytics.permissions/src/main/resources/queries.yaml lt;/a gt; lt;/p gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tested Database Types lt;/td gt; lt;td gt;H2, MySQL, Mssql, Oracle 11g , Postgres lt;br / gt; lt;/td gt; lt;/tr gt; lt;/tbody gt; lt;/table gt; #### Distributed Message Tracer lt;table gt; lt;colgroup gt; lt;col style= width: 13% / gt; lt;col style= width: 86% / gt; lt;/colgroup gt; lt;tbody gt; lt;tr class= odd gt; lt;td gt;Database Access Requirement lt;/td gt; lt;td gt;The Siddhi application and the dashbioard configured for the lt;a href= https://docs.wso2.com/display/SP440/Distributed+Message+Tracer gt;Distributed Message Tracer lt;/a gt; solution that is shipped with WSO2 SP by default access this database to both read and write data. lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Required/Optional lt;/td gt; lt;td gt;Optional, default is in H2. This database is only needed when lt;a href= https://docs.wso2.com/display/SP440/Distributed+Message+Tracer gt;Distribution Message Tracing lt;/a gt; is enabled. lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Default Datasource Name lt;/td gt; lt;td gt;Message_Tracing_DB lt;br / gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tables lt;/td gt; lt;td gt;SpanTable lt;br / gt; lt;/td gt; lt;/tr gt; lt;tr class= odd gt; lt;td gt;Schemas and Queries lt;/td gt; lt;td gt;This database uses Siddhi Queries to insert data. It reads data from lt;a href= https://github.com/wso2/analytics-solutions/blob/089b98ee6d7d6f7574f9547c8b30db140eec38c4/components/sp-solutions/org.wso2.analytics.solutions.sp.message.tracer/widgets/OpenTracingList/src/resources/widgetConf.json#L19 gt;TracingListGadget lt;/a gt; , lt;a href= https://github.com/wso2/analytics-solutions/blob/089b98ee6d7d6f7574f9547c8b30db140eec38c4/components/sp-solutions/org.wso2.analytics.solutions.sp.message.tracer/widgets/OpenTracingSearch/src/resources/widgetConf.json#L18 gt;TracingSearchGadget lt;/a gt; , lt;a href= https://github.com/wso2/analytics-solutions/blob/089b98ee6d7d6f7574f9547c8b30db140eec38c4/components/sp-solutions/org.wso2.analytics.solutions.sp.message.tracer/widgets/OpenTracingVisTimeline/src/resources/widgetConf.json#L19 gt;TracingTimelineGadget lt;/a gt; lt;p gt; lt;br / gt; lt;/p gt; lt;/td gt; lt;/tr gt; lt;tr class= even gt; lt;td gt;Tested Database Types lt;/td gt; lt;td gt;H2, MySQL, Mssql, Oracle 11g lt;br / gt; lt;/td gt; lt;/tr gt; lt;/tbody gt; lt;/table gt;","title":"Configuring Datasources"},{"location":"setup/defining-Data-Tables/","text":"Defining Data Tables This section explains how to configure data tables to store the events you need to persist to carry out time series aggregation. The data handled by WSO2 Stream Processor are stored in the following two types of tables: In-memory tables : If no store-backed tables are defined, data is stored in in-memory tables by default. Store-backed tables : These are tables that are defined by you in an external database. For a list of database types supported and instructions to define table for different database types, see Defining Tables for External Data Stores . Adding primary and index keys Both in-memory tables and tables backed by external databases support primary and index keys. These are defined to allow stored information to be searched and retrieved in an effective and efficient manner. Adding primary keys Attribute(s) within the event stream for which the event table is created can be specified as the primary key for the table. The purpose of primary key is to ensure that the value for a selected attribute is unique for each entry in the table. This prevents the duplication of entries saved in the table. Primary keys are configured via the @PrimaryKey annotation. Only one @PrimaryKey annotation is allowed per event table. When several attributes are given within Primary key annotation (e.g @PrimaryKey( 'key1', 'key2')), those attributes would act as a composite primary key. Syntax @PrimaryKey(' attribute_1 ') define table event_table ( attribute_1 attribute_type , attribute_2 attribute_type , attribute_3 attribute_type ); Example @PrimaryKey('symbol') define table StockTable (symbol string, price float, volume long); The above configuration ensures that each entry saved in the StockTable event table should have a unique value for the symbol attribute because this attribute is defined as the primary key. Adding indexes An attribute within the event stream for which the event table is created can be specified as the primary key for the table. This allows the entries stored within the table to be indexed by that attribute. Indexes are configured via the @Index annotation. An event table can have multiple attributes defined as index attributes. However, only one @Index annotation can be added per event table. Syntax To index by a single attribute: @Index(' attribute_1 ') define table event_table ( attribute_1 attribute_type , attribute_2 attribute_type , attribute_3 attribute_type ); To index by multiple attributes: @Index(' attribute_1 '' attribute_2 ') define table event_table ( attribute_1 attribute_type , attribute_2 attribute_type , attribute_3 attribute_type ); Example @Index('symbol') define table StockTable (symbol string, price float, volume long); The above configuration ensures that the entries stored in the StockTable event table are indexed by the symbol attribute.","title":"Defining Data Tables"},{"location":"setup/defining-Data-Tables/#defining-data-tables","text":"This section explains how to configure data tables to store the events you need to persist to carry out time series aggregation. The data handled by WSO2 Stream Processor are stored in the following two types of tables: In-memory tables : If no store-backed tables are defined, data is stored in in-memory tables by default. Store-backed tables : These are tables that are defined by you in an external database. For a list of database types supported and instructions to define table for different database types, see Defining Tables for External Data Stores .","title":"Defining Data Tables"},{"location":"setup/defining-Data-Tables/#adding-primary-and-index-keys","text":"Both in-memory tables and tables backed by external databases support primary and index keys. These are defined to allow stored information to be searched and retrieved in an effective and efficient manner.","title":"Adding primary and index keys"},{"location":"setup/defining-Data-Tables/#adding-primary-keys","text":"Attribute(s) within the event stream for which the event table is created can be specified as the primary key for the table. The purpose of primary key is to ensure that the value for a selected attribute is unique for each entry in the table. This prevents the duplication of entries saved in the table. Primary keys are configured via the @PrimaryKey annotation. Only one @PrimaryKey annotation is allowed per event table. When several attributes are given within Primary key annotation (e.g @PrimaryKey( 'key1', 'key2')), those attributes would act as a composite primary key.","title":"Adding primary keys"},{"location":"setup/defining-Data-Tables/#syntax","text":"@PrimaryKey(' attribute_1 ') define table event_table ( attribute_1 attribute_type , attribute_2 attribute_type , attribute_3 attribute_type );","title":"Syntax"},{"location":"setup/defining-Data-Tables/#example","text":"@PrimaryKey('symbol') define table StockTable (symbol string, price float, volume long); The above configuration ensures that each entry saved in the StockTable event table should have a unique value for the symbol attribute because this attribute is defined as the primary key.","title":"Example"},{"location":"setup/defining-Data-Tables/#adding-indexes","text":"An attribute within the event stream for which the event table is created can be specified as the primary key for the table. This allows the entries stored within the table to be indexed by that attribute. Indexes are configured via the @Index annotation. An event table can have multiple attributes defined as index attributes. However, only one @Index annotation can be added per event table.","title":"Adding indexes"},{"location":"setup/defining-Data-Tables/#syntax_1","text":"To index by a single attribute: @Index(' attribute_1 ') define table event_table ( attribute_1 attribute_type , attribute_2 attribute_type , attribute_3 attribute_type ); To index by multiple attributes: @Index(' attribute_1 '' attribute_2 ') define table event_table ( attribute_1 attribute_type , attribute_2 attribute_type , attribute_3 attribute_type );","title":"Syntax"},{"location":"setup/defining-Data-Tables/#example_1","text":"@Index('symbol') define table StockTable (symbol string, price float, volume long); The above configuration ensures that the entries stored in the StockTable event table are indexed by the symbol attribute.","title":"Example"},{"location":"setup/defining-Tables-for-Physical-Stores/","text":"Defining Tables for Physical Stores This section explains how to define data tables to store data handled by WSO2 Stream Processor in physical databases. T he @store annotation syntax for defining these tables differ based on the database type as well as where the properties are defined. The store properties(such as URL, username and password) can be defined in the following ways: Inline definition : The data store can be defined within the Siddhi application as shown in the example below: @Store(type='hbase', hbase.zookeeper.quorum='localhost') @primaryKey('name') define table SweetProductionTable (name string, amount double); !!! info This method is not recommended in a production environment because is less secure compared to the other methods. As references in the deployment file : In order to do this, the store configuration needs to be defined for the relevant deployment environment in the SP_HOME /conf/ PROFILE /deployment.yaml file as a ref (i.e., in a separate section siddhi: and subsection refs:) as shown in the example below. !!! info The database connection is started when a Siddhi application is deployed, and disconnected when the Siddhi application is undeployed. Therefore, this metho is not recommended if the same database is used across multiple Siddhi applications. siddhi: refs: - ref: name: 'store1' type: 'rdbms' properties: jdbc.url: 'jdbc:h2:./repository/database/ANALYTICS_EVENT_STORE' username: 'root' password: ${sec:store1.password} field.length='currentTime:100' jdbc.driver.name: 'org.h2.Driver' Then you need to refer to that store via the @store annotation as in the Siddhi application as shown in the example below. @Store(ref='store1') @PrimaryKey('id') @Index('houseId') define table SmartHomeTable (id string, value float, property bool, plugId int, householdId int, houseId int, currentTime string); Using WSO2 data sources configuration : Once a data source defined in the wso2.datasources section of the file, SP_HOME /conf/ PROFILE /deployment.yaml, the same connection can be used across different Siddhi applications. This is done by specifying the data source to which you need to connect via the @store annotation in the following format. @Store(type=' DATABASE_TYPE ', datasource=\u2019 carbon.datasource.name \u2019) !!! info The database connection pool is initialized at server startup, and destroyed at server shut down. This is further illustrated by the following example. @Store(type='rdbms', datasource=\u2019SweetFactoryDB\u2019)@PrimaryKey(\"symbol\") define table FooTable (symbol string, price float, volume long); For more information about definig datasources, see Configuring Datasources . The following database types are currently supported for WSO2 SP. Tip Before you begin: In order to create and use an event table to store data, the following should be completed: The required database (MySql, MongoDB, Oracle Database, etc) should be downloaded and installed. A database instance should be started. The user IDs used to perform the required table operations should be granted the relevant privileges. The relevant JDBC Driver must be downloaded and the jar must be put in the SP_HOME /lib directory. RDBMS Apache HBase Apache Solr MongoDB RDBMS The RDBMS database types that are currently supported are as follows: H2 MySQL Oracle database My SQL Server PostgreSQL IBM DB2 Query syntax The following is the syntax for an RDBMS event table configuration: @store(type=\"rdbms\", jdbc.url=\" jdbc.url \", username=\" username \", password=\" password \",pool.properties=\" prop1 : val1 , prop2 : val2 \") @PrimaryKey(\"col1\") @IndexBy(\"col3\") define table table_name (col1 datatype1, col2 datatype2, col3 datatype3); Parameters The following parameters are configured in the definition of an RDBMS event table. Parameter Description Required/Optional jdbc.url The JDBC URL via which the RDBMS data store is accessed. Required username The username to be used to access the RDBMS data store. Required password The password to be used to access the RDBMS data store. Required pool.properties Any pool parameters for the database connection must be specified as key value pairs. Required jndi.resource The name of the JNDI resource through which the connection is attempted. If this is found, the pool properties described above are not taken into account. Optional table.name The name of the RDBMS table created. Optional field.length The number of characters that the values for fields of the STRING type in the table definition must contain. If this is not specified, the default number of characters specific to the database type is considered. Optional In addition to the above parameters, you can add the @primary and @index annotations in the RDBMS table configuration. @primary : This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. @index : This specifies the fields that must be indexed at the database level. You can specify multiple values as a come-separated list. Example The following is an example of an RDBMS table definition: @Store(type=\"rdbms\", jdbc.url=\"jdbc:h2:repository/database/ANALYTICS_EVENT_STORE\", username=\"root\", password=\"root\",field.length=\"symbol:254\") @PrimaryKey(\"symbol\") define table FooTable (symbol string, price float, volume long); Apache HBase Query syntax The query syntax to define an HBase table is as follows. @Store(type=\"hbase\", any.hbase.property=\" STRING \", table.name=\" STRING \", column.family.name=\" STRING \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\") Parameters The following parameters are configured in the definition of an HBase event table: Parameter Description Required/Optional table.name The name with which the table should be persisted in the store. If no table name is specified, the table in the store is assigned the same name as the corresponding Siddhi table. Optional column.family.name The name of the HBase column family from which data must be stored/referred to. Required any.hbase.property Any property that can be specified for HBase connectivity in hbase-site.xml is also accepted by the HBase Store implementation. The most frequently used properties are... hbase.zookeeper.quorum - The hostname of the server in which the zookeeper node is run. hbase.zookeeper.property.clientPort - The port of the zookeeper node. Required In addition, the following annotations are used in the HBase definition. @primary : This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. Primary keys allow you to fetch records in a table by providing a unique reference for each record. Therefore, if you do not include one or more primary keys in the table definition, it is not possible to perform table operations such as searching, updating, deleting and joining. For more information about table operations, see Managing Stored Data via Streams and Managing Stored Data via REST APIs . @index : This specifies the fields that must be indexed at the database level. You can specify multiple values as a comma separated list. Example @Store(type=\u201dhbase\u201d, table.name=\u201dSomeTestTable\u201d, column.family.name=\u201dSomeCF\u201d, hbase.zookeeper.quorum=\u201dlocalhost\u201d, hbase.zookeeper.property.clientPort=\u201d2181\u201d) @PrimaryKey(symbol) define table FooTable (symbol string, price float, volume long); Apache Solr Query syntax The query syntax to define an SOLR table is as follows. @PrimaryKey(\"id\") @store(type=\u201csolr\u201d, url= solr-cloud-zookeeper-url , collection= solr-collection-name , base.config= config-name , shards= no-of-shards , replicas= no-of-replicas , schema= schema-definition , commit.async=true|false) define table Footable (time long, date string); Parameters The following parameters are configured in an SOLR table definition. Parameter Description Required/Optional collection The name of the solr collection/table. Required url The URL of the zookeeper master of SOLR cloud. Required base.config The default configuration that should be used for the SOLR schema. Optional shards The number of shards. Optional replica The number of replica. Optional schema The SOLR schema definition. Optional commit.async If this is set to true , the results all the operations carried out for the table (described below) are applied at a specified time interval. If this is set to false , the results of the operations are applied soon after they are performed with the vent arrival. e.g., If this is set to false , an event selected to be inserted into the table is inserted as soon as it arrives to the event stream. N/A Example This query defines an SOLR table named FooTable in which a schema that consists of the two attributes time (of long type) and date (of the string type) is maintained. The values for both attributes are stored. \"shards='2', replicas='2', schema='time long stored, date string stored', commit.async='true') \" + \"define table Footable(time long, date string); ## MongoDB #### Query syntax The following is the query syntax to define a MongoDB event table. @Store(type=\"mongodb\", mongodb.uri=\" MONGODB CONNECTION URI \") @PrimaryKey(\"ATTRIBUTE_NAME\") @IndexBy(\" ATTRIBUTE_NAME SORTING ORDER INDEX OPTIONS \") define table TABLE_NME ( ATTRIBUTE1_NAME ATTRIBUTE1_TYPE , ATTRIBUTE2_NAME ATTRIBUTE2_TYPE , ATTRIBUTE3_NAME ATTRIBUTE3_TYPE , ...); The ` mongodb.uri ` parameter specifies the URI via which MongoDB user store is accessed. In addition, the following annotations are used in the MongoDB definition. - ` @primary ` : This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. - ` @index ` : This specifies the fields that must be indexed at the database level. You can specify multiple values as a comma separated list. #### Example The following query defines a MongoDB table named ` FooTable ` with the ` symbol ` , ` price ` , and ` volume ` attributes. The ` symbol ` attribute is considered the primary key and it is also indexed. @Store(type=\"mongodb\", mongodb.uri=\"mongodb://admin:admin@localhost:27017/Foo?ssl=true\") @PrimaryKey(\"symbol\") @IndexBy(\"symbol 1 {background:true}\") define table FooTable (symbol string, price float, volume long);","title":"Defining Tables for Physical Stores"},{"location":"setup/defining-Tables-for-Physical-Stores/#defining-tables-for-physical-stores","text":"This section explains how to define data tables to store data handled by WSO2 Stream Processor in physical databases. T he @store annotation syntax for defining these tables differ based on the database type as well as where the properties are defined. The store properties(such as URL, username and password) can be defined in the following ways: Inline definition : The data store can be defined within the Siddhi application as shown in the example below: @Store(type='hbase', hbase.zookeeper.quorum='localhost') @primaryKey('name') define table SweetProductionTable (name string, amount double); !!! info This method is not recommended in a production environment because is less secure compared to the other methods. As references in the deployment file : In order to do this, the store configuration needs to be defined for the relevant deployment environment in the SP_HOME /conf/ PROFILE /deployment.yaml file as a ref (i.e., in a separate section siddhi: and subsection refs:) as shown in the example below. !!! info The database connection is started when a Siddhi application is deployed, and disconnected when the Siddhi application is undeployed. Therefore, this metho is not recommended if the same database is used across multiple Siddhi applications. siddhi: refs: - ref: name: 'store1' type: 'rdbms' properties: jdbc.url: 'jdbc:h2:./repository/database/ANALYTICS_EVENT_STORE' username: 'root' password: ${sec:store1.password} field.length='currentTime:100' jdbc.driver.name: 'org.h2.Driver' Then you need to refer to that store via the @store annotation as in the Siddhi application as shown in the example below. @Store(ref='store1') @PrimaryKey('id') @Index('houseId') define table SmartHomeTable (id string, value float, property bool, plugId int, householdId int, houseId int, currentTime string); Using WSO2 data sources configuration : Once a data source defined in the wso2.datasources section of the file, SP_HOME /conf/ PROFILE /deployment.yaml, the same connection can be used across different Siddhi applications. This is done by specifying the data source to which you need to connect via the @store annotation in the following format. @Store(type=' DATABASE_TYPE ', datasource=\u2019 carbon.datasource.name \u2019) !!! info The database connection pool is initialized at server startup, and destroyed at server shut down. This is further illustrated by the following example. @Store(type='rdbms', datasource=\u2019SweetFactoryDB\u2019)@PrimaryKey(\"symbol\") define table FooTable (symbol string, price float, volume long); For more information about definig datasources, see Configuring Datasources . The following database types are currently supported for WSO2 SP. Tip Before you begin: In order to create and use an event table to store data, the following should be completed: The required database (MySql, MongoDB, Oracle Database, etc) should be downloaded and installed. A database instance should be started. The user IDs used to perform the required table operations should be granted the relevant privileges. The relevant JDBC Driver must be downloaded and the jar must be put in the SP_HOME /lib directory. RDBMS Apache HBase Apache Solr MongoDB","title":"Defining Tables for Physical Stores"},{"location":"setup/defining-Tables-for-Physical-Stores/#rdbms","text":"The RDBMS database types that are currently supported are as follows: H2 MySQL Oracle database My SQL Server PostgreSQL IBM DB2","title":"RDBMS"},{"location":"setup/defining-Tables-for-Physical-Stores/#query-syntax","text":"The following is the syntax for an RDBMS event table configuration: @store(type=\"rdbms\", jdbc.url=\" jdbc.url \", username=\" username \", password=\" password \",pool.properties=\" prop1 : val1 , prop2 : val2 \") @PrimaryKey(\"col1\") @IndexBy(\"col3\") define table table_name (col1 datatype1, col2 datatype2, col3 datatype3);","title":"Query syntax"},{"location":"setup/defining-Tables-for-Physical-Stores/#parameters","text":"The following parameters are configured in the definition of an RDBMS event table. Parameter Description Required/Optional jdbc.url The JDBC URL via which the RDBMS data store is accessed. Required username The username to be used to access the RDBMS data store. Required password The password to be used to access the RDBMS data store. Required pool.properties Any pool parameters for the database connection must be specified as key value pairs. Required jndi.resource The name of the JNDI resource through which the connection is attempted. If this is found, the pool properties described above are not taken into account. Optional table.name The name of the RDBMS table created. Optional field.length The number of characters that the values for fields of the STRING type in the table definition must contain. If this is not specified, the default number of characters specific to the database type is considered. Optional In addition to the above parameters, you can add the @primary and @index annotations in the RDBMS table configuration. @primary : This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. @index : This specifies the fields that must be indexed at the database level. You can specify multiple values as a come-separated list.","title":"Parameters"},{"location":"setup/defining-Tables-for-Physical-Stores/#example","text":"The following is an example of an RDBMS table definition: @Store(type=\"rdbms\", jdbc.url=\"jdbc:h2:repository/database/ANALYTICS_EVENT_STORE\", username=\"root\", password=\"root\",field.length=\"symbol:254\") @PrimaryKey(\"symbol\") define table FooTable (symbol string, price float, volume long);","title":"Example"},{"location":"setup/defining-Tables-for-Physical-Stores/#apache-hbase","text":"","title":"Apache HBase"},{"location":"setup/defining-Tables-for-Physical-Stores/#query-syntax_1","text":"The query syntax to define an HBase table is as follows. @Store(type=\"hbase\", any.hbase.property=\" STRING \", table.name=\" STRING \", column.family.name=\" STRING \") @PrimaryKey(\"PRIMARY_KEY\") @Index(\"INDEX\")","title":"Query syntax"},{"location":"setup/defining-Tables-for-Physical-Stores/#parameters_1","text":"The following parameters are configured in the definition of an HBase event table: Parameter Description Required/Optional table.name The name with which the table should be persisted in the store. If no table name is specified, the table in the store is assigned the same name as the corresponding Siddhi table. Optional column.family.name The name of the HBase column family from which data must be stored/referred to. Required any.hbase.property Any property that can be specified for HBase connectivity in hbase-site.xml is also accepted by the HBase Store implementation. The most frequently used properties are... hbase.zookeeper.quorum - The hostname of the server in which the zookeeper node is run. hbase.zookeeper.property.clientPort - The port of the zookeeper node. Required In addition, the following annotations are used in the HBase definition. @primary : This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. Primary keys allow you to fetch records in a table by providing a unique reference for each record. Therefore, if you do not include one or more primary keys in the table definition, it is not possible to perform table operations such as searching, updating, deleting and joining. For more information about table operations, see Managing Stored Data via Streams and Managing Stored Data via REST APIs . @index : This specifies the fields that must be indexed at the database level. You can specify multiple values as a comma separated list.","title":"Parameters"},{"location":"setup/defining-Tables-for-Physical-Stores/#example_1","text":"@Store(type=\u201dhbase\u201d, table.name=\u201dSomeTestTable\u201d, column.family.name=\u201dSomeCF\u201d, hbase.zookeeper.quorum=\u201dlocalhost\u201d, hbase.zookeeper.property.clientPort=\u201d2181\u201d) @PrimaryKey(symbol) define table FooTable (symbol string, price float, volume long);","title":"Example"},{"location":"setup/defining-Tables-for-Physical-Stores/#apache-solr","text":"","title":"Apache Solr"},{"location":"setup/defining-Tables-for-Physical-Stores/#query-syntax_2","text":"The query syntax to define an SOLR table is as follows. @PrimaryKey(\"id\") @store(type=\u201csolr\u201d, url= solr-cloud-zookeeper-url , collection= solr-collection-name , base.config= config-name , shards= no-of-shards , replicas= no-of-replicas , schema= schema-definition , commit.async=true|false) define table Footable (time long, date string);","title":"Query syntax"},{"location":"setup/defining-Tables-for-Physical-Stores/#parameters_2","text":"The following parameters are configured in an SOLR table definition. Parameter Description Required/Optional collection The name of the solr collection/table. Required url The URL of the zookeeper master of SOLR cloud. Required base.config The default configuration that should be used for the SOLR schema. Optional shards The number of shards. Optional replica The number of replica. Optional schema The SOLR schema definition. Optional commit.async If this is set to true , the results all the operations carried out for the table (described below) are applied at a specified time interval. If this is set to false , the results of the operations are applied soon after they are performed with the vent arrival. e.g., If this is set to false , an event selected to be inserted into the table is inserted as soon as it arrives to the event stream. N/A","title":"Parameters"},{"location":"setup/defining-Tables-for-Physical-Stores/#example_2","text":"This query defines an SOLR table named FooTable in which a schema that consists of the two attributes time (of long type) and date (of the string type) is maintained. The values for both attributes are stored. \"shards='2', replicas='2', schema='time long stored, date string stored', commit.async='true') \" + \"define table Footable(time long, date string); ## MongoDB #### Query syntax The following is the query syntax to define a MongoDB event table. @Store(type=\"mongodb\", mongodb.uri=\" MONGODB CONNECTION URI \") @PrimaryKey(\"ATTRIBUTE_NAME\") @IndexBy(\" ATTRIBUTE_NAME SORTING ORDER INDEX OPTIONS \") define table TABLE_NME ( ATTRIBUTE1_NAME ATTRIBUTE1_TYPE , ATTRIBUTE2_NAME ATTRIBUTE2_TYPE , ATTRIBUTE3_NAME ATTRIBUTE3_TYPE , ...); The ` mongodb.uri ` parameter specifies the URI via which MongoDB user store is accessed. In addition, the following annotations are used in the MongoDB definition. - ` @primary ` : This specifies a list of comma-separated values to be treated as unique fields in the table. Each record in the table must have a unique combination of values for the fields specified here. - ` @index ` : This specifies the fields that must be indexed at the database level. You can specify multiple values as a comma separated list. #### Example The following query defines a MongoDB table named ` FooTable ` with the ` symbol ` , ` price ` , and ` volume ` attributes. The ` symbol ` attribute is considered the primary key and it is also indexed. @Store(type=\"mongodb\", mongodb.uri=\"mongodb://admin:admin@localhost:27017/Foo?ssl=true\") @PrimaryKey(\"symbol\") @IndexBy(\"symbol 1 {background:true}\") define table FooTable (symbol string, price float, volume long);","title":"Example"},{"location":"setup/deploying-si-as-minimum-ha-cluster/","text":"","title":"Deploying the Streaming Integrator Profile as a Minimum HA cluster"},{"location":"setup/installing-Stream-Processor-4.3.0-Using-Ansible/","text":"Installing Stream Processor 4.3.0 Using Ansible Prerequisites : I nstall Ansible by following the Installation guide here . Steps : Clone WSO2 Stream Processor Ansible git repository and switch to the relevant resource directory by executing the following commands. git clone https://github.com/wso2/ansible-sp cd ansible-sp Download the Deb or RPM binary distributions from following locations and copy to the files directory. WSO2 Stream Processor 4.3.0 package Download the JDBC driver from the following location and copy to the files directory. MySQL Connector/J Run the playbook. The existing Ansible scripts contain the configurations to set up a single node WSO2 Stream Processor pattern. In order to deploy the pattern, you need to replace the [ip_address] given in the inventory file under dev folder by the IP of the location where you need to host the Stream Processor . An example is given below. [sp] dashboard_1 ansible_host= ip_address ansible_user= ssh_user editor_1 ansible_host= ip_address ansible_user= ssh_user worker_1 ansible_host= ip_address ansible_user= ssh_user Run the following command to run the scripts. ansible-playbook -i dev site.yml Once the deployment is started, try to access the web UIs via the following URLs and default credentials on the web browser . Editor - https:// ip-address :9390/editor Dashboard - https:// ip-address :9643/monitoring Username: admin Password: admin Customize the WSO2 Ansible scripts The templates that are used by the Ansible scripts are in j2 format in-order to enable parameterization. If you need to alter the configurations given, please change the parameterized values in the yaml files under group_vars and host_vars. You can add customizations to custom.yml under tasks of each role. Step 1 Uncomment the following line in main.yml under the role you want to customize. Import_tasks: custom.yml Step 2 Add the configurations to the custom.yml . A sample is given below. - name: \"Copy custom file\" template: src: path/to/example/file/example.xml.j2 dest: destination/example.xml.j2 when: \"(inventory_hostname in groups['apim'])\"","title":"Installing Stream Processor 4.3.0 Using Ansible"},{"location":"setup/installing-Stream-Processor-4.3.0-Using-Ansible/#installing-stream-processor-430-using-ansible","text":"Prerequisites : I nstall Ansible by following the Installation guide here . Steps : Clone WSO2 Stream Processor Ansible git repository and switch to the relevant resource directory by executing the following commands. git clone https://github.com/wso2/ansible-sp cd ansible-sp Download the Deb or RPM binary distributions from following locations and copy to the files directory. WSO2 Stream Processor 4.3.0 package Download the JDBC driver from the following location and copy to the files directory. MySQL Connector/J Run the playbook. The existing Ansible scripts contain the configurations to set up a single node WSO2 Stream Processor pattern. In order to deploy the pattern, you need to replace the [ip_address] given in the inventory file under dev folder by the IP of the location where you need to host the Stream Processor . An example is given below. [sp] dashboard_1 ansible_host= ip_address ansible_user= ssh_user editor_1 ansible_host= ip_address ansible_user= ssh_user worker_1 ansible_host= ip_address ansible_user= ssh_user Run the following command to run the scripts. ansible-playbook -i dev site.yml Once the deployment is started, try to access the web UIs via the following URLs and default credentials on the web browser . Editor - https:// ip-address :9390/editor Dashboard - https:// ip-address :9643/monitoring Username: admin Password: admin","title":"Installing Stream Processor 4.3.0 Using Ansible"},{"location":"setup/installing-Stream-Processor-4.3.0-Using-Ansible/#customize-the-wso2-ansible-scripts","text":"The templates that are used by the Ansible scripts are in j2 format in-order to enable parameterization. If you need to alter the configurations given, please change the parameterized values in the yaml files under group_vars and host_vars. You can add customizations to custom.yml under tasks of each role. Step 1 Uncomment the following line in main.yml under the role you want to customize. Import_tasks: custom.yml Step 2 Add the configurations to the custom.yml . A sample is given below. - name: \"Copy custom file\" template: src: path/to/example/file/example.xml.j2 dest: destination/example.xml.j2 when: \"(inventory_hostname in groups['apim'])\"","title":"Customize the WSO2 Ansible scripts"},{"location":"setup/installing-Stream-Processor-4.3.0-Using-Puppet/","text":"Installing Stream Processor 4.3.0 Using Puppet Prerequisites : I nstall the Puppet server in the master node by following the Installation guide here . Install the Puppet agent in the agent nodes by following the Installation guide here . Steps : Clone WSO2 Stream Processor Puppet git repository and switch to the relevant resource directory by executing the following commands. git clone https://github.com/wso2/puppet-sp cd puppet-sp Download the Deb or RPM binary distributions from following locations and copy to the files directory in relevant modules. WSO2 Stream Processor 4.3.0 deb rpm Download the JDBC driver from the following location and copy to the files directory in relevant modules. MySQL Connector/J Run the existing scripts without customization. The existing Puppet scripts contain the configurations to set up a single node of the WSO2 Stream Processor runtime. Run the following command to deploy the Stream Processor server. export FACTOR_runtime=sp_editor puppet agent -vt Once the deployment is started, try to access the web UIs via the following URLs and default credentials on the web browser . https:// ip-address :9443/carbon Customizing the WSO2 Puppet scripts If you need to alter the configurations given, please change the parameterized values in the params.pp under manifests of each module. You can add customizations to the custom.pp file under the same manifests folder.","title":"Installing Stream Processor 4.3.0 Using Puppet"},{"location":"setup/installing-Stream-Processor-4.3.0-Using-Puppet/#installing-stream-processor-430-using-puppet","text":"Prerequisites : I nstall the Puppet server in the master node by following the Installation guide here . Install the Puppet agent in the agent nodes by following the Installation guide here . Steps : Clone WSO2 Stream Processor Puppet git repository and switch to the relevant resource directory by executing the following commands. git clone https://github.com/wso2/puppet-sp cd puppet-sp Download the Deb or RPM binary distributions from following locations and copy to the files directory in relevant modules. WSO2 Stream Processor 4.3.0 deb rpm Download the JDBC driver from the following location and copy to the files directory in relevant modules. MySQL Connector/J Run the existing scripts without customization. The existing Puppet scripts contain the configurations to set up a single node of the WSO2 Stream Processor runtime. Run the following command to deploy the Stream Processor server. export FACTOR_runtime=sp_editor puppet agent -vt Once the deployment is started, try to access the web UIs via the following URLs and default credentials on the web browser . https:// ip-address :9443/carbon","title":"Installing Stream Processor 4.3.0 Using Puppet"},{"location":"setup/installing-Stream-Processor-4.3.0-Using-Puppet/#customizing-the-wso2-puppet-scripts","text":"If you need to alter the configurations given, please change the parameterized values in the params.pp under manifests of each module. You can add customizations to the custom.pp file under the same manifests folder.","title":"Customizing the WSO2 Puppet scripts"},{"location":"setup/installing-Stream-Processor-4.3.0-Using-Vagrant/","text":"Installing Stream Processor 4.3.0 Using Vagrant This topic provides instructions on how to install WSO2 Stream Processor 4.3.0 using Vagrant. You can install the product with updates or install the WSO2 Stream Processor 4.3.0 general availability release (that does not include updates). Vagrant Resources for WSO2 Stream Processor with Updates Vagrant Resources for General Availability Releases Tip Before you begin: The system requirements are as follows: 3 GHz Dual-core Xeon/Opteron (or latest) 16 GB RAM 10 GB free disk space To install WSO2 Stream Processor with updates using Vagrant, follow the steps below: Install Vagrant by following the Vagrant installation guide . Install VirtualBox by following the VirtualBox installation guide . Clone WSO2 Stream Processor Vagrant git repository and switch to the relevant resource directory by executing the following commands. git clone https://github.com/wso2/vagrant-sp cd vagrant-sp Execute the following Vagrant command to start the selected deployment. Vagrant --updates up Once the deployment is started, access the Editor using the following URL. https://localhost:9743/editor You can view the Stream Processor samples as well as implement new Siddhi applications using the Editor. These Siddhi applications can then be imported and deployed in the worker node. Follow this document for a detailed guide on how to export a Siddhi application. This document provides details on deploying a Siddhi application using the Stream Processor Rest API. Use the worker node URL shown below when using the Stream Processor Rest API. https://localhost:9443/siddhi-apps Access the Stream Processor Dashboard via the following URL. https://localhost:9643/monitoring/ Access the servers using the following credentials. Username: admin Password: admin Tip Before you begin: The system requirements are as follows: 3 GHz Dual-core Xeon/Opteron (or latest) 16 GB RAM 10 GB free disk space To install WSO2 Stream Processor without updates using Vagrant, follow the steps below: Install Vagrant by following the Vagrant installation guide . Install VirtualBox by following the VirtualBox installation guide . Clone WSO2 Stream Processor Vagrant git repository and switch to the relevant resource directory by executing the following commands. git clone https://github.com/wso2/vagrant-sp cd vagrant-sp Execute the following Vagrant command to start the selected deployment. Vagrant --updates up Once the deployment is started, access the Editor using the following URL. https://localhost:9743/editor You can view the Stream Processor samples as well as implement new Siddhi applications using the Editor. These Siddhi applications can then be imported and deployed in the worker node. Follow this document for a detailed guide on how to export a Siddhi application. This document provides details on deploying a Siddhi application using the Stream Processor Rest API. Use the worker node URL shown below when using the Stream Processor Rest API. https://localhost:9443/siddhi-apps Access the Stream Processor Dashboard via the following URL. https://localhost:9643/monitoring/ Access the servers using the following credentials. Username: admin Password: admin","title":"Installing Stream Processor 4.3.0 Using Vagrant"},{"location":"setup/installing-Stream-Processor-4.3.0-Using-Vagrant/#installing-stream-processor-430-using-vagrant","text":"This topic provides instructions on how to install WSO2 Stream Processor 4.3.0 using Vagrant. You can install the product with updates or install the WSO2 Stream Processor 4.3.0 general availability release (that does not include updates). Vagrant Resources for WSO2 Stream Processor with Updates Vagrant Resources for General Availability Releases Tip Before you begin: The system requirements are as follows: 3 GHz Dual-core Xeon/Opteron (or latest) 16 GB RAM 10 GB free disk space To install WSO2 Stream Processor with updates using Vagrant, follow the steps below: Install Vagrant by following the Vagrant installation guide . Install VirtualBox by following the VirtualBox installation guide . Clone WSO2 Stream Processor Vagrant git repository and switch to the relevant resource directory by executing the following commands. git clone https://github.com/wso2/vagrant-sp cd vagrant-sp Execute the following Vagrant command to start the selected deployment. Vagrant --updates up Once the deployment is started, access the Editor using the following URL. https://localhost:9743/editor You can view the Stream Processor samples as well as implement new Siddhi applications using the Editor. These Siddhi applications can then be imported and deployed in the worker node. Follow this document for a detailed guide on how to export a Siddhi application. This document provides details on deploying a Siddhi application using the Stream Processor Rest API. Use the worker node URL shown below when using the Stream Processor Rest API. https://localhost:9443/siddhi-apps Access the Stream Processor Dashboard via the following URL. https://localhost:9643/monitoring/ Access the servers using the following credentials. Username: admin Password: admin Tip Before you begin: The system requirements are as follows: 3 GHz Dual-core Xeon/Opteron (or latest) 16 GB RAM 10 GB free disk space To install WSO2 Stream Processor without updates using Vagrant, follow the steps below: Install Vagrant by following the Vagrant installation guide . Install VirtualBox by following the VirtualBox installation guide . Clone WSO2 Stream Processor Vagrant git repository and switch to the relevant resource directory by executing the following commands. git clone https://github.com/wso2/vagrant-sp cd vagrant-sp Execute the following Vagrant command to start the selected deployment. Vagrant --updates up Once the deployment is started, access the Editor using the following URL. https://localhost:9743/editor You can view the Stream Processor samples as well as implement new Siddhi applications using the Editor. These Siddhi applications can then be imported and deployed in the worker node. Follow this document for a detailed guide on how to export a Siddhi application. This document provides details on deploying a Siddhi application using the Stream Processor Rest API. Use the worker node URL shown below when using the Stream Processor Rest API. https://localhost:9443/siddhi-apps Access the Stream Processor Dashboard via the following URL. https://localhost:9643/monitoring/ Access the servers using the following credentials. Username: admin Password: admin","title":"Installing Stream Processor 4.3.0 Using Vagrant"},{"location":"setup/installing-Stream-Processor-4.3.0/","text":"Installing Stream Processor 4.3.0 The following topics provide instructions on how to install WSO2 Stream Processor 4.3.0 using various platforms. Installing Stream Processor 4.3.0 Using Ansible Installing Stream Processor Using Docker Installing Stream Processor 4.3.0 Using Puppet Installing Stream Processor 4.3.0 Using Vagrant Installing Stream Processor Using Helm Installing Stream Processor Using AWS Cloud Formation Installing Stream Processor Using Docker Compose Installing Stream Processor Using Kubernetes Installing Stream Processor Using Apt Installing Stream Processor Using Yum Installing Stream Processor Using Brew","title":"Installing Stream Processor 4.3.0"},{"location":"setup/installing-Stream-Processor-4.3.0/#installing-stream-processor-430","text":"The following topics provide instructions on how to install WSO2 Stream Processor 4.3.0 using various platforms. Installing Stream Processor 4.3.0 Using Ansible Installing Stream Processor Using Docker Installing Stream Processor 4.3.0 Using Puppet Installing Stream Processor 4.3.0 Using Vagrant Installing Stream Processor Using Helm Installing Stream Processor Using AWS Cloud Formation Installing Stream Processor Using Docker Compose Installing Stream Processor Using Kubernetes Installing Stream Processor Using Apt Installing Stream Processor Using Yum Installing Stream Processor Using Brew","title":"Installing Stream Processor 4.3.0"},{"location":"setup/installing-Stream-Processor-Using-AWS-Cloud-Formation/","text":"Installing Stream Processor Using AWS Cloud Formation To install WSO2 Stream Processor using AWS Cloud Formation, follow the steps below: Create and upload an SSL certificate into AWS, which is required to initiate the SSL handshake for HTTPS. Create a key pair for the desired region, which is required to SSH into instances. Continue to the next step if you want to use an existing key pair. After selecting the preferred template in the preferred region, you should land on the AWS CloudFormation Select Template page. Fill in the parameter details and click the Create Stack . Enter valid WUM credentials in the CloudFormation template to deploy the product with the latest updates. Leave it blank to deploy the GA version of the product. !!! info Creating the stack takes several minutes (\\~20 minutes). It may also take around 15 more minutes for the services to become active.","title":"Installing Stream Processor Using AWS Cloud Formation"},{"location":"setup/installing-Stream-Processor-Using-AWS-Cloud-Formation/#installing-stream-processor-using-aws-cloud-formation","text":"To install WSO2 Stream Processor using AWS Cloud Formation, follow the steps below: Create and upload an SSL certificate into AWS, which is required to initiate the SSL handshake for HTTPS. Create a key pair for the desired region, which is required to SSH into instances. Continue to the next step if you want to use an existing key pair. After selecting the preferred template in the preferred region, you should land on the AWS CloudFormation Select Template page. Fill in the parameter details and click the Create Stack . Enter valid WUM credentials in the CloudFormation template to deploy the product with the latest updates. Leave it blank to deploy the GA version of the product. !!! info Creating the stack takes several minutes (\\~20 minutes). It may also take around 15 more minutes for the services to become active.","title":"Installing Stream Processor Using AWS Cloud Formation"},{"location":"setup/installing-Stream-Processor-Using-Apt/","text":"Installing Stream Processor Using Apt To install WSO2 Stream Processor with Apt, follow the steps below: To add the Apt key, issue the following command. apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys \"379CE192D401AB61\" Add the repository to the source list by issuing the following command. echo \"deb https://dl.bintray.com/wso2/deb sp_430 release\" | \\ sudo tee -a /etc/apt/sources.list Install the package by issuing the following command. apt update apt install wso2sp-4.3.0 Running the product The following commands are available for running each runtime of the WSO2 Stream Processor. Access the URLs on your favorite web browser. You can use the following credentials to log in: Username : admin Password : admin The instructions to run each profile are as follows: Dashboard profile Issue the following command in the terminal. wso2sp- SP_VERSION -dashboard You can access the different components in this profile via the following URLs. Component URL Dashboard Portal https://localhost:9643/portal Business Rules https://localhost:9643/business-rules Status Dashboard https://localhost:9643/monitoring Policies https://localhost:9643/policies Worker profile Issue the following command in the terminal. wso2sp- SP_VERSION -worker Manager profile Issue the following command in the terminal. wso2sp- SP_VERSION -manager Editor profile Issue the following command in the terminal. wso2sp- SP_VERSION -editor You can access the different components in this profile via the following URLs. Component URL Editor http://localhost:9390/editor Template Editor http://localhost:9390/template-editor","title":"Installing Stream Processor Using Apt"},{"location":"setup/installing-Stream-Processor-Using-Apt/#installing-stream-processor-using-apt","text":"To install WSO2 Stream Processor with Apt, follow the steps below: To add the Apt key, issue the following command. apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys \"379CE192D401AB61\" Add the repository to the source list by issuing the following command. echo \"deb https://dl.bintray.com/wso2/deb sp_430 release\" | \\ sudo tee -a /etc/apt/sources.list Install the package by issuing the following command. apt update apt install wso2sp-4.3.0","title":"Installing Stream Processor Using Apt"},{"location":"setup/installing-Stream-Processor-Using-Apt/#running-the-product","text":"The following commands are available for running each runtime of the WSO2 Stream Processor. Access the URLs on your favorite web browser. You can use the following credentials to log in: Username : admin Password : admin The instructions to run each profile are as follows:","title":"Running the product"},{"location":"setup/installing-Stream-Processor-Using-Apt/#dashboard-profile","text":"Issue the following command in the terminal. wso2sp- SP_VERSION -dashboard You can access the different components in this profile via the following URLs. Component URL Dashboard Portal https://localhost:9643/portal Business Rules https://localhost:9643/business-rules Status Dashboard https://localhost:9643/monitoring Policies https://localhost:9643/policies","title":"Dashboard profile"},{"location":"setup/installing-Stream-Processor-Using-Apt/#worker-profile","text":"Issue the following command in the terminal. wso2sp- SP_VERSION -worker","title":"Worker profile"},{"location":"setup/installing-Stream-Processor-Using-Apt/#manager-profile","text":"Issue the following command in the terminal. wso2sp- SP_VERSION -manager","title":"Manager profile"},{"location":"setup/installing-Stream-Processor-Using-Apt/#editor-profile","text":"Issue the following command in the terminal. wso2sp- SP_VERSION -editor You can access the different components in this profile via the following URLs. Component URL Editor http://localhost:9390/editor Template Editor http://localhost:9390/template-editor","title":"Editor profile"},{"location":"setup/installing-Stream-Processor-Using-Brew/","text":"Installing Stream Processor Using Brew The following sections cover how to install and run WSO2 Stream Processor using Brew. Installing the package Running the product Tip Before you begin: The system requirements for installing WSO2 Stream Processor using Brew is as follows: 3 GHz Dual-core Xeon/Opteron (or latest) 2 GB RAM Installing the package To install the package, follow the steps below: Install Brew by following the instructions here . Add the WSO2 repository as a tap if you have not already done so by issuing the following command. brew tap wso2/wso2 This enables you to download any WSO2 product via Homebrew. Install WSO2 SP by issuing the following command. brew install wso2sp-4.3.0 Running the product The following subsections provide instructions to run each runtime of WSO2 Stream Processor. Access the URLs via your favorite web browser by using the following credentials: Username : admin Password : admin Dashboard profile To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -dashboard Once the dashboard profile starts, access the required component via your favourite browser. The URLs for the components within this profile are as follows. Component URL Dashboard Portal https://localhost:9643/portal Business Rules https://localhost:9643/business-rules Status Dashboard https://localhost:9643/monitoring Policies https://localhost:9643/policies Editor profile To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -editor Once the dashboard profile starts, access the required component via your favourite browser. The URLs for the components within this profile are as follows. Component URL Editor https://localhost:9390/editor Template Editor https://localhost:9390/template-editor Manager profile To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -manager Worker profile To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -worker","title":"Installing Stream Processor Using Brew"},{"location":"setup/installing-Stream-Processor-Using-Brew/#installing-stream-processor-using-brew","text":"The following sections cover how to install and run WSO2 Stream Processor using Brew. Installing the package Running the product Tip Before you begin: The system requirements for installing WSO2 Stream Processor using Brew is as follows: 3 GHz Dual-core Xeon/Opteron (or latest) 2 GB RAM","title":"Installing Stream Processor Using Brew"},{"location":"setup/installing-Stream-Processor-Using-Brew/#installing-the-package","text":"To install the package, follow the steps below: Install Brew by following the instructions here . Add the WSO2 repository as a tap if you have not already done so by issuing the following command. brew tap wso2/wso2 This enables you to download any WSO2 product via Homebrew. Install WSO2 SP by issuing the following command. brew install wso2sp-4.3.0","title":"Installing the package"},{"location":"setup/installing-Stream-Processor-Using-Brew/#running-the-product","text":"The following subsections provide instructions to run each runtime of WSO2 Stream Processor. Access the URLs via your favorite web browser by using the following credentials: Username : admin Password : admin","title":"Running the product"},{"location":"setup/installing-Stream-Processor-Using-Brew/#dashboard-profile","text":"To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -dashboard Once the dashboard profile starts, access the required component via your favourite browser. The URLs for the components within this profile are as follows. Component URL Dashboard Portal https://localhost:9643/portal Business Rules https://localhost:9643/business-rules Status Dashboard https://localhost:9643/monitoring Policies https://localhost:9643/policies","title":"Dashboard profile"},{"location":"setup/installing-Stream-Processor-Using-Brew/#editor-profile","text":"To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -editor Once the dashboard profile starts, access the required component via your favourite browser. The URLs for the components within this profile are as follows. Component URL Editor https://localhost:9390/editor Template Editor https://localhost:9390/template-editor","title":"Editor profile"},{"location":"setup/installing-Stream-Processor-Using-Brew/#manager-profile","text":"To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -manager","title":"Manager profile"},{"location":"setup/installing-Stream-Processor-Using-Brew/#worker-profile","text":"To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -worker","title":"Worker profile"},{"location":"setup/installing-Stream-Processor-Using-Docker-Compose/","text":"Installing Stream Processor Using Docker Compose This topic provides instructions on how to install WSO2 Stream Processor using Docker Compose. You can install the product with updates or install the WSO2 Stream Processor general availability release (that does not include updates). Tip System Requirements 3 GHz Dual-core Xeon/Opteron (or latest) 8 GB RAM 10 GB free disk space To install the Stream Processor using Docker Compose, follow the steps below: Install Docker by following the instructions provided here . Login to the WSO2 Docker registry by executing following Docker command. When prompted, enter the username and password of your free trial account. docker login docker.wso2.com Clone the WSO2 Analytics Docker resource Git repository and switch to the relevant resource directory by executing the following commands. git clone https://github.com/wso2/docker-sp cd docker-sp/docker-compose Switch to the docker-compose/editor-worker-dashboard directory. Editor with Worker and Dashboard. cd editor-worker-dashboard WSO2 Stream Processor Distributed deployment. cd sp-distributed Execute the following Docker command to start the selected deployment. docker-compose up Once WSO2 Stream Processor is successfully deployed, access the following URLs on your web browser using the following credentials: username: admin password: admin Editor: https://localhost:9390/editor Dashboard: https://localhost:9643/monitoring You can view the WSO2 Stream Processor samples as well as implement new Siddhi applications using the editor. These Siddhi applications can then be imported and deployed in the worker node. Follow this document for a detailed guide on how to export a Siddhi application. This document provides details on deploying a Siddhi application using the WSO2 Stream Processor Rest API. Use the worker node URL shown below when using the WSO2 Stream Processor Rest API. https://localhost:9443/siddhi-apps Access the stream processor dashboard via the following URL using the following credentials: username: admin password: admin https://localhost:9643/monitoring/ To start using WSO2 Stream Processor, read the Quick Start Guide .","title":"Installing Stream Processor Using Docker Compose"},{"location":"setup/installing-Stream-Processor-Using-Docker-Compose/#installing-stream-processor-using-docker-compose","text":"This topic provides instructions on how to install WSO2 Stream Processor using Docker Compose. You can install the product with updates or install the WSO2 Stream Processor general availability release (that does not include updates). Tip System Requirements 3 GHz Dual-core Xeon/Opteron (or latest) 8 GB RAM 10 GB free disk space To install the Stream Processor using Docker Compose, follow the steps below: Install Docker by following the instructions provided here . Login to the WSO2 Docker registry by executing following Docker command. When prompted, enter the username and password of your free trial account. docker login docker.wso2.com Clone the WSO2 Analytics Docker resource Git repository and switch to the relevant resource directory by executing the following commands. git clone https://github.com/wso2/docker-sp cd docker-sp/docker-compose Switch to the docker-compose/editor-worker-dashboard directory. Editor with Worker and Dashboard. cd editor-worker-dashboard WSO2 Stream Processor Distributed deployment. cd sp-distributed Execute the following Docker command to start the selected deployment. docker-compose up Once WSO2 Stream Processor is successfully deployed, access the following URLs on your web browser using the following credentials: username: admin password: admin Editor: https://localhost:9390/editor Dashboard: https://localhost:9643/monitoring You can view the WSO2 Stream Processor samples as well as implement new Siddhi applications using the editor. These Siddhi applications can then be imported and deployed in the worker node. Follow this document for a detailed guide on how to export a Siddhi application. This document provides details on deploying a Siddhi application using the WSO2 Stream Processor Rest API. Use the worker node URL shown below when using the WSO2 Stream Processor Rest API. https://localhost:9443/siddhi-apps Access the stream processor dashboard via the following URL using the following credentials: username: admin password: admin https://localhost:9643/monitoring/ To start using WSO2 Stream Processor, read the Quick Start Guide .","title":"Installing Stream Processor Using Docker Compose"},{"location":"setup/installing-Stream-Processor-Using-Docker/","text":"Installing Stream Processor Using Docker Tip Before you begin: You need the following system requrements: 3 GHz Dual-core Xeon/Opteron (or latest) 8 GB RAM 10 GB free disk space Install Docker by following the instructions provided here . WSO2 provides open source Docker images to run WSO2 Stream Processor in Docker Hub. You can view these images from here . Downloading and installing WSO2 Stream Processor Issue the following commands to pull tghe required WSO2 Stream Processor profile with updates from the Docker image. Profile Command worker docker pull wso2/wso2sp-worker manager docker pull wso2/wso2sp-manager editor docker pull wso2/wso2sp-editor dashboard docker pull wso2/wso2sp-dashboard Running WSO2 Stream Processor To run WSO2 SP, follow the steps below: To start each WSO2 Stream Processor profile in a Docker container, issue the following commands: **For dashboard: ** docker run -it -p 9643:9643 wso2/wso2sp-dashboard **For editor: ** docker run -it \\ -p 9390:9390 \\ -p 9743:9743 \\ wso2/wso2sp-editor **For manager: ** docker run -it wso2/wso2sp-manager For worker: docker run -it wso2/wso2sp-worker Once the container is started, access the UIs of each profile via the following URLs on your favourite browser. You can enter admin as both the username and the password. Dashboard Business Rules : https://localhost:9643/business-rules Dashboard Portal : https://localhost:9643/portal Status Dashboard : https://localhost:9643/monitoring Editor Steam Processor Studio : https://localhost:9390/editor Template Editor : https://localhost:930/template-editor","title":"Installing Stream Processor Using Docker"},{"location":"setup/installing-Stream-Processor-Using-Docker/#installing-stream-processor-using-docker","text":"Tip Before you begin: You need the following system requrements: 3 GHz Dual-core Xeon/Opteron (or latest) 8 GB RAM 10 GB free disk space Install Docker by following the instructions provided here . WSO2 provides open source Docker images to run WSO2 Stream Processor in Docker Hub. You can view these images from here .","title":"Installing Stream Processor Using Docker"},{"location":"setup/installing-Stream-Processor-Using-Docker/#downloading-and-installing-wso2-stream-processor","text":"Issue the following commands to pull tghe required WSO2 Stream Processor profile with updates from the Docker image. Profile Command worker docker pull wso2/wso2sp-worker manager docker pull wso2/wso2sp-manager editor docker pull wso2/wso2sp-editor dashboard docker pull wso2/wso2sp-dashboard","title":"Downloading and installing WSO2 Stream Processor"},{"location":"setup/installing-Stream-Processor-Using-Docker/#running-wso2-stream-processor","text":"To run WSO2 SP, follow the steps below: To start each WSO2 Stream Processor profile in a Docker container, issue the following commands: **For dashboard: ** docker run -it -p 9643:9643 wso2/wso2sp-dashboard **For editor: ** docker run -it \\ -p 9390:9390 \\ -p 9743:9743 \\ wso2/wso2sp-editor **For manager: ** docker run -it wso2/wso2sp-manager For worker: docker run -it wso2/wso2sp-worker Once the container is started, access the UIs of each profile via the following URLs on your favourite browser. You can enter admin as both the username and the password. Dashboard Business Rules : https://localhost:9643/business-rules Dashboard Portal : https://localhost:9643/portal Status Dashboard : https://localhost:9643/monitoring Editor Steam Processor Studio : https://localhost:9390/editor Template Editor : https://localhost:930/template-editor","title":"Running WSO2 Stream Processor"},{"location":"setup/installing-Stream-Processor-Using-Helm/","text":"Installing Stream Processor Using Helm To install WSO2 Stream Processor using Helm, follow the steps below: Install Git , Helm (and Tiller) and Kubernetes client (tested with v1.10). As a result: An already setup Kubernetes cluster with NGINX Ingress Controller enabled. A pre-configured Network File System (NFS) to be used as the persistent volume for artifact sharing and persistence. In the NFS server instance, create a Linux system user account named wso2carbon with 802 as the user ID, and a system group named wso2 with 802 as the group ID. Add the wso2carbon user to the wso2 group. groupadd --system -g 802 wso2 useradd --system -g 802 -u 802 wso2carbon Setting up a minimum HA cluster To set up WSO2 SP as a minimum HA cluster using Helm, follow the steps below: Info In the context of this scenario, KUBERNETES_HOME refers to the the local copy of the GitHub repository, and HELM_HOME refers to the KUBERNETES_HOME /helm/pattern-distributed directory. Clone Kubernetes Resources for WSO2 Stream Processor Git repository. git clone https://github.com/wso2/kubernetes-sp.git Setup a Network File System (NFS) to be used for persistent storage. Then do the following. Create and export unique directory within the NFS server instance for the following Kubernetes Persistent Volume resource defined in the HELM_HOME /pattern-distributed/values.yaml file: sharedSiddhiFilesLocationPath Grant ownership to the wso2carbon user and the wso2 group, for each of the previously created directories. sudo chown -R wso2carbon:wso2 directory_name Grant read-write-execute permissions to the wso2carbon user, for each of the previously created directories. chmod -R 700 directory_name Provide configurations as follows. The default product configurations are available at HELM_HOME /pattern-distributed/confs directory. Update them if required. Open the HELM_HOME /pattern-distributed/values.yaml file and provide the following values. Parameter Description username Your WSO2 username. password Your WSO2 password. email Your WSO2 email. serverIp NFS Server IP. sharedDeploymentLocationPath The NFS location path of the shared Siddhi file directory (i.e.., SP_HOME /deployment/siddhi-files ) Deploy product database(s) using MySQL in Kubernetes. helm install --name sp-rdbms -f HELM_HOME /mysql/values.yaml stable/mysql --namespace wso2 For a serious deployment (e.g. production grade setup), it is recommended to connect product instances to a user owned and managed RDBMS instance. Deploy the fully distributed deployment of WSO2 Stream Processor. helm install --name RELEASE_NAME HELM_HOME /pattern-distributed Access product management consoles. Then do the following. Obtain the external IP (EXTERNAL-IP) of the Ingress resources by listing down the Kubernetes Ingresses. kubectl get ing -n wso2 Add the above host as an entry in /etc/hosts file as shown below: EXTERNAL-IP wso2sp-dashboard EXTERNAL-IP wso2sp-manager-1 EXTERNAL-IP wso2sp-manager-2 Navigate to https://wso2sp-dashboard/monitoring from your favorite browser.","title":"Installing Stream Processor Using Helm"},{"location":"setup/installing-Stream-Processor-Using-Helm/#installing-stream-processor-using-helm","text":"To install WSO2 Stream Processor using Helm, follow the steps below: Install Git , Helm (and Tiller) and Kubernetes client (tested with v1.10). As a result: An already setup Kubernetes cluster with NGINX Ingress Controller enabled. A pre-configured Network File System (NFS) to be used as the persistent volume for artifact sharing and persistence. In the NFS server instance, create a Linux system user account named wso2carbon with 802 as the user ID, and a system group named wso2 with 802 as the group ID. Add the wso2carbon user to the wso2 group. groupadd --system -g 802 wso2 useradd --system -g 802 -u 802 wso2carbon","title":"Installing Stream Processor Using Helm"},{"location":"setup/installing-Stream-Processor-Using-Helm/#setting-up-a-minimum-ha-cluster","text":"To set up WSO2 SP as a minimum HA cluster using Helm, follow the steps below: Info In the context of this scenario, KUBERNETES_HOME refers to the the local copy of the GitHub repository, and HELM_HOME refers to the KUBERNETES_HOME /helm/pattern-distributed directory. Clone Kubernetes Resources for WSO2 Stream Processor Git repository. git clone https://github.com/wso2/kubernetes-sp.git Setup a Network File System (NFS) to be used for persistent storage. Then do the following. Create and export unique directory within the NFS server instance for the following Kubernetes Persistent Volume resource defined in the HELM_HOME /pattern-distributed/values.yaml file: sharedSiddhiFilesLocationPath Grant ownership to the wso2carbon user and the wso2 group, for each of the previously created directories. sudo chown -R wso2carbon:wso2 directory_name Grant read-write-execute permissions to the wso2carbon user, for each of the previously created directories. chmod -R 700 directory_name Provide configurations as follows. The default product configurations are available at HELM_HOME /pattern-distributed/confs directory. Update them if required. Open the HELM_HOME /pattern-distributed/values.yaml file and provide the following values. Parameter Description username Your WSO2 username. password Your WSO2 password. email Your WSO2 email. serverIp NFS Server IP. sharedDeploymentLocationPath The NFS location path of the shared Siddhi file directory (i.e.., SP_HOME /deployment/siddhi-files ) Deploy product database(s) using MySQL in Kubernetes. helm install --name sp-rdbms -f HELM_HOME /mysql/values.yaml stable/mysql --namespace wso2 For a serious deployment (e.g. production grade setup), it is recommended to connect product instances to a user owned and managed RDBMS instance. Deploy the fully distributed deployment of WSO2 Stream Processor. helm install --name RELEASE_NAME HELM_HOME /pattern-distributed Access product management consoles. Then do the following. Obtain the external IP (EXTERNAL-IP) of the Ingress resources by listing down the Kubernetes Ingresses. kubectl get ing -n wso2 Add the above host as an entry in /etc/hosts file as shown below: EXTERNAL-IP wso2sp-dashboard EXTERNAL-IP wso2sp-manager-1 EXTERNAL-IP wso2sp-manager-2 Navigate to https://wso2sp-dashboard/monitoring from your favorite browser.","title":"Setting up a minimum HA cluster"},{"location":"setup/installing-Stream-Processor-Using-Kubernetes/","text":"Installing Stream Processor Using Kubernetes To install WSO2 Stream Processor using Kubernetes, follow the steps below: Install Git and Kubernetes client (tested with v1.10). As a result, an already setup Kubernetes cluster with NGINX Ingress Controller is enabled. A pre-configured Network File System (NFS) is used as the persistent volume for artifact sharing and persistence. In the NFS server instance, create a Linux system user account named wso2carbon with 802 as the user ID, and a system group named wso2 with 802 as the group ID. Add the wso2carbon user to the wso2 group. groupadd --system -g 802 wso2 useradd --system -g 802 -u 802 wso2carbon Fully distributed deployment of WSO2 Stream Processor To set up a fully distributed deployment of WSO2 Stream Processor with Kubernetes, follow the steps below: Clone the Kubernetes resources for WSO2 Stream Processor Git repository . Info The local copy of the Git repository is referred to as KUBERNETES_HOME from here onwards. git clone https://github.com/wso2/kubernetes-sp.git Setup a Network File System (NFS) to be used for persistent storage as follows. Create and export unique directories within the NFS server instance for each Kubernetes Persistent Volume resource defined in the KUBERNETES_HOME /pattern-distributed/volumes/persistent-volumes.yaml file. sudo chown -R wso2carbon:wso2 directory_name Grant ownership to the wso2carbon user and wso2 group, for each of the previously created directories by issuing the following command. chmod -R 700 directory_name Then, update each Kubernetes Persistent Volume resource with the corresponding NFS server IP (NFS_SERVER_IP), and the NFS server directory path (NFS_LOCATION_PATH) of the directory you exported. Setup product database(s) using MySQL in Kubernetes. Here, a NFS is needed for persisting MySQL DB data. Create and export a directory within the NFS server instance. Provide read-write-execute permissions to other users for the created folder. Then, update the Kubernetes Persistent Volume resource with the corresponding NFS server IP (NFS_SERVER_IP) and exported, NFS server directory path (NFS_LOCATION_PATH) in KUBERNETES_HOME /pattern-distributed/extras/rdbms/volumes/persistent-volumes.yaml . For a serious deployment (e.g. production grade setup), it is recommended to connect product instances to a user owned and managed RDBMS instance. Navigate to the KUBERNETES_HOME /pattern-distributed/scripts directory as follows. cd lt;KUBERNETES_HOME gt;/pattern-distributed/scripts Deploy the Kubernetes resources by executing the KUBERNETES_HOME /pattern-distributed/scripts/deploy.sh script as follows. ./deploy.sh --wso2-username= lt;WSO2_USERNAME gt; --wso2-password= lt;WSO2_PASSWORD gt; --cluster-admin-password= lt;K8S_CLUSTER_ADMIN_PASSWORD gt;WSO2_USERNAME: Your WSO2 username WSO2_USERNAME : Your WSO2 username WSO2_PASSWORD : Your WSO2 password K8S_CLUSTER_ADMIN_PASSWORD : Kubernetes cluster admin password Access product management consoles. Obtain the external IP (EXTERNAL-IP) of the Ingress resources by listing down the Kubernetes Ingresses. kubectl get ing The external IP can be found under the ADDRESS column of the output. Add the above host as an entry in the /etc/hosts file as shown below: \\ EXTERNAL-IP> wso2sp-dashboard lt;EXTERNAL-IP gt; wso2sp-manager-1 lt;EXTERNAL-IP gt; wso2sp-manager-2 Try navigating to https://wso2sp-dashboard/monitoring from your favorite browser.","title":"Installing Stream Processor Using Kubernetes"},{"location":"setup/installing-Stream-Processor-Using-Kubernetes/#installing-stream-processor-using-kubernetes","text":"To install WSO2 Stream Processor using Kubernetes, follow the steps below: Install Git and Kubernetes client (tested with v1.10). As a result, an already setup Kubernetes cluster with NGINX Ingress Controller is enabled. A pre-configured Network File System (NFS) is used as the persistent volume for artifact sharing and persistence. In the NFS server instance, create a Linux system user account named wso2carbon with 802 as the user ID, and a system group named wso2 with 802 as the group ID. Add the wso2carbon user to the wso2 group. groupadd --system -g 802 wso2 useradd --system -g 802 -u 802 wso2carbon","title":"Installing Stream Processor Using Kubernetes"},{"location":"setup/installing-Stream-Processor-Using-Kubernetes/#fully-distributed-deployment-of-wso2-stream-processor","text":"To set up a fully distributed deployment of WSO2 Stream Processor with Kubernetes, follow the steps below: Clone the Kubernetes resources for WSO2 Stream Processor Git repository . Info The local copy of the Git repository is referred to as KUBERNETES_HOME from here onwards. git clone https://github.com/wso2/kubernetes-sp.git Setup a Network File System (NFS) to be used for persistent storage as follows. Create and export unique directories within the NFS server instance for each Kubernetes Persistent Volume resource defined in the KUBERNETES_HOME /pattern-distributed/volumes/persistent-volumes.yaml file. sudo chown -R wso2carbon:wso2 directory_name Grant ownership to the wso2carbon user and wso2 group, for each of the previously created directories by issuing the following command. chmod -R 700 directory_name Then, update each Kubernetes Persistent Volume resource with the corresponding NFS server IP (NFS_SERVER_IP), and the NFS server directory path (NFS_LOCATION_PATH) of the directory you exported. Setup product database(s) using MySQL in Kubernetes. Here, a NFS is needed for persisting MySQL DB data. Create and export a directory within the NFS server instance. Provide read-write-execute permissions to other users for the created folder. Then, update the Kubernetes Persistent Volume resource with the corresponding NFS server IP (NFS_SERVER_IP) and exported, NFS server directory path (NFS_LOCATION_PATH) in KUBERNETES_HOME /pattern-distributed/extras/rdbms/volumes/persistent-volumes.yaml . For a serious deployment (e.g. production grade setup), it is recommended to connect product instances to a user owned and managed RDBMS instance. Navigate to the KUBERNETES_HOME /pattern-distributed/scripts directory as follows. cd lt;KUBERNETES_HOME gt;/pattern-distributed/scripts Deploy the Kubernetes resources by executing the KUBERNETES_HOME /pattern-distributed/scripts/deploy.sh script as follows. ./deploy.sh --wso2-username= lt;WSO2_USERNAME gt; --wso2-password= lt;WSO2_PASSWORD gt; --cluster-admin-password= lt;K8S_CLUSTER_ADMIN_PASSWORD gt;WSO2_USERNAME: Your WSO2 username WSO2_USERNAME : Your WSO2 username WSO2_PASSWORD : Your WSO2 password K8S_CLUSTER_ADMIN_PASSWORD : Kubernetes cluster admin password Access product management consoles. Obtain the external IP (EXTERNAL-IP) of the Ingress resources by listing down the Kubernetes Ingresses. kubectl get ing The external IP can be found under the ADDRESS column of the output. Add the above host as an entry in the /etc/hosts file as shown below: \\ EXTERNAL-IP> wso2sp-dashboard lt;EXTERNAL-IP gt; wso2sp-manager-1 lt;EXTERNAL-IP gt; wso2sp-manager-2 Try navigating to https://wso2sp-dashboard/monitoring from your favorite browser.","title":"Fully distributed deployment of WSO2 Stream Processor"},{"location":"setup/installing-Stream-Processor-Using-Yum/","text":"Installing Stream Processor Using Yum The following sections cover how to install and run WSO2 Stream Processor using Yum. Prerequisites Installing the package Running the product Prerequisites wget needs to be pre-installed. Installing the package To install the package, follow the steps below: To generate the repo file, issue the following command. wget https://bintray.com/wso2/rpm/rpm -O bintray-wso2-rpm.repo sudo mv bintray-wso2-rpm.repo /etc/yum.repos.d/ To install the package, issue the following command. yum update yum install wso2sp-4.3.0.x86_64 Running the product The following subsections provide instructions to run each runtime of WSO2 Stream Processor. Access the URLs via your favorite web browser by using the following credentials: Username : admin Password : admin Dashboard profile To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -dashboard Once the dashboard profile starts, access the required component via your favourite browser. The URLs for the components within this profile are as follows. Component URL Dashboard Portal https://localhost:9643/portal Business Rules https://localhost:9643/business-rules Status Dashboard https://localhost:9643/monitoring Policies https://localhost:9643/policies Editor profile To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -editor Once the dashboard profile starts, access the required component via your favourite browser. The URLs for the components within this profile are as follows. Component URL Editor https://localhost:9390/editor Template Editor https://localhost:9390/template-editor Manager profile To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -manager Worker profile To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -worker","title":"Installing Stream Processor Using Yum"},{"location":"setup/installing-Stream-Processor-Using-Yum/#installing-stream-processor-using-yum","text":"The following sections cover how to install and run WSO2 Stream Processor using Yum. Prerequisites Installing the package Running the product","title":"Installing Stream Processor Using Yum"},{"location":"setup/installing-Stream-Processor-Using-Yum/#prerequisites","text":"wget needs to be pre-installed.","title":"Prerequisites"},{"location":"setup/installing-Stream-Processor-Using-Yum/#installing-the-package","text":"To install the package, follow the steps below: To generate the repo file, issue the following command. wget https://bintray.com/wso2/rpm/rpm -O bintray-wso2-rpm.repo sudo mv bintray-wso2-rpm.repo /etc/yum.repos.d/ To install the package, issue the following command. yum update yum install wso2sp-4.3.0.x86_64","title":"Installing the package"},{"location":"setup/installing-Stream-Processor-Using-Yum/#running-the-product","text":"The following subsections provide instructions to run each runtime of WSO2 Stream Processor. Access the URLs via your favorite web browser by using the following credentials: Username : admin Password : admin","title":"Running the product"},{"location":"setup/installing-Stream-Processor-Using-Yum/#dashboard-profile","text":"To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -dashboard Once the dashboard profile starts, access the required component via your favourite browser. The URLs for the components within this profile are as follows. Component URL Dashboard Portal https://localhost:9643/portal Business Rules https://localhost:9643/business-rules Status Dashboard https://localhost:9643/monitoring Policies https://localhost:9643/policies","title":"Dashboard profile"},{"location":"setup/installing-Stream-Processor-Using-Yum/#editor-profile","text":"To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -editor Once the dashboard profile starts, access the required component via your favourite browser. The URLs for the components within this profile are as follows. Component URL Editor https://localhost:9390/editor Template Editor https://localhost:9390/template-editor","title":"Editor profile"},{"location":"setup/installing-Stream-Processor-Using-Yum/#manager-profile","text":"To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -manager","title":"Manager profile"},{"location":"setup/installing-Stream-Processor-Using-Yum/#worker-profile","text":"To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -worker","title":"Worker profile"},{"location":"setup/installing-Stream-Processor/","text":"Installing Stream Processor The following are options that you can use to install WSO2 Stream Processor. Installing Stream Processor 4.3.0","title":"Installing Stream Processor"},{"location":"setup/installing-Stream-Processor/#installing-stream-processor","text":"The following are options that you can use to install WSO2 Stream Processor. Installing Stream Processor 4.3.0","title":"Installing Stream Processor"},{"location":"setup/installing-si-using-ansible/","text":"Installing Stream Processor 4.3.0 Using Ansible Prerequisites : I nstall Ansible by following the Installation guide here . Steps : Clone WSO2 Stream Processor Ansible git repository and switch to the relevant resource directory by executing the following commands. git clone https://github.com/wso2/ansible-sp cd ansible-sp Download the Deb or RPM binary distributions from following locations and copy to the files directory. WSO2 Stream Processor 4.3.0 package Download the JDBC driver from the following location and copy to the files directory. MySQL Connector/J Run the playbook. The existing Ansible scripts contain the configurations to set up a single node WSO2 Stream Processor pattern. In order to deploy the pattern, you need to replace the [ip_address] given in the inventory file under dev folder by the IP of the location where you need to host the Stream Processor . An example is given below. [sp] dashboard_1 ansible_host= ip_address ansible_user= ssh_user editor_1 ansible_host= ip_address ansible_user= ssh_user worker_1 ansible_host= ip_address ansible_user= ssh_user Run the following command to run the scripts. ansible-playbook -i dev site.yml Once the deployment is started, try to access the web UIs via the following URLs and default credentials on the web browser . Editor - https:// ip-address :9390/editor Dashboard - https:// ip-address :9643/monitoring Username: admin Password: admin Customize the WSO2 Ansible scripts The templates that are used by the Ansible scripts are in j2 format in-order to enable parameterization. If you need to alter the configurations given, please change the parameterized values in the yaml files under group_vars and host_vars. You can add customizations to custom.yml under tasks of each role. Step 1 Uncomment the following line in main.yml under the role you want to customize. Import_tasks: custom.yml Step 2 Add the configurations to the custom.yml . A sample is given below. - name: \"Copy custom file\" template: src: path/to/example/file/example.xml.j2 dest: destination/example.xml.j2 when: \"(inventory_hostname in groups['apim'])\"","title":"Installing the Streaming Integrator Using Ansible"},{"location":"setup/installing-si-using-ansible/#installing-stream-processor-430-using-ansible","text":"Prerequisites : I nstall Ansible by following the Installation guide here . Steps : Clone WSO2 Stream Processor Ansible git repository and switch to the relevant resource directory by executing the following commands. git clone https://github.com/wso2/ansible-sp cd ansible-sp Download the Deb or RPM binary distributions from following locations and copy to the files directory. WSO2 Stream Processor 4.3.0 package Download the JDBC driver from the following location and copy to the files directory. MySQL Connector/J Run the playbook. The existing Ansible scripts contain the configurations to set up a single node WSO2 Stream Processor pattern. In order to deploy the pattern, you need to replace the [ip_address] given in the inventory file under dev folder by the IP of the location where you need to host the Stream Processor . An example is given below. [sp] dashboard_1 ansible_host= ip_address ansible_user= ssh_user editor_1 ansible_host= ip_address ansible_user= ssh_user worker_1 ansible_host= ip_address ansible_user= ssh_user Run the following command to run the scripts. ansible-playbook -i dev site.yml Once the deployment is started, try to access the web UIs via the following URLs and default credentials on the web browser . Editor - https:// ip-address :9390/editor Dashboard - https:// ip-address :9643/monitoring Username: admin Password: admin","title":"Installing Stream Processor 4.3.0 Using Ansible"},{"location":"setup/installing-si-using-ansible/#customize-the-wso2-ansible-scripts","text":"The templates that are used by the Ansible scripts are in j2 format in-order to enable parameterization. If you need to alter the configurations given, please change the parameterized values in the yaml files under group_vars and host_vars. You can add customizations to custom.yml under tasks of each role. Step 1 Uncomment the following line in main.yml under the role you want to customize. Import_tasks: custom.yml Step 2 Add the configurations to the custom.yml . A sample is given below. - name: \"Copy custom file\" template: src: path/to/example/file/example.xml.j2 dest: destination/example.xml.j2 when: \"(inventory_hostname in groups['apim'])\"","title":"Customize the WSO2 Ansible scripts"},{"location":"setup/installing-si-using-apt/","text":"Installing Stream Processor Using Apt To install WSO2 Stream Processor with Apt, follow the steps below: To add the Apt key, issue the following command. apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys \"379CE192D401AB61\" Add the repository to the source list by issuing the following command. echo \"deb https://dl.bintray.com/wso2/deb sp_430 release\" | \\ sudo tee -a /etc/apt/sources.list Install the package by issuing the following command. apt update apt install wso2sp-4.3.0 Running the product The following commands are available for running each runtime of the WSO2 Stream Processor. Access the URLs on your favorite web browser. You can use the following credentials to log in: Username : admin Password : admin The instructions to run each profile are as follows: Dashboard profile Issue the following command in the terminal. wso2sp- SP_VERSION -dashboard You can access the different components in this profile via the following URLs. Component URL Dashboard Portal https://localhost:9643/portal Business Rules https://localhost:9643/business-rules Status Dashboard https://localhost:9643/monitoring Policies https://localhost:9643/policies Worker profile Issue the following command in the terminal. wso2sp- SP_VERSION -worker Manager profile Issue the following command in the terminal. wso2sp- SP_VERSION -manager Editor profile Issue the following command in the terminal. wso2sp- SP_VERSION -editor You can access the different components in this profile via the following URLs. Component URL Editor http://localhost:9390/editor Template Editor http://localhost:9390/template-editor","title":"Installing the Streaming Integrator Using Apt"},{"location":"setup/installing-si-using-apt/#installing-stream-processor-using-apt","text":"To install WSO2 Stream Processor with Apt, follow the steps below: To add the Apt key, issue the following command. apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys \"379CE192D401AB61\" Add the repository to the source list by issuing the following command. echo \"deb https://dl.bintray.com/wso2/deb sp_430 release\" | \\ sudo tee -a /etc/apt/sources.list Install the package by issuing the following command. apt update apt install wso2sp-4.3.0","title":"Installing Stream Processor Using Apt"},{"location":"setup/installing-si-using-apt/#running-the-product","text":"The following commands are available for running each runtime of the WSO2 Stream Processor. Access the URLs on your favorite web browser. You can use the following credentials to log in: Username : admin Password : admin The instructions to run each profile are as follows:","title":"Running the product"},{"location":"setup/installing-si-using-apt/#dashboard-profile","text":"Issue the following command in the terminal. wso2sp- SP_VERSION -dashboard You can access the different components in this profile via the following URLs. Component URL Dashboard Portal https://localhost:9643/portal Business Rules https://localhost:9643/business-rules Status Dashboard https://localhost:9643/monitoring Policies https://localhost:9643/policies","title":"Dashboard profile"},{"location":"setup/installing-si-using-apt/#worker-profile","text":"Issue the following command in the terminal. wso2sp- SP_VERSION -worker","title":"Worker profile"},{"location":"setup/installing-si-using-apt/#manager-profile","text":"Issue the following command in the terminal. wso2sp- SP_VERSION -manager","title":"Manager profile"},{"location":"setup/installing-si-using-apt/#editor-profile","text":"Issue the following command in the terminal. wso2sp- SP_VERSION -editor You can access the different components in this profile via the following URLs. Component URL Editor http://localhost:9390/editor Template Editor http://localhost:9390/template-editor","title":"Editor profile"},{"location":"setup/installing-si-using-aws-cloud-formation/","text":"Installing Stream Processor Using AWS Cloud Formation To install WSO2 Stream Processor using AWS Cloud Formation, follow the steps below: Create and upload an SSL certificate into AWS, which is required to initiate the SSL handshake for HTTPS. Create a key pair for the desired region, which is required to SSH into instances. Continue to the next step if you want to use an existing key pair. After selecting the preferred template in the preferred region, you should land on the AWS CloudFormation Select Template page. Fill in the parameter details and click the Create Stack . Enter valid WUM credentials in the CloudFormation template to deploy the product with the latest updates. Leave it blank to deploy the GA version of the product. !!! info Creating the stack takes several minutes (\\~20 minutes). It may also take around 15 more minutes for the services to become active.","title":"Installing the Streaming Integrator Using AWS Cloud Formation"},{"location":"setup/installing-si-using-aws-cloud-formation/#installing-stream-processor-using-aws-cloud-formation","text":"To install WSO2 Stream Processor using AWS Cloud Formation, follow the steps below: Create and upload an SSL certificate into AWS, which is required to initiate the SSL handshake for HTTPS. Create a key pair for the desired region, which is required to SSH into instances. Continue to the next step if you want to use an existing key pair. After selecting the preferred template in the preferred region, you should land on the AWS CloudFormation Select Template page. Fill in the parameter details and click the Create Stack . Enter valid WUM credentials in the CloudFormation template to deploy the product with the latest updates. Leave it blank to deploy the GA version of the product. !!! info Creating the stack takes several minutes (\\~20 minutes). It may also take around 15 more minutes for the services to become active.","title":"Installing Stream Processor Using AWS Cloud Formation"},{"location":"setup/installing-si-using-brew/","text":"Installing Stream Processor Using Brew The following sections cover how to install and run WSO2 Stream Processor using Brew. Installing the package Running the product Tip Before you begin: The system requirements for installing WSO2 Stream Processor using Brew is as follows: 3 GHz Dual-core Xeon/Opteron (or latest) 2 GB RAM Installing the package To install the package, follow the steps below: Install Brew by following the instructions here . Add the WSO2 repository as a tap if you have not already done so by issuing the following command. brew tap wso2/wso2 This enables you to download any WSO2 product via Homebrew. Install WSO2 SP by issuing the following command. brew install wso2sp-4.3.0 Running the product The following subsections provide instructions to run each runtime of WSO2 Stream Processor. Access the URLs via your favorite web browser by using the following credentials: Username : admin Password : admin Dashboard profile To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -dashboard Once the dashboard profile starts, access the required component via your favourite browser. The URLs for the components within this profile are as follows. Component URL Dashboard Portal https://localhost:9643/portal Business Rules https://localhost:9643/business-rules Status Dashboard https://localhost:9643/monitoring Policies https://localhost:9643/policies Editor profile To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -editor Once the dashboard profile starts, access the required component via your favourite browser. The URLs for the components within this profile are as follows. Component URL Editor https://localhost:9390/editor Template Editor https://localhost:9390/template-editor Manager profile To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -manager Worker profile To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -worker","title":"Installing the Streaming Integrator Using Brew"},{"location":"setup/installing-si-using-brew/#installing-stream-processor-using-brew","text":"The following sections cover how to install and run WSO2 Stream Processor using Brew. Installing the package Running the product Tip Before you begin: The system requirements for installing WSO2 Stream Processor using Brew is as follows: 3 GHz Dual-core Xeon/Opteron (or latest) 2 GB RAM","title":"Installing Stream Processor Using Brew"},{"location":"setup/installing-si-using-brew/#installing-the-package","text":"To install the package, follow the steps below: Install Brew by following the instructions here . Add the WSO2 repository as a tap if you have not already done so by issuing the following command. brew tap wso2/wso2 This enables you to download any WSO2 product via Homebrew. Install WSO2 SP by issuing the following command. brew install wso2sp-4.3.0","title":"Installing the package"},{"location":"setup/installing-si-using-brew/#running-the-product","text":"The following subsections provide instructions to run each runtime of WSO2 Stream Processor. Access the URLs via your favorite web browser by using the following credentials: Username : admin Password : admin","title":"Running the product"},{"location":"setup/installing-si-using-brew/#dashboard-profile","text":"To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -dashboard Once the dashboard profile starts, access the required component via your favourite browser. The URLs for the components within this profile are as follows. Component URL Dashboard Portal https://localhost:9643/portal Business Rules https://localhost:9643/business-rules Status Dashboard https://localhost:9643/monitoring Policies https://localhost:9643/policies","title":"Dashboard profile"},{"location":"setup/installing-si-using-brew/#editor-profile","text":"To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -editor Once the dashboard profile starts, access the required component via your favourite browser. The URLs for the components within this profile are as follows. Component URL Editor https://localhost:9390/editor Template Editor https://localhost:9390/template-editor","title":"Editor profile"},{"location":"setup/installing-si-using-brew/#manager-profile","text":"To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -manager","title":"Manager profile"},{"location":"setup/installing-si-using-brew/#worker-profile","text":"To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -worker","title":"Worker profile"},{"location":"setup/installing-si-using-docker-compose/","text":"Installing Stream Processor Using Docker Compose This topic provides instructions on how to install WSO2 Stream Processor using Docker Compose. You can install the product with updates or install the WSO2 Stream Processor general availability release (that does not include updates). Tip System Requirements 3 GHz Dual-core Xeon/Opteron (or latest) 8 GB RAM 10 GB free disk space To install the Stream Processor using Docker Compose, follow the steps below: Install Docker by following the instructions provided here . Login to the WSO2 Docker registry by executing following Docker command. When prompted, enter the username and password of your free trial account. docker login docker.wso2.com Clone the WSO2 Analytics Docker resource Git repository and switch to the relevant resource directory by executing the following commands. git clone https://github.com/wso2/docker-sp cd docker-sp/docker-compose Switch to the docker-compose/editor-worker-dashboard directory. Editor with Worker and Dashboard. cd editor-worker-dashboard WSO2 Stream Processor Distributed deployment. cd sp-distributed Execute the following Docker command to start the selected deployment. docker-compose up Once WSO2 Stream Processor is successfully deployed, access the following URLs on your web browser using the following credentials: username: admin password: admin Editor: https://localhost:9390/editor Dashboard: https://localhost:9643/monitoring You can view the WSO2 Stream Processor samples as well as implement new Siddhi applications using the editor. These Siddhi applications can then be imported and deployed in the worker node. Follow this document for a detailed guide on how to export a Siddhi application. This document provides details on deploying a Siddhi application using the WSO2 Stream Processor Rest API. Use the worker node URL shown below when using the WSO2 Stream Processor Rest API. https://localhost:9443/siddhi-apps Access the stream processor dashboard via the following URL using the following credentials: username: admin password: admin https://localhost:9643/monitoring/ To start using WSO2 Stream Processor, read the Quick Start Guide .","title":"Installing the Streaming Integrator Using Docker Compose"},{"location":"setup/installing-si-using-docker-compose/#installing-stream-processor-using-docker-compose","text":"This topic provides instructions on how to install WSO2 Stream Processor using Docker Compose. You can install the product with updates or install the WSO2 Stream Processor general availability release (that does not include updates). Tip System Requirements 3 GHz Dual-core Xeon/Opteron (or latest) 8 GB RAM 10 GB free disk space To install the Stream Processor using Docker Compose, follow the steps below: Install Docker by following the instructions provided here . Login to the WSO2 Docker registry by executing following Docker command. When prompted, enter the username and password of your free trial account. docker login docker.wso2.com Clone the WSO2 Analytics Docker resource Git repository and switch to the relevant resource directory by executing the following commands. git clone https://github.com/wso2/docker-sp cd docker-sp/docker-compose Switch to the docker-compose/editor-worker-dashboard directory. Editor with Worker and Dashboard. cd editor-worker-dashboard WSO2 Stream Processor Distributed deployment. cd sp-distributed Execute the following Docker command to start the selected deployment. docker-compose up Once WSO2 Stream Processor is successfully deployed, access the following URLs on your web browser using the following credentials: username: admin password: admin Editor: https://localhost:9390/editor Dashboard: https://localhost:9643/monitoring You can view the WSO2 Stream Processor samples as well as implement new Siddhi applications using the editor. These Siddhi applications can then be imported and deployed in the worker node. Follow this document for a detailed guide on how to export a Siddhi application. This document provides details on deploying a Siddhi application using the WSO2 Stream Processor Rest API. Use the worker node URL shown below when using the WSO2 Stream Processor Rest API. https://localhost:9443/siddhi-apps Access the stream processor dashboard via the following URL using the following credentials: username: admin password: admin https://localhost:9643/monitoring/ To start using WSO2 Stream Processor, read the Quick Start Guide .","title":"Installing Stream Processor Using Docker Compose"},{"location":"setup/installing-si-using-docker/","text":"Installing Stream Processor Using Docker Tip Before you begin: You need the following system requrements: 3 GHz Dual-core Xeon/Opteron (or latest) 8 GB RAM 10 GB free disk space Install Docker by following the instructions provided here . WSO2 provides open source Docker images to run WSO2 Stream Processor in Docker Hub. You can view these images from here . Downloading and installing WSO2 Stream Processor Issue the following commands to pull tghe required WSO2 Stream Processor profile with updates from the Docker image. Profile Command worker docker pull wso2/wso2sp-worker manager docker pull wso2/wso2sp-manager editor docker pull wso2/wso2sp-editor dashboard docker pull wso2/wso2sp-dashboard Running WSO2 Stream Processor To run WSO2 SP, follow the steps below: To start each WSO2 Stream Processor profile in a Docker container, issue the following commands: **For dashboard: ** docker run -it -p 9643:9643 wso2/wso2sp-dashboard **For editor: ** docker run -it \\ -p 9390:9390 \\ -p 9743:9743 \\ wso2/wso2sp-editor **For manager: ** docker run -it wso2/wso2sp-manager For worker: docker run -it wso2/wso2sp-worker Once the container is started, access the UIs of each profile via the following URLs on your favourite browser. You can enter admin as both the username and the password. Dashboard Business Rules : https://localhost:9643/business-rules Dashboard Portal : https://localhost:9643/portal Status Dashboard : https://localhost:9643/monitoring Editor Steam Processor Studio : https://localhost:9390/editor Template Editor : https://localhost:930/template-editor","title":"Installing the Streaming Integrator Using Docker"},{"location":"setup/installing-si-using-docker/#installing-stream-processor-using-docker","text":"Tip Before you begin: You need the following system requrements: 3 GHz Dual-core Xeon/Opteron (or latest) 8 GB RAM 10 GB free disk space Install Docker by following the instructions provided here . WSO2 provides open source Docker images to run WSO2 Stream Processor in Docker Hub. You can view these images from here .","title":"Installing Stream Processor Using Docker"},{"location":"setup/installing-si-using-docker/#downloading-and-installing-wso2-stream-processor","text":"Issue the following commands to pull tghe required WSO2 Stream Processor profile with updates from the Docker image. Profile Command worker docker pull wso2/wso2sp-worker manager docker pull wso2/wso2sp-manager editor docker pull wso2/wso2sp-editor dashboard docker pull wso2/wso2sp-dashboard","title":"Downloading and installing WSO2 Stream Processor"},{"location":"setup/installing-si-using-docker/#running-wso2-stream-processor","text":"To run WSO2 SP, follow the steps below: To start each WSO2 Stream Processor profile in a Docker container, issue the following commands: **For dashboard: ** docker run -it -p 9643:9643 wso2/wso2sp-dashboard **For editor: ** docker run -it \\ -p 9390:9390 \\ -p 9743:9743 \\ wso2/wso2sp-editor **For manager: ** docker run -it wso2/wso2sp-manager For worker: docker run -it wso2/wso2sp-worker Once the container is started, access the UIs of each profile via the following URLs on your favourite browser. You can enter admin as both the username and the password. Dashboard Business Rules : https://localhost:9643/business-rules Dashboard Portal : https://localhost:9643/portal Status Dashboard : https://localhost:9643/monitoring Editor Steam Processor Studio : https://localhost:9390/editor Template Editor : https://localhost:930/template-editor","title":"Running WSO2 Stream Processor"},{"location":"setup/installing-si-using-helm/","text":"Installing Stream Processor Using Helm To install WSO2 Stream Processor using Helm, follow the steps below: Install Git , Helm (and Tiller) and Kubernetes client (tested with v1.10). As a result: An already setup Kubernetes cluster with NGINX Ingress Controller enabled. A pre-configured Network File System (NFS) to be used as the persistent volume for artifact sharing and persistence. In the NFS server instance, create a Linux system user account named wso2carbon with 802 as the user ID, and a system group named wso2 with 802 as the group ID. Add the wso2carbon user to the wso2 group. groupadd --system -g 802 wso2 useradd --system -g 802 -u 802 wso2carbon Setting up a minimum HA cluster To set up WSO2 SP as a minimum HA cluster using Helm, follow the steps below: Info In the context of this scenario, KUBERNETES_HOME refers to the the local copy of the GitHub repository, and HELM_HOME refers to the KUBERNETES_HOME /helm/pattern-distributed directory. Clone Kubernetes Resources for WSO2 Stream Processor Git repository. git clone https://github.com/wso2/kubernetes-sp.git Setup a Network File System (NFS) to be used for persistent storage. Then do the following. Create and export unique directory within the NFS server instance for the following Kubernetes Persistent Volume resource defined in the HELM_HOME /pattern-distributed/values.yaml file: sharedSiddhiFilesLocationPath Grant ownership to the wso2carbon user and the wso2 group, for each of the previously created directories. sudo chown -R wso2carbon:wso2 directory_name Grant read-write-execute permissions to the wso2carbon user, for each of the previously created directories. chmod -R 700 directory_name Provide configurations as follows. The default product configurations are available at HELM_HOME /pattern-distributed/confs directory. Update them if required. Open the HELM_HOME /pattern-distributed/values.yaml file and provide the following values. Parameter Description username Your WSO2 username. password Your WSO2 password. email Your WSO2 email. serverIp NFS Server IP. sharedDeploymentLocationPath The NFS location path of the shared Siddhi file directory (i.e.., SP_HOME /deployment/siddhi-files ) Deploy product database(s) using MySQL in Kubernetes. helm install --name sp-rdbms -f HELM_HOME /mysql/values.yaml stable/mysql --namespace wso2 For a serious deployment (e.g. production grade setup), it is recommended to connect product instances to a user owned and managed RDBMS instance. Deploy the fully distributed deployment of WSO2 Stream Processor. helm install --name RELEASE_NAME HELM_HOME /pattern-distributed Access product management consoles. Then do the following. Obtain the external IP (EXTERNAL-IP) of the Ingress resources by listing down the Kubernetes Ingresses. kubectl get ing -n wso2 Add the above host as an entry in /etc/hosts file as shown below: EXTERNAL-IP wso2sp-dashboard EXTERNAL-IP wso2sp-manager-1 EXTERNAL-IP wso2sp-manager-2 Navigate to https://wso2sp-dashboard/monitoring from your favorite browser.","title":"Installing the Streaming Integrator Using Helm"},{"location":"setup/installing-si-using-helm/#installing-stream-processor-using-helm","text":"To install WSO2 Stream Processor using Helm, follow the steps below: Install Git , Helm (and Tiller) and Kubernetes client (tested with v1.10). As a result: An already setup Kubernetes cluster with NGINX Ingress Controller enabled. A pre-configured Network File System (NFS) to be used as the persistent volume for artifact sharing and persistence. In the NFS server instance, create a Linux system user account named wso2carbon with 802 as the user ID, and a system group named wso2 with 802 as the group ID. Add the wso2carbon user to the wso2 group. groupadd --system -g 802 wso2 useradd --system -g 802 -u 802 wso2carbon","title":"Installing Stream Processor Using Helm"},{"location":"setup/installing-si-using-helm/#setting-up-a-minimum-ha-cluster","text":"To set up WSO2 SP as a minimum HA cluster using Helm, follow the steps below: Info In the context of this scenario, KUBERNETES_HOME refers to the the local copy of the GitHub repository, and HELM_HOME refers to the KUBERNETES_HOME /helm/pattern-distributed directory. Clone Kubernetes Resources for WSO2 Stream Processor Git repository. git clone https://github.com/wso2/kubernetes-sp.git Setup a Network File System (NFS) to be used for persistent storage. Then do the following. Create and export unique directory within the NFS server instance for the following Kubernetes Persistent Volume resource defined in the HELM_HOME /pattern-distributed/values.yaml file: sharedSiddhiFilesLocationPath Grant ownership to the wso2carbon user and the wso2 group, for each of the previously created directories. sudo chown -R wso2carbon:wso2 directory_name Grant read-write-execute permissions to the wso2carbon user, for each of the previously created directories. chmod -R 700 directory_name Provide configurations as follows. The default product configurations are available at HELM_HOME /pattern-distributed/confs directory. Update them if required. Open the HELM_HOME /pattern-distributed/values.yaml file and provide the following values. Parameter Description username Your WSO2 username. password Your WSO2 password. email Your WSO2 email. serverIp NFS Server IP. sharedDeploymentLocationPath The NFS location path of the shared Siddhi file directory (i.e.., SP_HOME /deployment/siddhi-files ) Deploy product database(s) using MySQL in Kubernetes. helm install --name sp-rdbms -f HELM_HOME /mysql/values.yaml stable/mysql --namespace wso2 For a serious deployment (e.g. production grade setup), it is recommended to connect product instances to a user owned and managed RDBMS instance. Deploy the fully distributed deployment of WSO2 Stream Processor. helm install --name RELEASE_NAME HELM_HOME /pattern-distributed Access product management consoles. Then do the following. Obtain the external IP (EXTERNAL-IP) of the Ingress resources by listing down the Kubernetes Ingresses. kubectl get ing -n wso2 Add the above host as an entry in /etc/hosts file as shown below: EXTERNAL-IP wso2sp-dashboard EXTERNAL-IP wso2sp-manager-1 EXTERNAL-IP wso2sp-manager-2 Navigate to https://wso2sp-dashboard/monitoring from your favorite browser.","title":"Setting up a minimum HA cluster"},{"location":"setup/installing-si-using-kubernetes/","text":"Installing Stream Processor Using Kubernetes To install WSO2 Stream Processor using Kubernetes, follow the steps below: Install Git and Kubernetes client (tested with v1.10). As a result, an already setup Kubernetes cluster with NGINX Ingress Controller is enabled. A pre-configured Network File System (NFS) is used as the persistent volume for artifact sharing and persistence. In the NFS server instance, create a Linux system user account named wso2carbon with 802 as the user ID, and a system group named wso2 with 802 as the group ID. Add the wso2carbon user to the wso2 group. groupadd --system -g 802 wso2 useradd --system -g 802 -u 802 wso2carbon Fully distributed deployment of WSO2 Stream Processor To set up a fully distributed deployment of WSO2 Stream Processor with Kubernetes, follow the steps below: Clone the Kubernetes resources for WSO2 Stream Processor Git repository . Info The local copy of the Git repository is referred to as KUBERNETES_HOME from here onwards. git clone https://github.com/wso2/kubernetes-sp.git Setup a Network File System (NFS) to be used for persistent storage as follows. Create and export unique directories within the NFS server instance for each Kubernetes Persistent Volume resource defined in the KUBERNETES_HOME /pattern-distributed/volumes/persistent-volumes.yaml file. sudo chown -R wso2carbon:wso2 directory_name Grant ownership to the wso2carbon user and wso2 group, for each of the previously created directories by issuing the following command. chmod -R 700 directory_name Then, update each Kubernetes Persistent Volume resource with the corresponding NFS server IP (NFS_SERVER_IP), and the NFS server directory path (NFS_LOCATION_PATH) of the directory you exported. Setup product database(s) using MySQL in Kubernetes. Here, a NFS is needed for persisting MySQL DB data. Create and export a directory within the NFS server instance. Provide read-write-execute permissions to other users for the created folder. Then, update the Kubernetes Persistent Volume resource with the corresponding NFS server IP (NFS_SERVER_IP) and exported, NFS server directory path (NFS_LOCATION_PATH) in KUBERNETES_HOME /pattern-distributed/extras/rdbms/volumes/persistent-volumes.yaml . For a serious deployment (e.g. production grade setup), it is recommended to connect product instances to a user owned and managed RDBMS instance. Navigate to the KUBERNETES_HOME /pattern-distributed/scripts directory as follows. cd lt;KUBERNETES_HOME gt;/pattern-distributed/scripts Deploy the Kubernetes resources by executing the KUBERNETES_HOME /pattern-distributed/scripts/deploy.sh script as follows. ./deploy.sh --wso2-username= lt;WSO2_USERNAME gt; --wso2-password= lt;WSO2_PASSWORD gt; --cluster-admin-password= lt;K8S_CLUSTER_ADMIN_PASSWORD gt;WSO2_USERNAME: Your WSO2 username WSO2_USERNAME : Your WSO2 username WSO2_PASSWORD : Your WSO2 password K8S_CLUSTER_ADMIN_PASSWORD : Kubernetes cluster admin password Access product management consoles. Obtain the external IP (EXTERNAL-IP) of the Ingress resources by listing down the Kubernetes Ingresses. kubectl get ing The external IP can be found under the ADDRESS column of the output. Add the above host as an entry in the /etc/hosts file as shown below: \\ EXTERNAL-IP> wso2sp-dashboard lt;EXTERNAL-IP gt; wso2sp-manager-1 lt;EXTERNAL-IP gt; wso2sp-manager-2 Try navigating to https://wso2sp-dashboard/monitoring from your favorite browser.","title":"Installing the Streaming Integrator Using Kubernetes"},{"location":"setup/installing-si-using-kubernetes/#installing-stream-processor-using-kubernetes","text":"To install WSO2 Stream Processor using Kubernetes, follow the steps below: Install Git and Kubernetes client (tested with v1.10). As a result, an already setup Kubernetes cluster with NGINX Ingress Controller is enabled. A pre-configured Network File System (NFS) is used as the persistent volume for artifact sharing and persistence. In the NFS server instance, create a Linux system user account named wso2carbon with 802 as the user ID, and a system group named wso2 with 802 as the group ID. Add the wso2carbon user to the wso2 group. groupadd --system -g 802 wso2 useradd --system -g 802 -u 802 wso2carbon","title":"Installing Stream Processor Using Kubernetes"},{"location":"setup/installing-si-using-kubernetes/#fully-distributed-deployment-of-wso2-stream-processor","text":"To set up a fully distributed deployment of WSO2 Stream Processor with Kubernetes, follow the steps below: Clone the Kubernetes resources for WSO2 Stream Processor Git repository . Info The local copy of the Git repository is referred to as KUBERNETES_HOME from here onwards. git clone https://github.com/wso2/kubernetes-sp.git Setup a Network File System (NFS) to be used for persistent storage as follows. Create and export unique directories within the NFS server instance for each Kubernetes Persistent Volume resource defined in the KUBERNETES_HOME /pattern-distributed/volumes/persistent-volumes.yaml file. sudo chown -R wso2carbon:wso2 directory_name Grant ownership to the wso2carbon user and wso2 group, for each of the previously created directories by issuing the following command. chmod -R 700 directory_name Then, update each Kubernetes Persistent Volume resource with the corresponding NFS server IP (NFS_SERVER_IP), and the NFS server directory path (NFS_LOCATION_PATH) of the directory you exported. Setup product database(s) using MySQL in Kubernetes. Here, a NFS is needed for persisting MySQL DB data. Create and export a directory within the NFS server instance. Provide read-write-execute permissions to other users for the created folder. Then, update the Kubernetes Persistent Volume resource with the corresponding NFS server IP (NFS_SERVER_IP) and exported, NFS server directory path (NFS_LOCATION_PATH) in KUBERNETES_HOME /pattern-distributed/extras/rdbms/volumes/persistent-volumes.yaml . For a serious deployment (e.g. production grade setup), it is recommended to connect product instances to a user owned and managed RDBMS instance. Navigate to the KUBERNETES_HOME /pattern-distributed/scripts directory as follows. cd lt;KUBERNETES_HOME gt;/pattern-distributed/scripts Deploy the Kubernetes resources by executing the KUBERNETES_HOME /pattern-distributed/scripts/deploy.sh script as follows. ./deploy.sh --wso2-username= lt;WSO2_USERNAME gt; --wso2-password= lt;WSO2_PASSWORD gt; --cluster-admin-password= lt;K8S_CLUSTER_ADMIN_PASSWORD gt;WSO2_USERNAME: Your WSO2 username WSO2_USERNAME : Your WSO2 username WSO2_PASSWORD : Your WSO2 password K8S_CLUSTER_ADMIN_PASSWORD : Kubernetes cluster admin password Access product management consoles. Obtain the external IP (EXTERNAL-IP) of the Ingress resources by listing down the Kubernetes Ingresses. kubectl get ing The external IP can be found under the ADDRESS column of the output. Add the above host as an entry in the /etc/hosts file as shown below: \\ EXTERNAL-IP> wso2sp-dashboard lt;EXTERNAL-IP gt; wso2sp-manager-1 lt;EXTERNAL-IP gt; wso2sp-manager-2 Try navigating to https://wso2sp-dashboard/monitoring from your favorite browser.","title":"Fully distributed deployment of WSO2 Stream Processor"},{"location":"setup/installing-si-using-puppet/","text":"Installing Stream Processor 4.3.0 Using Puppet Prerequisites : I nstall the Puppet server in the master node by following the Installation guide here . Install the Puppet agent in the agent nodes by following the Installation guide here . Steps : Clone WSO2 Stream Processor Puppet git repository and switch to the relevant resource directory by executing the following commands. git clone https://github.com/wso2/puppet-sp cd puppet-sp Download the Deb or RPM binary distributions from following locations and copy to the files directory in relevant modules. WSO2 Stream Processor 4.3.0 deb rpm Download the JDBC driver from the following location and copy to the files directory in relevant modules. MySQL Connector/J Run the existing scripts without customization. The existing Puppet scripts contain the configurations to set up a single node of the WSO2 Stream Processor runtime. Run the following command to deploy the Stream Processor server. export FACTOR_runtime=sp_editor puppet agent -vt Once the deployment is started, try to access the web UIs via the following URLs and default credentials on the web browser . https:// ip-address :9443/carbon Customizing the WSO2 Puppet scripts If you need to alter the configurations given, please change the parameterized values in the params.pp under manifests of each module. You can add customizations to the custom.pp file under the same manifests folder.","title":"Installing the Streaming Integrator Using Puppet"},{"location":"setup/installing-si-using-puppet/#installing-stream-processor-430-using-puppet","text":"Prerequisites : I nstall the Puppet server in the master node by following the Installation guide here . Install the Puppet agent in the agent nodes by following the Installation guide here . Steps : Clone WSO2 Stream Processor Puppet git repository and switch to the relevant resource directory by executing the following commands. git clone https://github.com/wso2/puppet-sp cd puppet-sp Download the Deb or RPM binary distributions from following locations and copy to the files directory in relevant modules. WSO2 Stream Processor 4.3.0 deb rpm Download the JDBC driver from the following location and copy to the files directory in relevant modules. MySQL Connector/J Run the existing scripts without customization. The existing Puppet scripts contain the configurations to set up a single node of the WSO2 Stream Processor runtime. Run the following command to deploy the Stream Processor server. export FACTOR_runtime=sp_editor puppet agent -vt Once the deployment is started, try to access the web UIs via the following URLs and default credentials on the web browser . https:// ip-address :9443/carbon","title":"Installing Stream Processor 4.3.0 Using Puppet"},{"location":"setup/installing-si-using-puppet/#customizing-the-wso2-puppet-scripts","text":"If you need to alter the configurations given, please change the parameterized values in the params.pp under manifests of each module. You can add customizations to the custom.pp file under the same manifests folder.","title":"Customizing the WSO2 Puppet scripts"},{"location":"setup/installing-si-using-vagrant/","text":"Installing Stream Processor 4.3.0 Using Vagrant This topic provides instructions on how to install WSO2 Stream Processor 4.3.0 using Vagrant. You can install the product with updates or install the WSO2 Stream Processor 4.3.0 general availability release (that does not include updates). Vagrant Resources for WSO2 Stream Processor with Updates Vagrant Resources for General Availability Releases Tip Before you begin: The system requirements are as follows: 3 GHz Dual-core Xeon/Opteron (or latest) 16 GB RAM 10 GB free disk space To install WSO2 Stream Processor with updates using Vagrant, follow the steps below: Install Vagrant by following the Vagrant installation guide . Install VirtualBox by following the VirtualBox installation guide . Clone WSO2 Stream Processor Vagrant git repository and switch to the relevant resource directory by executing the following commands. git clone https://github.com/wso2/vagrant-sp cd vagrant-sp Execute the following Vagrant command to start the selected deployment. Vagrant --updates up Once the deployment is started, access the Editor using the following URL. https://localhost:9743/editor You can view the Stream Processor samples as well as implement new Siddhi applications using the Editor. These Siddhi applications can then be imported and deployed in the worker node. Follow this document for a detailed guide on how to export a Siddhi application. This document provides details on deploying a Siddhi application using the Stream Processor Rest API. Use the worker node URL shown below when using the Stream Processor Rest API. https://localhost:9443/siddhi-apps Access the Stream Processor Dashboard via the following URL. https://localhost:9643/monitoring/ Access the servers using the following credentials. Username: admin Password: admin Tip Before you begin: The system requirements are as follows: 3 GHz Dual-core Xeon/Opteron (or latest) 16 GB RAM 10 GB free disk space To install WSO2 Stream Processor without updates using Vagrant, follow the steps below: Install Vagrant by following the Vagrant installation guide . Install VirtualBox by following the VirtualBox installation guide . Clone WSO2 Stream Processor Vagrant git repository and switch to the relevant resource directory by executing the following commands. git clone https://github.com/wso2/vagrant-sp cd vagrant-sp Execute the following Vagrant command to start the selected deployment. Vagrant --updates up Once the deployment is started, access the Editor using the following URL. https://localhost:9743/editor You can view the Stream Processor samples as well as implement new Siddhi applications using the Editor. These Siddhi applications can then be imported and deployed in the worker node. Follow this document for a detailed guide on how to export a Siddhi application. This document provides details on deploying a Siddhi application using the Stream Processor Rest API. Use the worker node URL shown below when using the Stream Processor Rest API. https://localhost:9443/siddhi-apps Access the Stream Processor Dashboard via the following URL. https://localhost:9643/monitoring/ Access the servers using the following credentials. Username: admin Password: admin","title":"Installing the Streaming Integrator Using Vagrant"},{"location":"setup/installing-si-using-vagrant/#installing-stream-processor-430-using-vagrant","text":"This topic provides instructions on how to install WSO2 Stream Processor 4.3.0 using Vagrant. You can install the product with updates or install the WSO2 Stream Processor 4.3.0 general availability release (that does not include updates). Vagrant Resources for WSO2 Stream Processor with Updates Vagrant Resources for General Availability Releases Tip Before you begin: The system requirements are as follows: 3 GHz Dual-core Xeon/Opteron (or latest) 16 GB RAM 10 GB free disk space To install WSO2 Stream Processor with updates using Vagrant, follow the steps below: Install Vagrant by following the Vagrant installation guide . Install VirtualBox by following the VirtualBox installation guide . Clone WSO2 Stream Processor Vagrant git repository and switch to the relevant resource directory by executing the following commands. git clone https://github.com/wso2/vagrant-sp cd vagrant-sp Execute the following Vagrant command to start the selected deployment. Vagrant --updates up Once the deployment is started, access the Editor using the following URL. https://localhost:9743/editor You can view the Stream Processor samples as well as implement new Siddhi applications using the Editor. These Siddhi applications can then be imported and deployed in the worker node. Follow this document for a detailed guide on how to export a Siddhi application. This document provides details on deploying a Siddhi application using the Stream Processor Rest API. Use the worker node URL shown below when using the Stream Processor Rest API. https://localhost:9443/siddhi-apps Access the Stream Processor Dashboard via the following URL. https://localhost:9643/monitoring/ Access the servers using the following credentials. Username: admin Password: admin Tip Before you begin: The system requirements are as follows: 3 GHz Dual-core Xeon/Opteron (or latest) 16 GB RAM 10 GB free disk space To install WSO2 Stream Processor without updates using Vagrant, follow the steps below: Install Vagrant by following the Vagrant installation guide . Install VirtualBox by following the VirtualBox installation guide . Clone WSO2 Stream Processor Vagrant git repository and switch to the relevant resource directory by executing the following commands. git clone https://github.com/wso2/vagrant-sp cd vagrant-sp Execute the following Vagrant command to start the selected deployment. Vagrant --updates up Once the deployment is started, access the Editor using the following URL. https://localhost:9743/editor You can view the Stream Processor samples as well as implement new Siddhi applications using the Editor. These Siddhi applications can then be imported and deployed in the worker node. Follow this document for a detailed guide on how to export a Siddhi application. This document provides details on deploying a Siddhi application using the Stream Processor Rest API. Use the worker node URL shown below when using the Stream Processor Rest API. https://localhost:9443/siddhi-apps Access the Stream Processor Dashboard via the following URL. https://localhost:9643/monitoring/ Access the servers using the following credentials. Username: admin Password: admin","title":"Installing Stream Processor 4.3.0 Using Vagrant"},{"location":"setup/installing-si-using-yum/","text":"Installing Stream Processor Using Yum The following sections cover how to install and run WSO2 Stream Processor using Yum. Prerequisites Installing the package Running the product Prerequisites wget needs to be pre-installed. Installing the package To install the package, follow the steps below: To generate the repo file, issue the following command. wget https://bintray.com/wso2/rpm/rpm -O bintray-wso2-rpm.repo sudo mv bintray-wso2-rpm.repo /etc/yum.repos.d/ To install the package, issue the following command. yum update yum install wso2sp-4.3.0.x86_64 Running the product The following subsections provide instructions to run each runtime of WSO2 Stream Processor. Access the URLs via your favorite web browser by using the following credentials: Username : admin Password : admin Dashboard profile To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -dashboard Once the dashboard profile starts, access the required component via your favourite browser. The URLs for the components within this profile are as follows. Component URL Dashboard Portal https://localhost:9643/portal Business Rules https://localhost:9643/business-rules Status Dashboard https://localhost:9643/monitoring Policies https://localhost:9643/policies Editor profile To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -editor Once the dashboard profile starts, access the required component via your favourite browser. The URLs for the components within this profile are as follows. Component URL Editor https://localhost:9390/editor Template Editor https://localhost:9390/template-editor Manager profile To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -manager Worker profile To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -worker","title":"Installing the Streaming Integrator Using Yum"},{"location":"setup/installing-si-using-yum/#installing-stream-processor-using-yum","text":"The following sections cover how to install and run WSO2 Stream Processor using Yum. Prerequisites Installing the package Running the product","title":"Installing Stream Processor Using Yum"},{"location":"setup/installing-si-using-yum/#prerequisites","text":"wget needs to be pre-installed.","title":"Prerequisites"},{"location":"setup/installing-si-using-yum/#installing-the-package","text":"To install the package, follow the steps below: To generate the repo file, issue the following command. wget https://bintray.com/wso2/rpm/rpm -O bintray-wso2-rpm.repo sudo mv bintray-wso2-rpm.repo /etc/yum.repos.d/ To install the package, issue the following command. yum update yum install wso2sp-4.3.0.x86_64","title":"Installing the package"},{"location":"setup/installing-si-using-yum/#running-the-product","text":"The following subsections provide instructions to run each runtime of WSO2 Stream Processor. Access the URLs via your favorite web browser by using the following credentials: Username : admin Password : admin","title":"Running the product"},{"location":"setup/installing-si-using-yum/#dashboard-profile","text":"To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -dashboard Once the dashboard profile starts, access the required component via your favourite browser. The URLs for the components within this profile are as follows. Component URL Dashboard Portal https://localhost:9643/portal Business Rules https://localhost:9643/business-rules Status Dashboard https://localhost:9643/monitoring Policies https://localhost:9643/policies","title":"Dashboard profile"},{"location":"setup/installing-si-using-yum/#editor-profile","text":"To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -editor Once the dashboard profile starts, access the required component via your favourite browser. The URLs for the components within this profile are as follows. Component URL Editor https://localhost:9390/editor Template Editor https://localhost:9390/template-editor","title":"Editor profile"},{"location":"setup/installing-si-using-yum/#manager-profile","text":"To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -manager","title":"Manager profile"},{"location":"setup/installing-si-using-yum/#worker-profile","text":"To run this profile, issue the following command in the terminal. wso2sp- SP_VERSION -worker","title":"Worker profile"},{"location":"setup/setting-up-physical-databases/","text":"","title":"Setting up physical databases"}]}